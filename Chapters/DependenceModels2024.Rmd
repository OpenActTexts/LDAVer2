
<!-- # Chap 1 -->

<!-- # Chap 2 -->

<!-- # Chap 3 -->

<!-- # Chap 4 -->

<!-- # Chap 5 -->

<!-- # Chap 6 -->

<!-- # Chap 7 -->

<!-- # Chap 8 -->

<!-- # Chap 9 -->

<!-- # Chap 10 -->

<!-- # Chap 11 -->

<!-- # Chap 12 -->

<!-- # Chap 13 -->

<!-- # Chap 14 -->

<!-- # Chap 15 -->


# Quantifying Dependence {#ChapDependenceModel}

*Chapter Preview*. Dependence modeling involves using statistical models to describe the dependence structure between random variables and enables us to understand the relationships between variables in a dataset. This chapter introduces readers to techniques for modeling and quantifying dependence or association of multivariate distributions. Section \@ref(S:Sec161) elaborates basic measures for modeling the dependence between variables. 

Section \@ref(S:Sec162) introduces an approach to modeling dependence using copulas which is reinforced with practical illustrations in Section \@ref(S:Sec163). The types of copula families and basic properties of copula functions are explained in Section \@ref(S:Sec164). The chapter concludes by explaining why the study of dependence modeling is important in Section \@ref(S:Sec166).


```{r echo=FALSE}
library(ggplot2)
Insample <- read.csv("Data/Insample.csv", header=T,na.strings=c("."), stringsAsFactors=FALSE)

Claim <- Insample$y
Coverage <- Insample$BCcov/1000000
NoClaimCredit <- Insample$NoClaimCredit
EntityType <- ifelse(Insample$TypeCity == 1, "City",
                   ifelse(Insample$TypeCounty == 1, "County",
                          ifelse(Insample$TypeMisc == 1, "Misc",
                                 ifelse(Insample$TypeSchool == 1, "School",
                                        ifelse(Insample$TypeTown == 1, "Town",
                                               ifelse(Insample$TypeVillage == 1, "Village", 0))))))

```




## Classic Measures of Scalar Associations {#S:Sec161}

***

In this section, you learn how to:

- Estimate correlation using the Pearson method 
- Use rank based measures like Spearman, Kendall to estimate correlation
- Measure tail dependency

***




In this chapter, we consider the first two variables from an insurance dataset of sample size ($n = 1500$) introduced in @frees1998understanding and is now readily available in the `copula` package; *losses* and *expenses*.

*  ${\tt LOSS}$, general liability claims from the Insurance Services Office, Inc. (ISO)
*  ${\tt ALAE}$, specifically attributable to the settlement of individual claims (e.g. lawyer's fees, claims investigation expenses)

We would like to know whether the distribution of ${\tt LOSS}$ depends on the distribution of ${\tt ALAE}$ or whether they are statistically independent. To visualize the relationship between losses and expenses, the scatterplots in Figure \@ref(fig:Fig161) are created on dollar and log dollar scales. It is difficult to see any relationship between the two variables in the left-hand panel. Their dependence is more evident when viewed on the log scale, as in the right-hand panel. This section elaborates basic measures for modeling the dependence between variables. 

(ref:Fig161) **Scatter Plot of LOSS and ALAE**

```{r  Fig161, echo=FALSE, out.width='80%',fig.cap='(ref:Fig161)', fig.height=5.5, fig.width=9}
library(copula)
data(loss) # loss data from copula library
LOSS <- loss$loss
ALAE <- loss$alae
par(mfrow=c(1, 2))
plot(LOSS, ALAE, cex=.5, ann = FALSE,xaxt='n',yaxt='n', xlim=c(0,2200000), ylim=c(0, 520000)) # dollar scale
mtext(side = 1, text = "LOSS", line = 1.8,cex=0.9)
xtick<-c(10000, 400000,800000,1200000,1600000, 2000000)
xticklab<-c("10K","400K","800K","1.2M","1.6M","2.0M")
axis(side=1, at=xtick, labels = FALSE)
text(x=xtick, -35000,labels = xticklab,pos=1, xpd = TRUE, line=2)

mtext(side = 2, text = "ALAE", line = 3.5, cex=0.9)
ytick<-c(5000, 100000, 200000, 300000, 400000, 500000)
yticklab<-c("5K", "100K","200K","300K", "400K","500K")
axis(side=2, at=ytick, labels = FALSE)
text(x=-400000,y= ytick,labels = yticklab, xpd = TRUE, line=2)

plot(log(LOSS), log(ALAE),cex=.5, ann = FALSE,xaxt='n',yaxt='n', xlim=c(2,14.60397), ylim=c(2, 14.60397)) # log scale

mtext(side = 1, text = "log(LOSS)", line = 1.8,cex=0.9)
xtick<-c(2,4,6,8,10,12,14)
xticklab<-c("2","4","6","8","10","12","14")
axis(side=1, at=xtick, labels = FALSE)
text(x=xtick, 1.2,labels = xticklab,pos=1, xpd = TRUE, line=2)


mtext(side = 2, text = "log(ALAE)", line = 3.0, cex=0.9)
ytick<-c(2,4,6,8,10,12, 14)
yticklab<-c("2","4","6","8","10","12", "14")
axis(side=2, at=ytick, labels = FALSE)
text(x=0.25,y= ytick,labels = yticklab, xpd = TRUE, line=2)

```


`r HideRCode('ScaHis.1',"R Code for Loss versus Expense Scatterplots")`

```{r, echo=HtmlEval, ref.label = 'Fig161', eval=FALSE}
```

</div>  



### Association Measures for Quantitative Variables

For this section, consider a pair of random variables $(X,Y)$ having joint distribution function $F(\cdot)$ and a random sample $(X_i,Y_i), i=1, \ldots, n$. For the continuous case, suppose that $F(\cdot)$ has  absolutely continuous marginals with marginal density functions.

#### Pearson Correlation {-}

Define the sample covariance function $\widehat{Cov}(X,Y) = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})$, where $\bar{X}$ and $\bar{Y}$ are the sample means of $X$ and $Y$, respectively. Then, the `r Gloss("product-moment (Pearson) correlation")` can be written as
$$
r = \frac{\widehat{Cov}(X,Y)}{\sqrt{\widehat{Cov}(X,X)\widehat{Cov}(Y,Y)}}
= \frac{\widehat{Cov}(X,Y)}{\sqrt{\widehat{Var}(X)}\sqrt{\widehat{Var}(Y)}}.
$$
The correlation statistic $r$ is widely used to capture linear association between random variables. It is a (nonparametric) estimator of the correlation parameter $\rho$, defined to be the covariance divided by the product of standard deviations. 

This statistic has several important features. Unlike regression estimators, it is symmetric between random variables, so the correlation between $X$ and $Y$ equals the correlation between $Y$ and $X$. It is unchanged by linear transformations of random variables (up to sign changes) so that we can multiply random variables or add constants as is helpful for interpretation. The range of the statistic is $[-1,1]$ which does not depend on the distribution of either $X$ or $Y$.

Further, in the case of independence, the correlation coefficient $r$ is 0. However, it is well known that zero correlation does not in general imply independence, one exception is the case of normally distributed random variables. The correlation statistic $r$ is also a (maximum likelihood) estimator of the association parameter for the bivariate normal distribution. So, for normally distributed data, the correlation statistic $r$ can be used to assess independence. For additional interpretations of this well-known statistic, readers will enjoy @lee1988thirteen.

You can obtain the `r Gloss('Pearson correlation')` statistic $r$ using the `cor()` function in `R` and selecting the `pearson` method. This is demonstrated below by using the ${\tt LOSS}$ rating variable in millions of dollars and ${\tt ALAE}$ amount variable in dollars from the dataset in Figure \@ref(fig:Fig161). 



```{r include=FALSE}
r<-cor(LOSS,ALAE, method = c("pearson"))
round(r,2)

r<-cor(LOSS,log(ALAE), method = c("pearson"))
round(r,2)

```

`r HideRCode('pearson.1','R Code for Pearson Correlation Statistic')`

```{r, echo=HtmlEval, eval=FALSE}
### Pearson correlation between LOSS and ALAE
r<-cor(LOSS,ALAE, method = c("pearson"))
round(r,2)

Output:
[1] 0.4

### Pearson correlation between log(LOSS) and log(ALAE)
r<-cor(LOSS,log(ALAE), method = c("pearson"))
round(r,2)

Output:
[1] 0.33
```
</div>  
  

From the `R` output above, $r=0.4$, which indicates a positive association between ${\tt LOSS}$ and ${\tt ALAE}$. This means that as the loss amount of a claim increases we expect expenses to increase. 


### Rank Based Measures {#S:Sec1612}

#### Spearman's Rho {-}

The Pearson correlation coefficient does have the drawback that it is not invariant to nonlinear transforms of the data. For example, the correlation between $X$ and $\log Y$ can be quite different from the correlation between $X$ and $Y$. As we see from the `R` code for the Pearson correlation statistic above, the correlation statistic $r$ between the ${\tt ALAE}$ variable in logarithmic dollars and the ${\tt LOSS}$ amounts variable in dollars is $0.33$ as compared to $0.4$ when we calculate the correlation between the ${\tt ALAE}$ variable in dollars and the ${\tt LOSS}$ amounts variable in dollars. This limitation is one reason for considering alternative statistics.

Alternative measures of correlation are based on ranks of the data. Let $R(X_j)$ denote the rank of $X_j$ from the sample $X_1, \ldots, X_n$ and similarly for $R(Y_j)$. Let $R(X) = \left(R(X_1), \ldots, R(X_n)\right)'$ denote the vector of ranks, and similarly for $R(Y)$. For example, if $n=3$ and $X=(24, 13, 109)$, then $R(X)=(2,1,3)$. A comprehensive introduction of rank statistics can be found in, for example, @hettmansperger1984statistical. Also, ranks can be used to obtain the empirical distribution function, refer to Section \@ref(S:Sec441) for more on the empirical distribution function.

With this, the correlation measure of @spearman1904proof is simply the product-moment correlation computed on the ranks:
$$
r_S = \frac{\widehat{Cov}(R(X),R(Y))}{\sqrt{\widehat{Cov}(R(X),R(X))\widehat{Cov}(R(Y),R(Y))}}
= \frac{\widehat{Cov}(R(X),R(Y))}{(n^2-1)/12} .
$$
You can obtain the Spearman correlation statistic $r_S$ using the `cor()` function in `R` and selecting the `spearman` method. From below, the Spearman correlation between the ${\tt LOSS}$ variable and ${\tt ALAE}$ variable is $0.45$.

```{r comment="" , include=FALSE}
rs<-cor(LOSS,ALAE, method = c("spearman"))
round(rs,2)

rs<-cor(LOSS,log(ALAE), method = c("spearman"))
round(rs,2)
```

`r HideRCode('spearman.1','R Code for Spearman Correlation Statistic')`

```{r echo=HtmlEval, eval=FALSE}
### Spearman correlation between LOSS and ALAE ###
rs<-cor(LOSS,ALAE, method = c("spearman"))
round(rs,2)

Output:
[1] 0.45

### Spearman correlation between LOSS and log(ALAE) ###
rs<-cor(LOSS,log(ALAE), method = c("spearman"))
round(rs,2)

Output:
[1] 0.45
```
</div>  

We can show that the Spearman correlation statistic is invariant under strictly increasing transformations. From the `R` Code for the Spearman correlation statistic above, $r_S=0.45$ between the ${\tt ALAE}$ variable in logarithmic dollars and ${\tt LOSS}$ amount variable in dollars. 


**Example 16.1.1. Calculation by Hand.** You are given the following six observations:
$$
\small{
\begin{array}{|c|c|c|}
\hline \textbf{Observation} & x \textbf{ value} & y \textbf{ value}\\
\hline 1 & 15 & 19  \\
\hline 2 & 9 & 7  \\
\hline 3 & 5 & 13   \\
\hline 4 & 3 & 15   \\
\hline 5 & 21 & 17   \\
\hline 6 & 12 & 11   \\
\hline
\end{array}
}
$$
Calculate the sample Spearman's $\rho$. 


`r HideExample('16.1.1','Show Example Solution')`

`r SolnBegin()` The Spearman correlation is simply the product-moment correlation computed on the ranks:
$$
\small{
r_S = \frac{\widehat{Cov}(R(X),R(Y))}{\sqrt{\widehat{Cov}(R(X),R(X))\widehat{Cov}(R(Y),R(Y))}}
= \frac{\widehat{Cov}(R(X),R(Y))}{(n^2-1)/12} .
}
$$
where $\widehat{Cov}(X,Y) = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})$. 

Then we have: 
`r SolnEnd()`
$$
\scriptsize 
\begin{array}{|c|c|c|c|c|c|c|c|}
\hline \textbf{Obs} & x  & y  &\textbf{Rank of} & \textbf{Rank of}  & R(X)_i - \bar{R(X)}& R(Y)_i - \bar{R(Y)} &(R(X)_i - \bar{R(X)}) \times \\
   & \textbf{ value}& \textbf{ value} & x_i ~~ (R(X)) & y_i  ~~ (R(Y)) & & &(R(Y)_i - \bar{R(Y)}) \\
\hline 1 & 15 & 19. & 2 & 1    & 2-3.5=-1.5 & 1-3.5=-2.5 & 3.75\\
\hline 2 & 9 & 7  & 4 & 6     &  4-3.5=0.5 &  6-3.5= 2.5 & 1.25\\
\hline 3 & 5 & 13  & 5 &4     & 5-3.5=1.5  &  4-3.5= 0.5& 0.75\\
\hline 4 & 3 & 15  & 6 & 3    & 6-3.5=2.5 &  3-3.5= -0.5&-1.25\\
\hline 5 & 21 & 17  & 1 & 2   & 1-3.5=-2.5  & 2-3.5= -1.5& 3.75\\
\hline 6 & 12 & 11  & 3 & 5   & 3-3.5=-0.5 & 5-3.5= 1.5& -0.75\\
\hline Total &  &  &  &   & & & 7.5\\
\hline
\end{array}
$$
`r LObjBegin()` 
Note that: $\bar{R(X)}=\bar{R(Y)}=\frac{1+2+3+4+5+6}{6}=3.5$. 

Then, 
$$
\widehat{Cov}(R(X),R(Y))=\frac{1}{n} \sum_{i=1}^n (R(X)_i - \bar{R(X)})(R(Y)_i - \bar{R(Y)})=\frac{7.5}{6}=1.25.
$$
Applying the formula
$$
r_S = \frac{\widehat{Cov}(R(X),R(Y))}{(n^2-1)/12}=\frac{1.25}{(6^2-1)/12}=0.42857 .
$$
`r LObjEnd()` 

</div>

*** 


#### Kendall's Tau {-}

An alternative measure that uses ranks is based on the concept of *concordance*. An observation pair $(X,Y)$ is said to be `r Gloss('concordant')` (`r Gloss('discordant')`) if the observation with a larger value of $X$ has also the larger (smaller) value of $Y$. Then $\Pr(concordance) = \Pr[ (X_1-X_2)(Y_1-Y_2) >0 ]$ , $\Pr(discordance) = \Pr[ (X_1-X_2)(Y_1-Y_2) <0 ]$, $\Pr(tie) = \Pr[ (X_1-X_2)(Y_1-Y_2) =0 ]$ and
$$
\begin{array}{rl}
\tau(X,Y) &= \Pr(concordance) - \Pr(discordance) \\
& = 2\Pr(concordance) - 1 + \Pr(tie).
\end{array}
$$
Thus, the population parameter Kendall's tau, $\tau=\tau(X,Y)$, measures whether higher values of one variable generally correspond to higher values of another variables, regardless of the actual values of those variables. 

To estimate this, the pairs $(X_i,Y_i)$ and $(X_j,Y_j)$ are said to be concordant if the product $sgn(X_j-X_i)sgn(Y_j-Y_i)$ equals 1 and discordant if the product equals -1. Here, $sgn(x)=1,0,-1$ as $x>0$, $x=0$, $x<0$, respectively. With this, we can express the (statistical) association measure of @kendall1938new, known as `r Gloss("Kendall's tau")`, as
$$
\begin{array}{rl}
\hat{\tau} &= \frac{2}{n(n-1)} \sum_{i<j} ~sgn(X_j-X_i) \times sgn(Y_j-Y_i)\\
&= \frac{2}{n(n-1)} \sum_{i<j} ~sgn(R(X_j)-R(X_i)) \times sgn(R(Y_j)-R(Y_i)) .
\end{array}
$$
Interestingly, @hougaard2000analysis, page 137, attributes the original discovery of this statistic to @fechnerkollektivmasslehre, noting that Kendall's discovery was independent and more complete than the original work.

You can obtain Kendall's tau using the `cor()` function in `R` and selecting the `kendall` method. From below, $\hat{\tau}=0.32$ between the ${\tt LOSS}$ variable in dollars and the ${\tt ALAE}$ variable in dollars. When there are ties in the data, the `cor()` function computes *Kendall's tau_b* as proposed by @kendall1945. 

```{r include=FALSE}
tau<-cor(LOSS,ALAE, method = c("kendall"))
round(tau,2)

tau<-cor(LOSS,log(ALAE), method = c("kendall"))
round(tau,2)
```

`r HideRCode('kendall.1',"R Code for Kendall's Tau")`

```{r echo=HtmlEval, eval=FALSE}
### Kendall's tau correlation between LOSS and ALAE ###
tau<-cor(LOSS,ALAE, method = c("kendall"))
round(tau,2)

Output:
[1]  0.32

### Kendall's tau correlation between LOSS and log(ALAE) ###
tau<-cor(LOSS,log(ALAE), method = c("kendall"))
round(tau,2)

Output:
[1] 0.32
```
</div>  

Also, to show that the Kendall's tau is invariant under strictly increasing transformations, we see that $\hat{\tau}=0.32$ between the ${\tt ALAE}$ variable in logarithmic dollars and the ${\tt LOSS}$ amount variable in dollars. 



**Example 16.1.2. Calculation by Hand.** You are given the following six observations:
$$
\small{
\begin{array}{|c|c|c|}
\hline \textbf{Observation} & x \textbf{ value} & y \textbf{ value}\\
\hline 1 & 15 & 19  \\
\hline 2 & 9 & 7  \\
\hline 3 & 5 & 13   \\
\hline 4 & 3 & 15   \\
\hline 5 & 21 & 17   \\
\hline 6 & 12 & 11   \\
\hline
\end{array}
}
$$
Calculate the sample Kendall's $\tau$.


`r HideExample('16.1.2','Show Example Solution')`

`r SolnBegin()` We can obtain the Kendall's tau using:
$$
\hat{\tau} = \frac{2}{n(n-1)} \sum_{i<j} ~sgn(X_j-X_i) \times sgn(Y_j-Y_i)
$$
Here, $sgn(x)=1,0,-1$ as $x>0$, $x=0$, $x<0$, respectively. For each pair of observations $i,j$ so that $i<j$, the pairs $(X_i,Y_i)$ and $(X_j,Y_j)$ are said to be concordant if the product $sgn(X_j-X_i)sgn(Y_j-Y_i)$ equals 1 and discordant if the product equals -1. This is summarized in the table below, where a 1 indicates concordance, -1 sign indicates discordance. Note: The pairs compared are in the upper triangle.
$$
\small \begin{array}{|c|c|c|c|c|c|c|}
\hline i/ j & j=1 & j=2 & j=3 & j=4 & j=5 & j=6\\
\hline i=1    &       & 1     & 1         & 1     & -1        & 1\\
\hline i=2    &       &       & -1        &-1     & 1        &1 \\
\hline i=3    &       &       &           &-1     & 1        & -1\\
\hline i=4    &       &       &           &       & 1        & -1 \\
\hline i=5    &       &       &           &       &           & 1\\
\hline i=6    &       &       &           &       &           &  \\
\hline
\end{array}
$$
There are 9 concordant pairs and 6 discordant pairs. Applying the formula
$$
\hat{\tau} = \frac{2}{n(n-1)} \sum_{i<j} ~sgn(X_j-X_i) \times sgn(Y_j-Y_i)=\frac{2}{6(6-1)}(3)=0.2 .
$$

`r SolnEnd()`

</div>

*** 


### Tail Dependence Coefficients {#S:Sec1613}


Tail dependence is a statistical concept that measures the strength of the dependence between two variables in the tails of their distribution. Specifically, tail dependence measures the correlation or dependence between the extreme values of two variables beyond a certain threshold, that is, the dependence in the corner of the lower-left quadrant or upper-right quadrant of the bivariate distribution. Tail dependence is essential in many areas of finance, economics, and risk management. For example, it is relevant in analyzing extreme events, such as financial crashes, natural disasters, and pandemics. In these situations, tail dependence can help to determine the likelihood of joint extreme events occurring and to develop strategies to manage the associated risks.

In Figure \@ref(fig:Fig162), the concept of tail dependence is demonstrated through an example. The figure showcases two randomly generated variables with a Kendall's Tau of 0.7. On the left side of Figure \@ref(fig:Fig162), the variables ($X$ and $Y$) are simulated using the bivariate normal distribution, while on the right side, they are generated using the bivariate $t$-distribution. Although both sides display a Kendall's Tau of 0.7, there is a difference in the upper right quadrant (above the dashed lines) on each panel. In the left panel, the values in the upper right quadrant (upper tails of $X$ and $Y$) are independent, while in the right panel, the upper tail values appear to be correlated (the upper right corners of the right panel contain 4 points). This suggests that the probability of $Y$ occurring above a high threshold (e.g., the dashed line in the figure) when $X$ exceeds the same threshold is higher in the right panel than in the left panel of Figure \@ref(fig:Fig162).



(ref:Fig162) **Left Panel: Upper tails of $X$ and $Y$ are independent. Right Panel: Upper tails of $X$ and $Y$ appear to be dependent.**

```{r Fig162, echo=FALSE, out.width='90%',fig.cap='(ref:Fig162)'}

library(copula)
tau_use<-0.7
theta.n<-iTau(normalCopula(), tau = tau_use)
theta.t<-iTau(tCopula(df=2), tau= tau_use)


##Samples from corresponding mvdc objects 
set.seed(271)
n<-400
N01marg<-list(list(mean=0.2, sd=0.7), list(mean=0.2, sd=0.7))
## For the normal copula 
h.n<-mvdc(normalCopula(theta.n), c("norm", "norm"), N01marg)
X.n<-rMvdc(n, mvdc = h.n)


#cor(X.n[,1],X.n[,2], method = "spearman")

## For the t copula 
h.t<-mvdc(tCopula(theta.t, df=2), c("norm", "norm"), N01marg)
X.t<-rMvdc(n, mvdc = h.t)

#cor(X.t[,1],X.t[,2],method = "spearman")

## Plotting information for scatter plots 
qstar<-0.03
q<-qnorm(c(qstar,1-qstar)) 
lim<-range(q, X.n, X.t)
#lim<-c(floor(lim[1]), ceiling(lim[2]))
lim<-c(floor(lim[1]), ceiling(lim[2]))

par(mfrow=c(1,2))
plot(X.n, xlim=lim, ylim=lim, xlab= "X", ylab="Y",cex=0.4,col = alpha(1, 0.6), ann = FALSE,xaxt='n',yaxt='n') 
abline(h=q[2], v=q[2], lty=2)
ll<-sum(apply(X.n<=q[1], 1, all))
ur<-sum(apply(X.n>=q[2], 1, all))
mtext(sprintf("upper right:%X", ur),cex=0.8, side = 1, line = -1.5)

mtext(side = 1, text = "X", line = 1.4,cex=0.9)
xtick<-c(-3, -2, -1, 0, 1, 2, 3)
xticklab<-c("-3","-2","-1", "0","1", "2","3")
axis(side=1, at=xtick, labels = FALSE)
text(x=xtick, -3.4,labels = xticklab,pos=1, xpd = TRUE, line=2)


mtext(side = 2, text = "Y", line = 1.4, cex=0.9)
ytick<-c(-3, -2, -1, 0, 1, 2, 3)
yticklab<-c("-3","-2","-1", "0","1", "2","3")
axis(side=2, at=ytick, labels = FALSE)
text(x=-3.8,y= ytick,labels = yticklab, xpd = TRUE, line=2)

plot(X.t, xlim=lim, ylim=lim, xlab= "X", ylab="Y",cex=0.4,col = alpha(1, 0.6), ann = FALSE,xaxt='n',yaxt='n') 
abline(h=q[2], v=q[2], lty=2)
ll<-sum(apply(X.t<=q[1], 1, all))
ur<-sum(apply(X.t>=q[2], 1, all))
mtext(sprintf("upper right:%X", ur),cex=0.8, side = 1, line = -1.5)


mtext(side = 1, text = "X", line = 1.4,cex=0.9)
xtick<-c(-3, -2, -1, 0, 1, 2, 3)
xticklab<-c("-3","-2","-1", "0","1", "2","3")
axis(side=1, at=xtick, labels = FALSE)
text(x=xtick, -3.4,labels = xticklab,pos=1, xpd = TRUE, line=2)


mtext(side = 2, text = "Y", line = 1.4, cex=0.9)
ytick<-c(-3, -2, -1, 0, 1, 2, 3)
yticklab<-c("-3","-2","-1", "0","1", "2","3")
axis(side=2, at=ytick, labels = FALSE)
text(x=-3.8,y= ytick,labels = yticklab, xpd = TRUE, line=2)

```



Consider a pair of random variables $(X,Y)$, from definitions provided in Joe (1997), the upper tail dependent coefficient denoted by $\lambda_{\text{up}}$ is given by:

$$
\lambda_\text{up} {=} \lim _{u \rightarrow 1^-} \mathrm{Pr}\left\{X>F_X^{-1}(u) \mid Y>F_Y^{-1}(u)\right\},
$$
in case the limit exists. Here, $F_X^{-1}(u)$ and $F_Y^{-1}(u)$ denote the quantiles of $X$ and $Y$ at the level $u$. Then, $X$ and $Y$ are said to be upper tail-dependent if $\lambda_\text{up}\in(0,1]$ and upper tail-independent if $\lambda_\text{up}=0$. When a variable reaches an extreme high value, the upper tail-dependent condition indicates that the other variable also reaches an extremely high value. On the other hand, the upper tail-independent suggests that the extreme values of the two variables are not related to each other. Similarly, the lower tail dependence coefficient, $\lambda_{\text{lo}}$, is defined as:
$$
\lambda_{\text{lo}} {=} \lim _{u \rightarrow 0^+} \mathrm{Pr}\left\{X \leq F_X^{-1}(u) \mid Y \leq F_Y^{-1}(u)\right\} .
$$
Let $R(X_j)$  and $R(Y_j)$ denote the rank of $X_j$ and $Y_j$, $j=1,\ldots, n$, respectively. From @Schmidt05, non-parametric estimates of $\lambda_\text{up}$ and $\lambda_\text{lo}$ are given by:
$$
\hat{\lambda}_{\text{lo}}=\frac{1}{k} \sum_{j=1}^n I\left\{R(X_j)\leq k, R(Y_j) \leq k\right\},
$$
and
$$
\begin{aligned}
\hat{\lambda}_{\text{up}} & =\frac{1}{k} \sum_{j=1}^n I\left\{R(X_j)>n-k, R(Y_j)>n-k\right\},
\end{aligned}
$$
where $k \in {1, . . .,n}$ is the threshold rank and a parameter to be chosen by the analyst, $k=k(n) \rightarrow \infty \text { and } k / n \rightarrow 0 \text { as } n \rightarrow \infty \text {, }$. Here, $n$ is the sample size, and $I\{\cdot\}$ takes the value of 1 if the condition is satisfied, and 0 otherwise. 

Figure \@ref(fig:Fig163) shows the scatter plot of the ranks of the ${\tt LOSS}$ variable and the ${\tt ALAE}$ variable. You can obtain the upper and lower tail dependent coefficient using the `tdc()` function from the `FRAPO` package in `R`. From below, $\hat{\lambda}_{\text{up}}=0.39$, at $k=75$ (note that $n=1500$), between the ${\tt LOSS}$ variable and the ${\tt ALAE}$ variable and $\hat{\lambda}_{\text{lo}}=0.13$. The results implies the losses and expenses variables appear to be more upper-tailed dependent. 

(ref:Fig163) **Scatter Plot of Ranks of LOSS and ALAE**

```{r Fig163, echo=FALSE,fig.height=3.9, fig.width=3.9,fig.cap='(ref:Fig163)'}
## Claim , Closed
#set.seed(10)
#summary(rank(Claim))
#summary(rank(Coverage))
#rankdat<-as.data.frame(cbind(rank(Claim),rank(Coverage)))
#colnames(rankdat)<-c("ClaimR","CoverageR")
#rankdat$l_up<-ifelse(rankdat$ClaimR>1139&rankdat$CoverageR>1139, 1,0)
#sum(rankdat$l_up)/4500
#table(rankdat$l_up)
#View(rankdat)
plot(rank(LOSS),rank(ALAE), ylab='LOSS',xlab='ALAE',xlim=c(0,1500),ylim=c(0,1500),main = "",cex=0.5,ann = FALSE,xaxt='n',yaxt='n',col = alpha(1, 0.4))
#abline(0,1)
#abline(h=1300)
#abline(v=1300)
abline(h=1425, v=1425, lty=2, col=adjustcolor("black", 0.6))
abline(h=75, v=75, lty=2, col=adjustcolor("black", 0.6))

title(list("", font = 2, cex=1), adj = 0.5, line = 0.5, font.main=2)
#summary(lnClaim10)
#summary(lnPremiumCAllT)
mtext(side = 1, text = "LOSS (ranks)", line = 1.5,cex=0.9)
xtick<-c(10, 300, 600, 900, 1200,1500)
#xtick<-c(0,1.386294,2.079442,2.484907,2.772589,2.995732,3.178054)
xticklab<-c("10","300","600","900", "1200","1500")
axis(side=1, at=xtick, labels = FALSE)
text(x=xtick, -100,labels = xticklab,pos=1, xpd = TRUE, line=2)


mtext(side = 2, text = "ALAE (ranks)", line = 2.5, cex=0.9)
ytick<-c(10, 300, 600, 900, 1200,1500)
yticklab<-c("10","300","600","900", "1200","1500")
axis(side=2, at=ytick, labels = FALSE)
text(x=-250,y= ytick,labels = yticklab, xpd = TRUE, line=2)

```



`r HideRCode('TDC.1',"R Code for Tail Dependence Coefficient")`

```{r echo=HtmlEval, eval=FALSE}
library(FRAPO)
Lambda_up<-tdc(as.data.frame(cbind(LOSS,ALAE)),lower = F, k=75)
round(Lambda_up,2)

Output:
    LOSS ALAE
LOSS 1.00 0.39
ALAE 0.39 1.00


Lambda_lo<-tdc(as.data.frame(cbind(LOSS,ALAE)),lower = T, k=75)
round(Lambda_lo,2)

Output:
     LOSS ALAE
LOSS 1.00 0.13
ALAE 0.13 1.00


```
</div>  


```{r echo=FALSE }
NoClaimCredit<-Insample$NoClaimCredit
Fire5<-Insample$Fire5
NoClaimCredit<-Insample$NoClaimCredit
EntityType<-ifelse(Insample$TypeCity==1,"City",
                   ifelse(Insample$TypeCounty==1,"County",
                          ifelse(Insample$TypeMisc==1,"Misc",
                                 ifelse(Insample$TypeSchool==1,"School",
                                        ifelse(Insample$TypeTown==1,"Town",
                                               ifelse(Insample$TypeVillage==1,"Village",0))))))

NoClaimCredit<-as.numeric(Insample$NoClaimCredit)
AlarmCredit<-as.numeric(ifelse(Insample$AC00==1,"1",
                   ifelse(Insample$AC05==1,"2",
                          ifelse(Insample$AC10==1,"3",
                                 ifelse(Insample$AC15==1,"4",0)))))

```


```{r child = './Quizzes/Quiz161.html', eval = QUIZ}
```


## Introduction to Copulas  {#S:Sec162}

***

In this section, you learn how to:

- Describe a multivariate distribution function in terms of a copula  function.

***

### Definition of a Copula

Copulas are widely used in insurance and many other fields to model the dependence among multivariate outcomes as they expresses the dependence between the variables explicitly. Recall that the joint cumulative distribution function ($cdf$) for two variables $Y_1$ and $Y_2$ is given by: 
$$
{F}(y_1, y_2)= {\Pr}(Y_1 \leq y_1, Y_2 \leq y_2) .
$$
For the multivariate case in $p$ dimensions, we have:
$$
{F}(y_1,\ldots, y_p)= {\Pr}(Y_1 \leq y_1,\ldots ,Y_p \leq y_p) .
$$
The joint distribution considers both the marginal distributions and how the variables are related to each other. However, it expresses this dependence implicitly. Copulas offer a different method that allows us to break down the joint distribution of variables into individual components (the marginal distributions and a copula) that can be adjusted separately.

A `r Gloss('copula')` is a multivariate distribution function with uniform marginals. Specifically, let $\{U_1, \ldots, U_p\}$ be $p$ uniform random variables on $(0,1)$. Their distribution function $${C}(u_1, \ldots, u_p) = \Pr(U_1 \leq u_1, \ldots, U_p \leq u_p),$$

is a copula. We seek to use copulas in applications that are based on more than just uniformly distributed data. Thus, consider arbitrary marginal distribution functions ${F}_1(y_1)$,...,${F}_p(y_p)$. Then, we can define a multivariate distribution function using the copula such that 
\begin{equation}
{F}(y_1, \ldots, y_p)= {C}({F}_1(y_1), \ldots, {F}_p(y_p)).
(\#eq:DFDefined)
\end{equation}
Here, $F$ is a multivariate distribution function, and the resulting value from the copula function is limited to a range of $[0,1]$ as it relates to probabilities. @sklar1959fonctions showed that $any$ multivariate distribution function $F$, can be written in the form of equation \@ref(eq:DFDefined), that is, using a copula representation.

Sklar also showed that, if the marginal distributions are continuous, then there is a unique copula representation. Hence, copulas can be used instead of joint distribution functions. In order to be considered valid, they must meet the necessary requirements of a valid joint cumulative distribution function. In this chapter we focus on copula modeling with  continuous variables. A copula $C$ is considered to be **absolutely continuous** if the **density** 
$$
c(u_1, \ldots, u_p)=\frac{\partial^p}{\partial_{u_p}\ldots \partial_{u_1}}C(u_1, \ldots, u_p),
$$
exists. For the discrete case, readers can see @joe2014dependence and @genest2007methods. For the bivariate case where $p=2$, we can write a copula and the distribution function of two random variables as 
$$
{C}(u_1, \, u_2) = \Pr(U_1 \leq u_1, \, U_2 \leq u_2)
$$
and
$$
{F}(y_1, \, y_2)= {C}({F}_1(y_1),  {F}_p(y_2)).
$$
One example of a bivariate copula is the product copula, also called the independence copula, as it captures the property of independence of the two variables $Y_1$ and $Y_2$. The copula (distribution function) is
$$
{F}(y_1, \, y_2)= {C}({F}_1(y_1),  {F}_p(y_2))={F}_1(y_1){F}_p(y_2)=u_1u_2=\Pi(u).
$$
In Figure \@ref(fig:Fig164), both the distribution function and scatter plot of observations generated from the independence copula are displayed. The scatter plot indicates that there is no correlation between the two components, $U_1$ and $U_2$.


(ref:Fig164) **Independence Copula.** Left: Scatterplot of observations from Independence Copula. Right: Plot for distribution function for Independence Copula.

```{r Fig164, echo=FALSE, fig.height=4, fig.width=8, fig.cap='(ref:Fig164)'}
par(mfrow=c(1,2))
d<-2
ic<-indepCopula(dim=d)
set.seed(100)
U<-rCopula(1000, copula = ic) # generate a sample
plot(U, xlab=quote(u[1]), ylab=quote(u[2]), ann = FALSE,xaxt='n',yaxt='n')

mtext(side = 1, text = quote(u[1]), line = 2.0,cex=0.9)
xtick<-c(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)
#xtick<-c(0,1.386294,2.079442,2.484907,2.772589,2.995732,3.178054)
xticklab<-c("0.0","0.2","0.4","0.6", "0.8","1.0")
axis(side=1, at=xtick, labels = FALSE)
text(x=xtick, -0.08,labels = xticklab,pos=1, xpd = TRUE, line=2)


mtext(side = 2, text = quote(u[2]), line = 2.5, cex=0.9)
ytick<-c(0.0, 0.2, 0.4, 0.6, 0.8,1.0)
yticklab<-c("0.0","0.2","0.4","0.6", "0.8","1.0")
axis(side=2, at=ytick, labels = FALSE)
text(x=-0.15,y= ytick,labels = yticklab, xpd = TRUE, line=2)


persp(ic, pCopula, zlab="C(u,v)", theta=50, cex.lab=0.5, cex.axis = 0.5 )
      #xlab = expression(u[1]),ylab = expression(u[2])


```


`r HideRCode('IndPlot.1',"R Code for Independence Copula Plots")`

```{r, echo=HtmlEval, ref.label = 'Fig164', eval=FALSE}
```

</div>



There is another type of copula that is frequently utilized, known as Frank's Copula [@frank1979simultaneous]. This copula can represent both positive and negative dependence and has a straightforward analytic structure. The copula (distribution function) is

\begin{equation}
{C}(u_1,u_2) = \frac{1}{\gamma} \log \left( 1+ \frac{ (\exp(\gamma
u_1) -1)(\exp(\gamma u_2) -1)} {\exp(\gamma) -1} \right).
(\#eq:FrankCopula)
\end{equation}

This is a bivariate distribution function with its domain on the unit square $[0,1]^2.$ Here $\gamma$ is the dependence parameter, that is, the range of dependence is controlled by the parameter $\gamma$. Positive association increases as $\gamma$ increases. As we will see, this positive association can be summarized with `r Gloss("Spearman's rho")` ($\rho_S$) and Kendall's tau ($\tau$). 


In Figure \@ref(fig:Fig165), we can see scatterplots of data generated from the Frank's copula. As $\gamma$ value changes, we observe that components $U_1$ and $U_2$ become positively or negatively dependent. When $\theta$ approaches 0, \@ref(eq:FrankCopula) transforms into an independence copula. Also,  Figure \@ref(fig:Fig166) provides the distribution and density functions for Frank's copula when $\gamma=12$. In Section \@ref(S:Sec164), we will explore copula functions other than the commonly used Frank's copula.

(ref:Fig165) **Scatterplot of Observations from Frank's Copula**. $\gamma=12$ (left), $\gamma=0$ (middle) and $\gamma=-12$ (right). 

```{r Fig165, echo=FALSE, fig.height=3.2, , fig.cap='(ref:Fig165)'}
par(mfrow=c(1,3))
d<-2
gamma_par<-12
fc<-frankCopula(gamma_par, dim=d)
set.seed(100)
n<-1000
U<-rCopula(n, copula = fc) # generate a sample with gamma=5
U0<-rCopula(n, copula = setTheta(fc,value = 0)) # generate a sample with gamma=0
U12<-rCopula(n, copula = setTheta(fc,value = -12)) # generate a sample with gamma=-5
plot(U, xlab=quote(U[1]), ylab=quote(U[2]))
plot(U0, xlab=quote(U[1]), ylab=quote(U[2]))
plot(U12, xlab=quote(U[1]), ylab=quote(U[2]))

```



(ref:Fig166) Left: Plot for distribution function for Frank's Copula ($\gamma=12$). Right: Plot for the density function for Frank's Copula ($\gamma=12$).

```{r Fig166, echo=FALSE, fig.cap='(ref:Fig166)', fig.height=4}
par(mfrow=c(1,2))
d<-2
gamma_par<-12
fc<-frankCopula(gamma_par, dim=d)
persp(fc, pCopula, zlab="C(u,v)", theta=50, cex.lab=0.5, cex.axis = 0.5)
persp(fc, dCopula, zlab="c(u,v)", theta=50, cex.lab=0.5, cex.axis = 0.5)
```

`r HideRCode('CopPlot.1',"R Code for Frank Copula Plots")`

```{r echo=HtmlEval, eval=FALSE}

## Scatterplots
par(mfrow=c(1,3))
d<-2
gamma_par<-12
fc<-frankCopula(gamma_par, dim=d)
set.seed(100)
n<-1000
U<-rCopula(n, copula = fc) # generate a sample with gamma=5
U0<-rCopula(n, copula = setTheta(fc,value = 0)) # generate a sample with gamma=0
U12<-rCopula(n, copula = setTheta(fc,value = -12)) # generate a sample with gamma=-5
plot(U, xlab=quote(U[1]), ylab=quote(U[2]))
plot(U0, xlab=quote(U[1]), ylab=quote(U[2]))
plot(U12, xlab=quote(U[1]), ylab=quote(U[2]))


## Distribution and density functions
par(mfrow=c(1,2))
d<-2
gamma_par<-12
fc<-frankCopula(gamma_par, dim=d)
persp(fc, pCopula, zlab="C(u,v)", theta=50, cex.lab=0.5, cex.axis = 0.5)
persp(fc, dCopula, zlab="c(u,v)", theta=50, cex.lab=0.5, cex.axis = 0.5)

```

</div>  


**Example 16.2.1. Copula Representation Example**

Suppose we have a variable $X$ that follows a Pareto distribution with a scale parameter of $\theta=10$ and a shape parameter of $\alpha=1.6$. Additionally, let $Y$ be an exponential variable with a mean value of 8. Write $F_{X,Y}(7.2, 4.1)$ in the form $C(u,v)$. 

Note: $F_{X,Y}(x,y)$  is the joint distribution function and $C(u,v)$ is the copula that links $X$ and $Y$. 


`r HideExample('16.2.1','Show Example Solution')`

`r SolnBegin()` Denote the marginal distribution functions of $X$ and $Y$ as $F_X(x)$ and $F_Y(y)$, respectively. Since $F_{X,Y}(7.2,4.1)=C[F_X(7.2),F_Y(4.1)]$, we can use the marginal distribution functions to obtain the arguments of the copula function:

For the Pareto variable, $X$:
$$
F_X(7.2)=1- \left(  \frac{10}{7.2+ 10} \right) ^{1.6} =0.58;
$$
For the Exponential variable, $Y$:
$$
F_Y(4.1)=1-e^{-0.125\times 4.1}=0.40.
$$
Hence, $F_{X,Y}(7.2,4.1)=C[0.58,0.40]$.

`r SolnEnd()`

</div>

*** 

### Sklarâ€™s Theorem


In @sklar1959fonctions, Sklar showcased how copulas can capture the dependence structure of a group of random variables. This principle has since been referred to as Sklar's Theorem and serves as the cornerstone of copula theory. Dependence modeling with copulas for continuous multivariate distributions allows for the separation of modeling the univariate marginals and the dependence structure, where a copula can represent the dependence structure.


1. For a $p$-variate distribution $F$, with marginal cumulative distribution functions $F_1, \ldots, F_p$, the copula associated with $F$ is a distribution function $C: [0,1]^p  \rightarrow [0,1]$ with $U(0,1)$ margins that satisfies:
\begin{equation}
{F}\left(\mathbf{y}\right)= {C}\left({F}_1\left(y_1\right), \ldots, {F}_p\left(y_p\right)\right), \quad \mathbf{y}=\{y_1 \ldots y_p\} \in \mathbb{R}^p.
(\#eq:DFDefined2)
\end{equation} 
If $F$ is a continuous $p$-variate distribution function with univariate margins $F_1, \ldots, F_p$ and quantile functions $F_1^{-1}, \ldots, F_p^{-1}$, then:
$$
C\left(\mathbf{u}\right) =F\left(F_1^{-1}\left(u_1\right), \ldots, F_p^{-1}\left(u_p\right)\right), \quad \mathbf{u} \in [0,1]^p ,
$$
is the unique choice. 

2. The converse also holds: If $C$ is a copula and $F_1, \ldots, F_p$ are univariate cumulative distribution functions, then the function $F$ defined by \@ref(eq:DFDefined2) is a joint cumulative distribution function with marginal cumulative distribution functions $F_1, \ldots, F_p$.



`r HideProofTheory('Copula.1',"Proof of Sklar's Theorem")`

***

`r LObjBegin()` $\textbf{Proof}$. Suppose we have $Y_1, \ldots, Y_p \sim F$, $U\sim \text{U}(0,1)$ and assume $Y_1, \ldots, Y_p$ is continuous, which implies $F_1^{-1}(U_i)=Y_i$. Then, the random variables have a multivariate distribution function $C$, given by: 
$$
\begin{array}{cl}
{C}\left(u_1, \ldots, u_p\right)&=\Pr\left( U_1 \leq u_1, \ldots, U_p \leq u_p\right)\\
&=\Pr\left( {F}_1\left(Y_1\right) \leq u_1, \ldots, {F}_p\left(Y_p\right) \leq u_p\right)\\
&=\Pr\left( Y_1 \leq {F}_1^{-1}(u_1), \ldots, Y_p \leq {F}_p^{-1}(u_p)\right)\\
&=F\left(F_1^{-1}\left(u_1\right), \ldots, F_p^{-1}\left(u_p\right)\right)\\
&=F\left(y_1, \ldots, y_p\right) .\\
\end{array}
$$
Hence: 
$$
{C}\left(F_1(y_1), \ldots, F_p(y_p)\right)=F\left(y_1, \ldots, y_p\right) .
$$
`r LObjEnd()`

***

</div>


According to the first part of Sklar's theorem, there is a unique underlying copula that is unknown, and it can be estimated from the data available. After estimating the margins and copula, they are usually combined using  \@ref(eq:DFDefined2) to give the estimated multivariate distribution function. Also, Sklar's theorem's second part enables the construction of adaptable multivariate distribution functions with specified univariate margins. These functions are useful in more intricate models, like pricing models.

```{r child = './Quizzes/Quiz162.html', eval = QUIZ}
```



## Application Using Copulas  {#S:Sec163}

***

In this section, you learn how to:

- Discover dependence structure between random variables
- Model the dependence with a copula function

***

This section analyzes the insurance losses and expenses data with the statistical program `R`. The data set is visualized in Figure \@ref(fig:Fig161). The model fitting process is started by marginal modeling of each of the two variables, ${\tt LOSS}$ and ${\tt ALAE}$. Then we model the joint distribution of these marginal outcomes.


### Marginal Models

We first examine the `r Gloss('marginal distributions')` of losses and expenses before going through the joint modeling. The histograms show that both  ${\tt LOSS}$ and ${\tt ALAE}$ are right-skewed and `r Gloss("fat-tailed")`. Because of these features, for both marginal distributions of losses and expenses, we consider a Pareto distribution, distribution function of the form
$$
F(y)=1- \left(  \frac{\theta}{y+ \theta} \right) ^{\alpha}.
$$
Here, $\theta$ is a scale parameter and $\alpha$ is a shape parameter. Section \@ref(S:ContinuousDistributions) provides details of this distribution.

The marginal distributions of losses and expenses are fit using the method of maximum likelihood. Specifically, we use the `vglm` function from the `R VGAM` package. Firstly, we fit the marginal distribution of ${\tt ALAE}$. Parameters are summarized in [Table 16.6](#tab:16.6).

```{r include=FALSE}
library(VGAM)
fit.ALAE <- vglm(ALAE ~ 1, paretoII(location=0, lscale="loge", lshape="loge")) # fit the model by vglm function
#coef(fit.ALAE, matrix=TRUE)
```

`r HideRCode('Reg.1',"R Code for Pareto Fitting of ALAE")`

```{r echo=HtmlEval, eval=FALSE}
library(VGAM)
fit.ALAE <- vglm(ALAE ~ 1, paretoII(location=0, lscale="loge", lshape="loge")) # fit the model by vlgm function
coef(fit.ALAE, matrix=TRUE) # extract fitted model coefficients, matrix=TRUE gives logarithm of estimated parameters instead of default normal scale estimates

Output: 
               loge(scale) loge(shape)
 (Intercept)     9.624673   0.7988753
 
                  scale        shape 
 (Intercept)  15133.603598     2.223039 
```
</div>  


We repeat this procedure to fit the marginal distribution of the ${\tt LOSS}$ variable. Because the loss variable also seems right-skewed and heavy-tailed data, we also model the marginal distribution with the Pareto distribution (although with different parameters). 

```{r include=FALSE}
fit.LOSS <- vglm(LOSS ~ 1, paretoII, trace=TRUE)
```

`r HideRCode('ParFit.1',"R Code for Pareto Fitting of LOSS")`

```{r echo=HtmlEval, eval=FALSE}
fit.LOSS <- vglm(LOSS ~ 1, paretoII, trace=TRUE)
Coef(fit.LOSS)

Output: 
       scale        shape 
16228.14797     1.23766

```
</div>  

<a id=tab:16.6></a>

[Table 16.6]: ./ChapDependenceModel.html#tab:16.6

**[Table 16.6]{#tab:16.6}. Summary of Pareto Maximum Likelihood Fitted Parameters from the LGPIF Data**


$$
{\small \begin{matrix}
\begin{array}{l|r|r} 
    \hline
 & \text{Shape } \hat{\theta}     &\text{Scale }    \hat{\alpha}     \\
  \hline
\text{ALAE}  & 15133.60360  &  2.22304    \\
\text{LOSS}  &    16228.14797  &  1.23766 \\
   \hline
\end{array}
\end{matrix}}
$$
To visualize the fitted distribution of  ${\tt LOSS}$ and ${\tt ALAE}$ variables, one can use the estimated parameters and plot the corresponding distribution function and density function. For more details on the selection of marginal models, see Chapter \@ref(ChapModelSelection).


### Probability Integral Transformation {#S:Sec1632}

When studying simulation, in Section \@ref(S:Sec812) we learned about the `r Gloss('inverse transform method')`. This is a way of mapping a $U(0,1)$ random variable into a random variable $X$ with distribution function $F$ via the inverse of the distribution, that is, $X = F^{-1}(U)$. The `r Gloss('probability integral transformation')` goes in the other direction, it states that $F(X) = U$. Although the inverse transform result is available when the underlying random variable is continuous, discrete or a hybrid combination of the two, the probability integral transform is mainly useful when the distribution is continuous. That is the focus of this chapter.

We use the probability integral transform for two purposes: (1) for diagnostic purposes, to check that we have correctly specified a distribution function and (2) as an input into the copula function in equation \@ref(eq:DFDefined).

For the first purpose, we can check to see whether the Pareto is a reasonable distribution to model our marginal distributions. Given the fitted Pareto distribution, the variable ${\tt ALAE}$ is transformed to the variable $u_1$, which follows a uniform distribution on $[0,1]$:
$$
u_1 = \hat{F}_{1}(ALAE) = 1 - \left( \frac{\hat{\theta}}{\hat{\theta}+ALAE} \right)^{\hat{\alpha}}.
$$

```{r echo=FALSE}
theta.ALAE = Coef(fit.ALAE)[1]
alpha.ALAE = Coef(fit.ALAE)[2]
u1 = 1 - (1 + (ALAE/theta.ALAE))^(-alpha.ALAE) # or u1=pparetoII(ALAE, location=0, scale=theta, shape=alpha)
```

After applying the probability integral transformation to  the ${\tt ALAE}$ variable, we plot the histogram of *Transformed* ${\tt ALAE}$ in Figure \@ref(fig:Fig167). This plot appears reasonably close to what we expect to see with a uniform distribution, suggesting that the Pareto distribution is a reasonable specification.

(ref:Fig167) **Histogram of Transformed ALAE**

```{r Fig167, out.width='60%', echo=FALSE, fig.cap='(ref:Fig167)'}
hist(u1, main="", xlab="" )
```

In the same way, the variable ${\tt LOSS}$ is also transformed to the variable $u_2$, which follows a uniform distribution on $[0,1]$. The left-hand panel of Figure \@ref(fig:Fig168) shows a plot the histogram of *Transformed* ${\tt ALAE}$, again reinforcing the Pareto distribution specification. 
For another way of looking at the data, the variable $u_2$ can be transformed to a *normal score* with the quantile function of standard normal distribution. As we see in Figure \@ref(fig:Fig168), normal scores of the variable ${\tt LOSS}$ are approximately marginally standard normal. This figure is helpful because analysts are used to looking for patterns of approximate normality (which seems to be evident in the figure). The logic is that, if the Pareto distribution is correctly specified, then transformed losses $u_2$ should be approximately normal, and the normal scores $\Phi^{-1}(u_2)$, should be approximately normal. (Here, $\Phi$ is the cumulative standard normal distribution function.)

(ref:Fig168) **Histogram of Transformed Loss.** The left-hand panel shows the distribution of probability integral transformed losses. The right-hand panel shows the distribution for the corresponding normal scores.

```{r Fig168, echo=FALSE, out.width='80%',fig.cap='(ref:Fig168)'}
theta.LOSS = Coef(fit.LOSS)[1]
alpha.LOSS = Coef(fit.LOSS)[2]
u2 = 1 - (1 + (LOSS/theta.LOSS))^(-alpha.LOSS)
par(mfrow=c(1, 2))
hist(u2, main="", xlab="")
hist(qnorm(u2), main="", xlab="")
```

`r HideRCode('transalae.1',"R Code for Histograms of Transformed Variables")`

```{r, echo=HtmlEval, ref.label = 'Fig168', eval=FALSE}
```

</div>  

### Joint Modeling with Copula Function

Before jointly modeling losses and expenses, we draw the scatterplot of transformed variables $(U_1, U_2)$ and the scatterplot of normal scores in Figure \@ref(fig:Fig169). The left-hand panel is a plot of $U_1$ versus $U_2$, where $U_1 = \hat{F}_1(ALAE)$ and $U_2=\hat{F}_2(LOSS)$). Then we transform each one using an inverse standard normal distribution function, $\Phi^{-1}(\cdot)$, or `qnorm` in `R` to get normal scores.  As in Figure \@ref(fig:Fig161), it is difficult to see patterns in the left-hand panel. However, with rescaling, patterns are evident in the right-hand panel. To learn more details about normal scores and their applications in copula modeling, see @joe2014dependence. 

(ref:Fig169) Left: Scatter plot for transformed variables.  Right:Scatter plot for normal scores

```{r Fig169, echo=FALSE, out.width='80%', fig.cap='(ref:Fig169)'}
par(mfrow = c(1, 2))
plot(u1, u2, cex = 0.5, xlim = c(-0.1,1.1), ylim = c(-0.1,1.1),
     xlab = "Transformed ALAE", ylab = "Transformed LOSS")
plot(qnorm(u1), qnorm(u2), xlab = expression("qnorm"(u[1])) , ylab =expression("qnorm"(u[2])) )
```

`r HideRCode('Cor.1',"R Code for Scatter Plots and Correlation")`

```{r, echo=HtmlEval, ref.label = 'Fig169', eval=FALSE}
```

```{r echo=HtmlEval, eval=FALSE}
cor(u1, u2, method = "spearman")
Output: 

[1] 0.451872
```

</div>  

The right-hand panel of Figure \@ref(fig:Fig161) shows us there is a positive dependency between these two random variables. This can be summarized using, for example, Spearman's rho  that turns out to be 0.451. As we learned in Section \@ref(S:Sec1612), this statistic depends only on the order of the two variables through their respective ranks. Therefore, the statistic is the same for (1) the original data in Figure \@ref(fig:Fig161), (2) the data transformed to uniform scales in the left-hand panel of Figure \@ref(fig:Fig169), and (3) the normal scores in the right-hand panel of Figure \@ref(fig:Fig169).
 
The next step is to calculate estimates of the copula parameters. One option is to use traditional maximum likelihood and determine all the parameters at the same time which can be computationally burdensome. Even in our simple example, this means maximizing a (log) likelihood function over five parameters, two for the marginal ${\tt ALAE}$ distribution, two for the marginal ${\tt LOSS}$ distribution, and one for the copula. A widely alternative, known as the *inference for margins (IFM)* approach, is to simply use the fitted marginal distributions, $u_1$ and $u_2$, as inputs when determining the copula. This is the approach taken here. In the following code, you will see that the fitted copula parameter becomes $\hat{\gamma} = 3.114$.


```{r echo=FALSE}
# transformed data matrix uu
uu = cbind(u1,u2) 
# Frank's copula object with arbitrary parameter value
frank.cop <- archmCopula("frank", param= c(5), dim = 2) 
# fit copula using ML method, starting value 0.4
fit.ml <- fitCopula(frank.cop, uu, method="ml", start=c(0.4))
#summary(fit.ml)

```

`r HideRCode('FrankCopula.1',"R Code for IFM Fitting with Frank's Copula")`

```{r echo=HtmlEval, eval=FALSE}
# transformed data matrix uu
uu = cbind(u1,u2) 
# Frank's copula object with arbitrary parameter value
frank.cop <- archmCopula("frank", param= c(5), dim = 2) 
# fit copula using ML method, starting value 0.4
fit.ml <- fitCopula(frank.cop, uu, method="ml", start=c(0.4))

Call: fitCopula(copula, data = data, method = "ml", start = ..2)
Fit based on "maximum likelihood" and 1500 2-dimensional observations.
Frank copula, dim. d = 2 
      Estimate Std. Error
alpha    3.114      0.169
The maximized loglikelihood is 172.6 

```
</div>  

To visualize the fitted Frank's copula, the distribution function and density function perspective plots are drawn in Figure \@ref(fig:Fig1610).

(ref:Fig1610) **Frank's Copula**. Left: Plot for distribution function for Frank's Copula. Right:Plot for density function for Frank's Copula

```{r Fig1610, echo=FALSE, fig.cap='(ref:Fig1610)', fig.height=4}
#par(mfrow=c(1,2))
frank.cop <- archmCopula("frank", param= c(3.114), dim = 2) 
par(mar=c(3.2,3,.2,.2),mfrow=c(1,2))
persp(frank.cop, pCopula, theta=50, zlab="C(u,v)", xlab ="u", ylab="v", cex.lab=0.5, cex.axis = 0.5)
persp(frank.cop, dCopula, theta=50, zlab="c(u,v)",  xlab ="u", ylab="v", cex.lab=0.5, cex.axis = 0.5)

```


`r HideRCode('DistriPlot.1',"R Code for Frank's Copula Plots")`

```{r, echo=HtmlEval, ref.label = 'Fig1610', eval=FALSE}
```

</div>


We can estimate the anticipated expenses when losses surpass a specific threshold by utilizing the fitted Frank copula based on the data on losses and expenses. For instance, according to the data, the mean expense when losses exceed $\$200,000$ is $\$58,807$. However, when we apply the fitted Frank copula, the projected expenses when losses exceed $\$200,000$ is $\$26,767$. This suggests that the Frank copula doesn't provide an accurate estimate and may not be suitable for this dataset. We will now explore other copula types.


`r HideRCode('ExpectedExp.1','R Code for using the fitted Frank copula to calculate the expected level of expenses')`

```{r echo=HtmlEval, eval=FALSE}
library(copula)
library(VGAM)
## From data
lossData_Ori<-as.data.frame(cbind(loss$alae,loss$loss))
colnames(lossData_Ori)<-c("alae","loss")
Threshold<-200000
sum(lossData_Ori$alae*(lossData_Ori$loss>Threshold))/sum((lossData_Ori$loss>Threshold)) 

Output:
[1] 58806.75


### Using fitted Frank copula
set.seed(2010)
nSim <- 10000
frank.cop <- archmCopula("frank", param= c(3.114), dim = 2)
mcc <- mvdc(frank.cop, margins = c("paretoII","paretoII"), 
            paramMargins = list(list(scale = theta.ALAE, shape=alpha.ALAE),
                                list(scale = theta.LOSS, shape=alpha.LOSS)))

LossData_Sim <- as.data.frame(rMvdc(nSim, mvdc = mcc))
Threshold<-200000
sum(LossData_Sim$alae*(LossData_Sim$loss>Threshold))/sum((LossData_Sim$loss>Threshold)) 


Output:
[1] 26766.84
```
</div> 


## Types of Copulas {#S:Sec164}

***

In this section, you learn how to:

-  Define the basic types of elliptical copulas, including the normal, $t$
-  Define basic types of Archimedean copulas


***

There are several families of copulas that have been described in the literature. Two main families of the copula families are the **Archimedean** and **Elliptical** copulas.

### Normal (Gaussian) Copulas

We started our study with Frank's copula in equation \@ref(eq:FrankCopula) because it can capture both positive and negative dependence and has a readily understood analytic form. However, extensions to multivariate cases where $p>2$ are not easy and so we look to alternatives. In particular, the normal, or Gaussian, distribution has been used for many years in empirical work, starting with Gauss in 1887. So, it is natural to turn to this distribution as a benchmark for understanding multivariate dependencies.

For a multivariate normal distribution, think of $p$ normal random variables, each with mean zero and standard deviation one. Their dependence is controlled by $\boldsymbol \Sigma$, a `r Gloss('correlation matrix')`, with ones on the diagonal. The number in the $i$th row and $j$th column, say $\boldsymbol \Sigma_{ij}$, gives the correlation between the $i$th and $j$th normal random variables. This collection of random variables has a multivariate normal distribution with probability density function
\begin{equation}
\phi_N (\mathbf{z})= \frac{1}{(2 \pi)^{p/2}\sqrt{\det \boldsymbol \Sigma}}
\exp\left( -\frac{1}{2} \mathbf{z}^{\prime} \boldsymbol
\Sigma^{-1}\mathbf{z}\right).
(\#eq:Normalpdf)
\end{equation}
To develop the corresponding copula version, it is possible to start with equation \@ref(eq:DFDefined), evaluate this using normal variables, and go through a bit of calculus. Instead, we simply state as a definition, the normal (Gaussian) **copula** density function is
$$
c_N(u_1,  \ldots, u_p) = \phi_N \left(\Phi^{-1}(u_1), \ldots, \Phi^{-1}(u_p) \right) \prod_{j=1}^p \frac{1}{\phi(\Phi^{-1}(u_j))}.
$$
Here, we use $\Phi$ and $\phi$ to denote the standard normal distribution and density functions. Unlike the usual probability density function $\phi_N$, the copula density function has its domain on the hyper-cube $[0,1]^p$. For contrast, Figure \@ref(fig:Fig1611) compares these two density functions.

(ref:Fig1611) **Bivariate Normal Probability Density Function Plots.** The left-hand panel is a traditional bivariate normal probability density function. The right-hand plot is a plot of the copula density for the normal distribution.



```{r Fig1611, echo=FALSE, fig.cap='(ref:Fig1611)', fig.height= 3.2}
par(mfrow=c(1, 2))
#  BIVARIATE NORMAL DF WITH RHO = 0.25
norm.pdf <- mvdc(normalCopula(0.25), c("norm", "norm"),
          list(list(mean = 0, sd =1), list(mean = 0, sd =1)))
persp(norm.pdf, dMvdc, xlim = c(-3, 3), ylim=c(-3,3),   
        zlab = expression("\U03A6"), 
        xlab ="x", ylab="y", cex.lab=0.8, cex.axis = 0.3)

#  BIVARIATE NORMAL COPULA WITH RHO = 0.25
norm.cop <- ellipCopula("normal", param = c(0.25),dim = 2, dispstr = "un")
persp(norm.cop, dCopula, theta = 30, zlab="c(u,v)",
        xlab ="u", ylab="v", cex.lab=0.8, cex.axis = 0.3)

```


`r HideRCode('DistriPlot.2',"R Code for Normal pdf and Normal Copula pdf Plots")`

```{r, echo=HtmlEval, ref.label = 'Fig1611', eval=FALSE}
```

</div>


### *t*- and Elliptical Copulas

Another copula used widely in practice is the $t$- copula. Both the $t$- and the normal copula are special cases of a family known as *elliptical* copulas, so we introduce this general family first, then specialize to the case of the $t$- copula.

The normal and the $t$- distributions are examples of symmetric distributions. More generally, `r Gloss('elliptical distributions')` is a class of distributions that are symmetric and can be multivariate. In short, an elliptical distribution is a type of symmetric, multivariate distribution. The multivariate normal and multivariate $t$- are special types of elliptical distributions.

`r Gloss('Elliptical copulas')` are constructed from elliptical distributions. This copula decomposes a (multivariate) elliptical distribution into their univariate elliptical marginal distributions by Sklar's theorem. Properties of elliptical copulas can be obtained from the properties of the corresponding elliptical distributions, see for example, @hofertelements.

In general, a $p$-dimensional vector of random variables has an *elliptical distribution* if the density can be written as
$$
h_E (\mathbf{z})= \frac{k_p}{\sqrt{\det \boldsymbol \Sigma}}
g_p \left( \frac{1}{2} (\mathbf{z}- \boldsymbol \mu)^{\prime}
\boldsymbol \Sigma^{-1}(\mathbf{z}- \boldsymbol \mu) \right) ,
$$
for $\mathbf{z} \in R^p$ and $k_p$ is a constant, determined so the density integrates to one. The function $g_p(\cdot)$ is called a *generator* because it can be used to produce different distributions. [Table 16.7](#tab:16.7) summarizes a few choices used in actuarial practice. The choice $g_p(x) = \exp(-x)$ gives rises to the normal *pdf* in equation \@ref(eq:Normalpdf). The choice $g_p(x) = \exp(-(1+2x/r)^{-(p+r)/2})$ gives rise to a multivariate $t$- distribution with $r$ degrees of freedom with *pdf*
$$
h_{t_r} (\mathbf{z})= \frac{k_p}{\sqrt{\det \boldsymbol \Sigma}}
\exp\left[- \left( 1+ \frac{(\mathbf{z}- \boldsymbol \mu)^{\prime} \boldsymbol \Sigma^{-1}(\mathbf{z}- \boldsymbol \mu)}{r} \right)^{-(p+r)/2}\right] .
$$

<a id=tab:16.7></a>

[Table 16.7]: ./ChapDependenceModel.html#tab:16.7

**[Table 16.7]{#tab:16.7}. Generator Functions ($g_p(\cdot)$) for Selected Elliptical Distributions**


$$
\small\begin{array}{lc}
\hline & Generator \\
 Distribution &  g_p(x)  \\
\hline
 \text{Normal distribution} &  e^{-x}\\
 t-\text{distribution with }r \text{ degrees of freedom} &   (1+2x/r)^{-(p+r)/2}\\
 \text{Cauchy} &  (1+2x)^{-(p+1)/2}\\
\text{Logistic} &  e^{-x}/(1+e^{-x})^2\\
 \text{Exponential power} &   \exp(-rx^s)\\
\hline
\end{array}
$$


We can use elliptical distributions to generate copulas. Because copulas are concerned primarily with relationships, we may restrict our considerations to the case where $\mu = \mathbf{0}$ and  $\boldsymbol \Sigma$ is a correlation matrix. With these restrictions, the marginal distributions of the multivariate elliptical copula are identical; we use $H$ to refer to this marginal distribution function and $h$ is the corresponding density. This marginal density is $h(z) = k_1 g_1(z^2/2).$ For example, in the normal case we have $H(\cdot)=\Phi(\cdot)$ and $h(\cdot)=\phi(\cdot)$.

We are now ready to define the *pdf* of the *elliptical copula*, a function defined on the unit cube $[0,1]^p$  as

$$
{c}_E(u_1,  \ldots, u_p) = h_E \left(H^{-1}(u_1), \ldots,
H^{-1}(u_p) \right) \prod_{j=1}^p \frac{1}{h(H^{-1}(u_j))}.
$$

As noted above, most empirical work focuses on the normal copula and $t$-copula. Specifically, $t$-copulas are useful for modeling  the dependency in the tails of bivariate distributions, especially in financial risk analysis applications. The $t$-copulas with same  association parameter in varying the degrees of freedom parameter show us different tail dependency structures. For more information about $t$-copulas, readers can see @joe2014dependence and @hofertelements.



We used the same approach as with the fitted Frank copula to fit the Normal and $t$ copula. The `R` code below fits the Normal and $t$ copula and estimates the expected level of expenses when losses exceed $\$200,000$. The results show that the estimated expenses using the fitted Normal copula when losses exceed $\$200,000$ is $\$35,411$. However, this is not a good fit compared to the mean expense of $\$58,807$ from the losses and expenses data. When losses exceed $\$200,000$, the fitted $t$ copula estimates expenses to be $\$47,354$, making it a better fit than the Normal copula.



`r HideRCode('ExpectedExp.2','R Code for using the fitted Normal and t copula to calculate the expected level of expenses')`

```{r echo=HtmlEval, eval=FALSE}

## Fit Gaussian Copula and Calculate Expenses. 
uu = cbind(u1,u2) 
# Normal copula object with arbitrary parameter value
normal.cop <- ellipCopula("normal", param=c(0.04), dim = 2)
# fit copula using ML method, starting value 0.4
fit.mlnc <- fitCopula(normal.cop, uu, method="ml", start=c(0.4))

normal.cop <- ellipCopula("normal", param=c(0.4783), dim = 2)

set.seed(2010)
nSim <- 10000
normal.cop <- ellipCopula("normal", param=c(0.4783), dim = 2)
mcc <- mvdc(normal.cop, margins = c("paretoII","paretoII"), 
            paramMargins = list(list(scale = theta.ALAE, shape=alpha.ALAE),
                                list(scale = theta.LOSS, shape=alpha.LOSS)))

LossData_Sim <- as.data.frame(rMvdc(nSim, mvdc = mcc))
colnames(LossData_Sim)<-c("alae","loss")
Threshold<-200000
sum(LossData_Sim$alae*(LossData_Sim$loss>Threshold))/sum((LossData_Sim$loss>Threshold)) 


Output:
[1]  35410.6


## Fit t Copula and Calculate Expenses. 
uu = cbind(u1,u2) 
# t copula object with arbitrary parameter value
t.cop <- ellipCopula("t", param=c(0.04), dim = 2)
# fit copula using ML method, starting value 0.4
fit.mlt <- fitCopula(t.cop, uu, method="ml", start=c(0.4,2))


t.cop <- ellipCopula("t", param=c(0.4816), dim = 2,df = 9.6474)

set.seed(2010)
nSim <- 10000
t.cop <- ellipCopula("t", param=c(0.4816), dim = 2,df = 9.6474)
mcc <- mvdc(t.cop, margins = c("paretoII","paretoII"), 
            paramMargins = list(list(scale = theta.ALAE, shape=alpha.ALAE),
                                list(scale = theta.LOSS, shape=alpha.LOSS)))

LossData_Sim <- as.data.frame(rMvdc(nSim, mvdc = mcc))
colnames(LossData_Sim)<-c("alae","loss")
Threshold<-200000
sum(LossData_Sim$alae*(LossData_Sim$loss>Threshold))/sum((LossData_Sim$loss>Threshold)) 


Output:
[1]  47353.78
```

</div> 





### Archimedean Copulas

This class of copulas is also constructed from a *generator* function. For Archimedean copulas, we assume that $g(\cdot)$ is a convex, decreasing function with domain [0,1] and range $[0, \infty)$ such that $g(0)=0$. Use $g^{-1}$ for the inverse function of $g$. Then the function

$$
C_g(u_1, \ldots, u_p) = g^{-1} \left(g(u_1)+ \cdots + g(u_p) \right)
$$

is said to be an *Archimedean* copula distribution function. 

For the bivariate case, $p=2$, an Archimedean copula function can be written by the function

$$
C_{g}(u_1, \, u_2) = g^{-1} \left(g(u_1) + g(u_2) \right).
$$

Some important special cases of Archimedean copulas include the Frank, Clayton/Cook-Johnson, and Gumbel/Hougaard copulas. Each copula class is derived from different generator functions. As another useful special case, recall the Frank's copula described in Sections \@ref(S:Sec162) and  \@ref(S:Sec163). To illustrate, we now provide explicit expressions for the Clayton and Gumbel/Hougaard copulas. 

#### Clayton Copula {-}

For $p=2$, the Clayton copula with parameter $\gamma \in [-1,\infty)$ is defined by

$$
C_{\gamma}^C(u)=\max\{u_1^{-\gamma}+u_2^{-\gamma}-1,0\}^{1/\gamma}, \quad u \in [0,1]^2.
$$

This is a bivariate distribution function defined on the unit square $[0,1]^2.$ The range of dependence is controlled by the parameter $\gamma$, similar to Frank's copula.

#### Gumbel-Hougaard Copula {-}

The Gumbel-Hougaard copula is parametrized by $\gamma \in [1,\infty)$ and defined by 

$$
C_{\gamma}^{GH}(u)=\exp\left(-\left(\sum_{i=1}^2 (-\log u_i)^{\gamma}\right)^{1/\gamma}\right), \quad u\in[0,1]^2.
$$

For more information on Archimedean copulas, see @joe2014dependence, @frees1998understanding, and @genest1986bivariate.

We used the same approach to fit the Clayton and Gumbel-Hougaard copulas as we did for the fitted Frank copula. The `R` code below fits these two copulas and determines the expected expense level for losses higher than $\$200,000$. Our analysis shows that the estimated expenses for losses exceeding $\$200,000$ using the fitted Clayton copula are $\$14,209$, while the fitted Gumbel-Hougaard copula predicts $\$58,554$. Of all the copula types considered, the Gumbel-Hougaard copula provides the best fit for this data. For more on Goodness-of-fit tests, see @hofertelements. 



`r HideRCode('ExpectedExp.3','R Code for using the fitted Clayton and Gumbel-Hougaard copula to calculate the expected level of expenses')`

```{r echo=HtmlEval, eval=FALSE}
## Fit Clayton Copula and Calculate Expenses. 
uu = cbind(u1,u2) 
# Clayton's copula object with arbitrary parameter value
clayton.cop <- archmCopula("clayton", param= c(5), dim = 2) 
# fit copula using ML method, starting value 0.4
fitclayton.ml <- fitCopula(clayton.cop, uu, method="ml", start=c(0.4))
summary(fitclayton.ml)

clayton.cop <- archmCopula("clayton", param= c(0.5678), dim = 2) 

set.seed(2010)
nSim <- 10000
clayton.cop <- archmCopula("clayton", param= c(0.5678), dim = 2) 
mcc <- mvdc(clayton.cop, margins = c("paretoII","paretoII"), 
            paramMargins = list(list(scale = theta.ALAE, shape=alpha.ALAE),
                                list(scale = theta.LOSS, shape=alpha.LOSS)))

LossData_Sim <- as.data.frame(rMvdc(nSim, mvdc = mcc))
colnames(LossData_Sim)<-c("alae","loss")
Threshold<-200000
sum(LossData_Sim$alae*(LossData_Sim$loss>Threshold))/sum((LossData_Sim$loss>Threshold)) 


Output:
[1]  14208.16

## Fit Gumbel-Hougaard Copula and Calculate Expenses. 
uu = cbind(u1,u2) 
# gumbel's copula object with arbitrary parameter value
gumbel.cop <- archmCopula("gumbel", param= c(5), dim = 2) 
# fit copula using ML method, starting value 0.4
fitgumbel.ml <- fitCopula(gumbel.cop, uu, method="ml", start=c(0.4))
summary(fitgumbel.ml)

gumbel.cop <- archmCopula("gumbel", param= c(1.444), dim = 2) 

set.seed(2010)
nSim <- 10000
gumbel.cop <- archmCopula("gumbel", param= c(1.444), dim = 2) 
mcc <- mvdc(gumbel.cop, margins = c("paretoII","paretoII"), 
            paramMargins = list(list(scale = theta.ALAE, shape=alpha.ALAE),
                                list(scale = theta.LOSS, shape=alpha.LOSS)))

LossData_Sim <- as.data.frame(rMvdc(nSim, mvdc = mcc))
colnames(LossData_Sim)<-c("alae","loss")
Threshold<-200000
sum(LossData_Sim$alae*(LossData_Sim$loss>Threshold))/sum((LossData_Sim$loss>Threshold)) 


Output:
[1]  58554.37

```

</div> 


*** 

```{r child = './Quizzes/Quiz164.html', eval = QUIZ}
```



## Properties of Copulas

***

In this section, you learn how to:

-   Interpret bounds that limit copula distribution functions as the amount of dependence varies
-  Calculate measures of association for different copulas and interpret their properties
-  Interpret tail dependency for different copulas

***

With many choices of copulas available, it is helpful for analysts to understand general features of how these alternatives behave.

### Bounds on Association {#S:Sec1651}

Any distribution function is bounded below by zero and from above by one. Additional types of bounds are available in multivariate contexts. These bounds are useful when studying dependencies. That is, as an analyst thinks about variables as being extremely dependent, one has available bounds that cannot be exceeded, regardless of the dependence. The most widely used bounds in dependence modeling are known as the *Fr&eacute;chet-H&ouml;effding* bounds, given as

$$
\max( u_1 +\cdots+ u_p - p +1, 0) \leq  C(u_1,  \ldots, u_p) \leq \min (u_1,  \ldots,u_p).
$$

To see the right-hand side of this equation, note that 

$$
C(u_1,\ldots, u_p) = \Pr(U_1 \leq u_1, \ldots, U_p \leq u_p) \leq  \Pr(U_j \leq u_j),
$$

for $j=1,\ldots,p$. The bound is achieved when $U_1 = \cdots = U_p$. To see the left-hand side when $p=2$, consider $U_2=1-U_1$. In this case, if $1-u_2 < u_1$ then 

$$
\Pr(U_1 \leq u_1, U_2 \leq u_2) = \Pr ( 1-u_2 \leq U_1 < u_1) =u_1+u_2-1.
$$
See, for example, @nelsen1997introduction for additional discussion. 

To see how these bounds relate to the concept of dependence, consider the case of $p=2$. As a benchmark, first note that the product copula,  $C(u_1,u_2)=u_1 \cdot u_2$, is the result of assuming independence between random variables. Now, from the above discussion, we see that the lower bound is achieved when the two random variables are perfectly negatively related ($U_2=1-U_1$). Further, it is clear that the upper bound is achieved when they are perfectly positively related ($U_2=U_1$). To emphasize this, the `r Gloss('Frechet-Hoeffding bounds')` for two random variables appear in Figure \@ref(fig:Fig1612).

(ref:Fig1612) **Perfect Positive and Perfect Negative Dependence Plots**

```{r Fig1612, out.width='60%', echo=FALSE, fig.cap='(ref:Fig1612)'}
set.seed(1980)
U <- runif(100)
par(mfrow=c(1, 2))
plot(cbind(U,1-U), xlab=quote(U[1]), ylab=quote(U[2]),main="Perfect Negative Dependency") # W for p=2
plot (cbind(U,U), xlab=quote(U[1]),ylab=quote(U[2]),main="Perfect Positive Dependency")  #M for p=2
```

`r HideRCode('plot.1',"R Code for the Fr&eacute;chet-H&ouml;effding Bounds for Two Random Variables")`

```{r, echo=HtmlEval, ref.label = 'Fig1612', eval=FALSE}
```

</div>  


Let's assign the Fr&eacute;chet-H&ouml;effding lower bound as $W$ and the upper bound as $M$. That is, $W=\max(u_1+\cdots+u_p-p+1, 0)$ and $M=\min(u_1,\ldots,u_p)$. It's important to note that $W$ is a copula only if $p=2$, while $M$ is a copula for all $p\geq2$. In dimension two, $W=\max(u_1+u_2-1, 0)$ is known as the **counter-monotonic copula**. It captures the inverse relationship between two variables, that is, two random variables that are perfectly negatively related. On the other hand, $M=\min(u_1,u_2)$ in dimension two is known as the **comonotone copula**. It captures the relationship between two variables where one is related to the other by a strictly increasing function, that is, two random variables that are perfectly positively dependent. The co-monotonic copulas can be extended to the multivariate case. However, it's not possible to extend the counter-monotonic copula because it's not possible to have three or more variables where each pair has a direct inverse relationship.


**Example 16.5.1. Largest Possible Value Example**

Suppose we have a variable $X$ that follows a Pareto distribution with a scale parameter of $\theta=10$ and a shape parameter of $\alpha=1.6$. Additionally, let $Y$ be an exponential variable with a mean value of 8. Let $F_{X,Y}(x,y)$ be the joint distribution function. What is the largest possible value of $F_{X,Y}(7.2,4.1)$? 


`r HideExample('16.5.1','Show Example Solution')`

`r SolnBegin()` Let $C(u,v)$ is the copula that links $X$ and $Y$. Denote the marginal distribution functions of $X$ and $Y$ as $F_X(x)$ and $F_Y(y)$, respectively. Since $F_{X,Y}(7.2,4.1)=C[F_X(7.2),F_Y(4.1)]$, we can use the marginal distribution functions to obtain the arguments of the copula function:

For the Pareto variable, $X$:
$$ F_X(7.2)=1- \left(  \frac{10}{7.2+ 10} \right) ^{1.6} =0.58.$$
For the Exponential variable, $Y$:

$$F_Y(4.1)=1-e^{-0.125\times 4.1}=0.40.$$

Hence, $F_{X,Y}(7.2,4.1)=C[0.58,0.40]$. Now:

The largest value of the joint distribution function is obtained when the dependence structure is comonotonic, or $C(u, v)=\min (u, v)$. Hence, the answer required is $\min (0.58,0.40)=0.40$.

`r SolnEnd()` 

</div>

*** 


### Measures of Association {#S:Sec1652}

Empirical versions of Spearman's rho and Kendall's tau were introduced in Section \@ref(S:Sec1612), respectively. The interesting thing about these expressions is that these summary measures of association are based **only** on the ranks of each variable. Thus, any strictly increasing transform does not affect these measures of association. Specifically, consider two random variables, $Y_1$ and $Y_2$, and let m$_1$ and m$_2$ be strictly increasing functions. Then, the association, when measured by Spearman's rho or Kendall's tau, between $m_1(Y_1)$ and $m_2(Y_2)$ does not change regardless of the choice of m$_1$ and m$_2$. For example, this allows analysts to consider dollars, Euros, or log dollars, and still retain the same essential dependence. As we have seen in Section \@ref(S:Sec161), this is not the case with the Pearson's measure of correlation.

@schweizer1981nonparametric established that the copula accounts for all the dependence in the sense that the way $Y_1$ and $Y_2$ "move together" is captured by the copula, regardless of the scale in which each variable is measured. They also showed that (population versions of) the two standard nonparametric measures of association could be expressed solely in terms of the copula function. Spearman's correlation coefficient is given by
\begin{equation}
\rho_S = 12 \int_0^1 \int_0^1  \left\{C(u,v) - uv \right\} du dv.
(\#eq:TheorySpearman)
\end{equation}
Kendall's tau is given by
$$
\tau= 4 \int_0^1  \int_0^1  C(u,v)~dC(u,v) - 1 .
$$
For these expressions, we assume that $Y_1$ and $Y_2$ have a jointly continuous distribution function. 

**Example. Loss versus Expenses**. Earlier, in Section \@ref(S:Sec163), we saw that the Spearman's correlation was 0.452, calculated with the `rho` function. Then, we fit Frank's copula to these data, and estimated the dependence parameter to be $\hat{\gamma} = 3.114$. As an alternative, the following code shows how to use the empirical version of equation \@ref(eq:TheorySpearman). In this case, the Spearman's correlation coefficient is 0.462, which is close to the sample Spearman's correlation coefficient, 0.452.

```{r echo=FALSE}
param = fit.ml@estimate # fitted copula parameter
frank.cop <- archmCopula("frank", param= param, dim = 2)
#rho(frank.cop) # Spearman's correlation

```

`r HideRCode('fittedCop.1',"R Code for Spearman's Correlation Using Frank's Copula")`

```{r echo=HtmlEval, eval=FALSE}
(param = fit.ml@estimate)
frank.cop <- archmCopula("frank", param= param, dim = 2)
rho(frank.cop) 

Output : 
[1] 0.4622722

```
</div> 

### Tail Dependency

As discussed in Section \@ref(S:Sec1613), there are applications in which it is useful to distinguish the part of the distribution in which the association is strongest. For example, in insurance it is helpful to understand association among the largest losses, that is, association in the right tails of the data. This subsection defines upper and lower tail dependency in terms of copulas.   


To capture this type of dependency, we use the *right-tail concentration* function, defined as
$$
R(z) = \frac{\Pr(U_1 >z, U_2 > z)}{1-z} =\Pr(U_1 > z | U_2 > z) =\frac{1 - 2z + C(z,z)}{1-z} .
$$
As a benchmark, $R(z)$ will be equal to $z$ under independence. Joe (1997) uses the term "upper tail dependence parameter" for $R = \lim_{z \rightarrow 1} R(z)$. 

In the same way, one can define the *left-tail concentration* function as 
$$
L(z) = \frac{\Pr(U_1 \leq z, U_2 \leq z)}{z}=\Pr(U_1 \leq z | U_2 \leq z) =\frac{ C(z,z)}{z},
$$ 
with the lower tail dependence parameter $L = \lim_{z \rightarrow 0} L(z)$. A `r Gloss('tail dependency')` concentration function captures the probability of two random variables simultaneously having extreme values.

It is of interest to see how well a given copula can capture tail dependence. To this end, we calculate the left and right tail concentration functions for four different types of copulas; Normal, Frank, Gumbel and $t$- copulas. The results are summarized for concentration function values for these four copulas in  [Table 16.8](#tab:16.8). As in @venter2002tails, we show $L(z)$ for $z\leq 0.5$ and $R(z)$ for $z>0.5$ in the tail dependence plot in Figure \@ref(fig:Fig1613). We interpret the tail dependence plot to mean that both the Frank and Normal copula exhibit no tail dependence whereas the $t$- and the Gumbel do so. The $t$- copula is symmetric in its treatment of upper and lower tails.


<a id=tab:16.8></a>

[Table 16.8]: ./ChapDependenceModel.html#tab:16.8

**[Table 16.8]{#tab:16.8}. Tail Dependence Parameters for Four Copulas**


$$
{\small \begin{matrix}
\begin{array}{l|rr} 
    \hline
\text{Copula} & \text{Lower}    & \text{Upper}     \\
\hline
\text{Frank}  & 0  & 0   \\
\text{Gumbel}  & 0   & 0.74    \\
\text{Normal}  & 0   & 0    \\
t-  & 0.10   & 0.10    \\
   \hline
\end{array}
\end{matrix}}
$$




```{r TailConcentration, echo=FALSE}
library(copula)
U1 = seq(0,0.5, by=0.002)
U2 = seq(0.5,1, by=0.002)
U = rbind(U1, U2)
TailFunction <- function(Tailcop) {
  lowertail <- pCopula(cbind(U1,U1), Tailcop)/U1
  uppertail <- (1-2*U2 +pCopula(cbind(U2,U2), Tailcop))/(1-U2)
  jointtail <- rbind(lowertail,uppertail)
}
Tailcop1 <- archmCopula(family = "frank", param= c(0.05), dim = 2)
Tailcop2 <- archmCopula(family = "gumbel",param = 3)
Tailcop3 <- ellipCopula("normal", param = c(0.25),dim = 2, dispstr = "un")
Tailcop4 <- ellipCopula("t", param = c(0.25),dim = 2, dispstr = "un", df=5)
jointtail1 <- TailFunction(Tailcop1)
jointtail2 <- TailFunction(Tailcop2)
jointtail3 <- TailFunction(Tailcop3)
jointtail4 <- TailFunction(Tailcop4)

```

`r HideRCode('Tail.1',"R Code for Tail Copula Functions for Different Copulas")`

```{r, echo=HtmlEval, ref.label = 'TailConcentration', eval=FALSE}
```

</div> 

(ref:Fig1613) **Tail Dependence Plots**

```{r Fig1613, out.width='80%', echo=FALSE,fig.cap='(ref:Fig1613)'}
plot(U,jointtail1, cex=.2, xlim=c(0,1),ylab="Tail Dependence", ylim=c(0,1))
lines(U,jointtail2, type="p",lty=1, cex=.2)
lines(U,jointtail3, type="p",lty=1, cex=.2)
lines(U,jointtail4, type="p",lty=1, cex=.2)
text(0.75, 0.1, "Frank", cex=1.3)        #1
text(0.1, 0.8, "Gumbel", cex=1.3)        #2
text(0.25, 0.1, "normal", cex=1.3)       #3
arrows(.17, 0.1, .07, 0.12,code=2, angle=20, length=0.1)
text(0.9, 0.4, "t with 5 df", cex=1.3)   #4
```

`r HideRCode('Tailplot.1',"R Code for Tail Dependence Plot for Different Copulas")`

```{r, echo=HtmlEval, ref.label = 'Fig1613', eval=FALSE}
```

</div>



**Example 16.5.2. Lower Tail Dependence Coefficient Example**

The bivariate distribution function $C(u,v)=uv$. What is the lower tail dependence coefficient of this copula?


`r HideExample('16.5.2','Show Example Solution')`

`r SolnBegin()` 

$$
\begin{aligned}
\lambda_{lo} & =\lim _{u \rightarrow 0^{+}} \frac{C(u, u)}{u}=\lim _{u \rightarrow 0^{+}} \frac{u^2}{u} =\lim _{u \rightarrow 0^{+}} u=0
\end{aligned}
$$

`r SolnEnd()` 

</div>

*** 

```{r child = './Quizzes/Quiz165.html', eval = QUIZ}
```


## Importance of Dependence Modeling {#S:Sec166}


***

In this section, you learn how to:

-  Explain the importance of dependence modeling
-  Explain the importance of copulas for regression applications 

***


### Why is Dependence Modeling Important?

Dependence modeling is important because it enables us to understand the dependence structure by defining the relationship between variables in a dataset. In insurance, ignoring dependence modeling may not impact pricing but could lead to misestimation of required capital to cover losses. For instance, from Section  \@ref(S:Sec163) , it is seen that there was a positive relationship between ${\tt LOSS}$ and ${\tt ALAE}$. This means that, if there is a large loss then we expect expenses to be large as well and ignoring this relationship could lead to mis-estimation of reserves.


To illustrate the importance of dependence modeling, we refer you back to portfolio management [Example 13.4.6](#Ex:13.4.6) that assumed that the property and liability risks are independent. Now, we incorporate dependence by allowing the four lines of business to depend on one another through a Gaussian copula. In [Table 16.9](#tab:16.9), we show that dependence affects the portfolio quantiles ($VaR_q$), although not the expected values. For instance, the $VaR_{0.99}$ for total risk which is the amount of capital required to ensure, with a $99\%$ degree of certainty that the firm does not become technically insolvent is higher when we incorporate dependence. This leads to less capital being allocated when dependence is ignored and can cause unexpected solvency problems. 

\newpage

<a id=tab:16.9></a>  

[Table 16.9]: ./ChapDependenceModel.html#tab:16.9

**[Table 16.9]{#tab:16.9}. Results for Portfolio Expected Value and Quantiles ($VaR_q$)**




$$
{\small \begin{matrix}
\begin{array}{l|rrrr} 
    \hline
 \text{Independent} &\text{Expected}   & VaR_{0.9}  & VaR_{0.95}  & VaR_{0.99}  \\
                   &\text{Value}      &            &             &             \\
     \hline              
\text{Retained}    & 269              &  300       & 300         & 300         \\
\text{Insurer}     & 2,274            &  4,400     & 6,173       & 11,859      \\
\text{Total}       & 2,543            &  4,675     & 6,464       & 12,159      \\
   \hline
\text{Gaussian Copula}&\text{Expected}& VaR_{0.9}  & VaR_{0.95}  & VaR_{0.99}  \\
                      &\text{Value}    &           &             &              \\
     \hline                      
\text{Retained}       & 269            &  300      & 300         &  300         \\
\text{Insurer}        & 2,340          &  4,988    & 7,339       & 14,905       \\
\text{Total}          & 2,609          &  5,288    & 7,639       & 15,205       \\
   \hline
\end{array}
\end{matrix}}
$$



```{r PortfolioResults, include=FALSE}
# For the gamma distributions, use
alpha1 <- 2;      theta1 <- 100
alpha2 <- 2;      theta2 <- 200
# For the Pareto distributions, use
alpha3 <- 2;      theta3 <- 1000
alpha4 <- 3;      theta4 <- 2000
# Deductibles
d1     <- 100
d2     <- 200

# Simulate the risks
nSim <- 10000  #number of simulations
set.seed(2017) #set seed to reproduce work 
X1 <- rgamma(nSim,alpha1,scale = theta1)  
X2 <- rgamma(nSim,alpha2,scale = theta2)  
# For the Pareto Distribution, use
library(VGAM)
X3 <- rparetoII(nSim,scale=theta3,shape=alpha3)
X4 <- rparetoII(nSim,scale=theta4,shape=alpha4)
# Portfolio Risks
S         <- X1 + X2 + X3 + X4
Sretained <- pmin(X1,d1) + pmin(X2,d2)
Sinsurer  <- S - Sretained

# Expected Claim Amounts
ExpVec <- t(as.matrix(c(mean(Sretained),mean(Sinsurer),mean(S))))
colnames(ExpVec) <- c("Retained", "Insurer","Total")
round(ExpVec,digits=2)

# Quantiles
quantMat <- rbind(
  quantile(Sretained, probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(Sinsurer,  probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(S       ,  probs=c(0.80, 0.90, 0.95, 0.99)))
rownames(quantMat) <- c("Retained", "Insurer","Total")
round(quantMat,digits=2)

#plot(density(S), main="Density of Total Portfolio Risk S", xlab="S")

### Normal Copula ##
library(VGAM)
library(copula)
library(GB2)
library(statmod)
library(numDeriv)
set.seed(2017)
parm<-c(0.5,0.5,0.5,0.5,0.5,0.5)
nc <- normalCopula(parm, dim = 4, dispstr = "un")
mcc <- mvdc(nc, margins = c("gamma", "gamma","paretoII","paretoII"), 
            paramMargins = list(list(scale = theta1, shape=alpha1),
                                list(scale = theta2, shape=alpha2),
                                list(scale = theta3, shape=alpha3),
                                list(scale = theta4, shape=alpha4)))
X <- rMvdc(nSim, mvdc = mcc)

X1<-X[,1]
X2<-X[,2]
X3<-X[,3]
X4<-X[,4]

# Portfolio Risks
S         <- X1 + X2 + X3 + X4
Sretained <- pmin(X1,d1) + pmin(X2,d2)
Sinsurer  <- S - Sretained

# Expected Claim Amounts
ExpVec <- t(as.matrix(c(mean(Sretained),mean(Sinsurer),mean(S))))
colnames(ExpVec) <- c("Retained", "Insurer","Total")
round(ExpVec,digits=2)

# Quantiles
quantMat <- rbind(
  quantile(Sretained, probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(Sinsurer,  probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(S       ,  probs=c(0.80, 0.90, 0.95, 0.99)))
rownames(quantMat) <- c("Retained", "Insurer","Total")
round(quantMat,digits=2)

#plot(density(S), main="Density of Total Portfolio Risk S", xlab="S")

```


`r HideRCode('SimG.1',"R Code for Simulation Using Gaussian Copula")`

```{r echo=HtmlEval, eval=FALSE}
# For the gamma distributions, use
alpha1 <- 2;      theta1 <- 100
alpha2 <- 2;      theta2 <- 200
# For the Pareto distributions, use
alpha3 <- 2;      theta3 <- 1000
alpha4 <- 3;      theta4 <- 2000
# Deductibles
d1     <- 100
d2     <- 200


# Simulate the risks
nSim <- 10000  #number of simulations
set.seed(2017) #set seed to reproduce work 
X1 <- rgamma(nSim,alpha1,scale = theta1)  
X2 <- rgamma(nSim,alpha2,scale = theta2)  
# For the Pareto Distribution, use
library(VGAM)
X3 <- rparetoII(nSim,scale=theta3,shape=alpha3)
X4 <- rparetoII(nSim,scale=theta4,shape=alpha4)
# Portfolio Risks
S         <- X1 + X2 + X3 + X4
Sretained <- pmin(X1,d1) + pmin(X2,d2)
Sinsurer  <- S - Sretained

# Expected Claim Amounts
ExpVec <- t(as.matrix(c(mean(Sretained),mean(Sinsurer),mean(S))))
colnames(ExpVec) <- c("Retained", "Insurer","Total")
round(ExpVec,digits=2)

# Quantiles
quantMat <- rbind(
  quantile(Sretained, probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(Sinsurer,  probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(S       ,  probs=c(0.80, 0.90, 0.95, 0.99)))
rownames(quantMat) <- c("Retained", "Insurer","Total")
round(quantMat,digits=2)

plot(density(S), main="Density of Total Portfolio Risk S", xlab="S")

### Normal Copula ##
library(VGAM)
library(copula)
library(GB2)
library(statmod)
library(numDeriv)
set.seed(2017)
parm<-c(0.5,0.5,0.5,0.5,0.5,0.5)
nc <- normalCopula(parm, dim = 4, dispstr = "un")
mcc <- mvdc(nc, margins = c("gamma", "gamma","paretoII","paretoII"), 
            paramMargins = list(list(scale = theta1, shape=alpha1),
                                list(scale = theta2, shape=alpha2),
                                list(scale = theta3, shape=alpha3),
                                list(scale = theta4, shape=alpha4)))
X <- rMvdc(nSim, mvdc = mcc)

X1<-X[,1]
X2<-X[,2]
X3<-X[,3]
X4<-X[,4]

# Portfolio Risks
S         <- X1 + X2 + X3 + X4
Sretained <- pmin(X1,d1) + pmin(X2,d2)
Sinsurer  <- S - Sretained

# Expected Claim Amounts
ExpVec <- t(as.matrix(c(mean(Sretained),mean(Sinsurer),mean(S))))
colnames(ExpVec) <- c("Retained", "Insurer","Total")
round(ExpVec,digits=2)

# Quantiles
quantMat <- rbind(
  quantile(Sretained, probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(Sinsurer,  probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(S       ,  probs=c(0.80, 0.90, 0.95, 0.99)))
rownames(quantMat) <- c("Retained", "Insurer","Total")
round(quantMat,digits=2)

plot(density(S), main="Density of Total Portfolio Risk S", xlab="S")

```


</div>  


It should be noted that there are various methods of conducting dependence modeling, but copulas are effective for many actuarial applications. It's important to stress that each copula function captures a distinct dependency structure based on its functional form and dependence parameters. Therefore, utilizing copulas without comprehending their limitations and properties can lead to biased and statistically incorrect results. Since selecting the right copula involves extensive effort, here are some general tips that can assist:

1. When analyzing data, diagnostic and exploratory analysis can provide insight into the dependence structure of the data, which can help determine suitable copula functions. For Archimedean Copulas specifically, understanding the dependence structure can narrow down the appropriate type of copula function. For instance, the Gumbel-Hougaard copula is not suitable for negative dependency, but the Frank Copula can effectively capture three distinct types of dependency in the data.

2. Researchers cannot rely on Normal copula or Frank copula functions to capture the upper and lower tail dependency in data. Instead, a $t$ copula with low degrees of freedom works well for both tails. The Gumbel-Hougaard copula shows some upper tail dependence but less or no lower tail dependence, while the Clayton copula exhibits strong lower tail dependence.



### Copula Regression


In regression studies, the response variable is determined by a group of explanatory variables. This is often one of the initial statistical methods used to understand the connection between the response and explanatory variables. However, Linear Models and Generalized Linear Models can impose constraints on the selection of distributions for the response variables, which can be restrictive for practical data scenarios. For example, insurance claim amounts and financial asset returns typically exhibit heavy-tailed and skewed distributions, and may not adhere to normality patterns, with the possibility of having extreme values.

The use of copulas in regression is gaining attention in the field of actuarial science. Copula regression separates the dependency structure from the selection of marginal distributions, allowing for greater flexibility in choosing distributions for actuarial applications. The parameters for the marginal distributions and the copula distribution can be estimated either separately or together. The maximum likelihood method is often effective for estimating the parameters. However, for copula regression parameter estimation, the inference for margins method (IFM) is commonly used. Copula functions preserve the marginals and make predictions using the dependent variable's conditional mean given the covariates. See  @kramer2013total, @parsa2011copula; for detailed  examples on copula regression. 




```{r child = './Quizzes/Quiz166.html', eval = QUIZ}
```


## Further Resources and Contributors {#S:Sec167}


#### Contributors {-}

-  **Edward (Jed) Frees** and **Nii-Armah Okine**, University of Wisconsin-Madison, and **Emine Selin SarÄ±daÅŸ**, Mimar Sinan University, are the principal authors of the initial version of this chapter. 
   - Chapter reviewers include: Runhuan Feng, Fei Huang, Himchan Jeong, Min Ji, and Toby White.
-  **Nii-Armah Okine**, Appalachian State University, and **Emine Selin SarÄ±daÅŸ**, Mimar Sinan University, are the principal authors of the second edition of this chapter. Email: okinean@appstate.edu and selin.saridas@msgsu.edu.tr for chapter comments and suggested improvements.
   - Chapter reviewers include MÃ©lina Mailhot.



### TS 16.A. Other Classic Measures of Scalar Associations {-}

#### TS 16.A.1. Blomqvist's Beta {-}

@blomqvist1950measure developed a measure of dependence now known as `r Gloss("Blomqvist's beta")`, also called the *median concordance coefficient* and the *medial correlation coefficient*. Using distribution functions, this parameter can be expressed as

\begin{equation*}
\beta_B = 4F\left(F^{-1}_X(1/2),F^{-1}_Y(1/2) \right) - 1.
\end{equation*}

That is, first evaluate each marginal at its median ($F^{-1}_X(1/2)$ and $F^{-1}_Y(1/2)$, respectively). Then, evaluate the bivariate distribution function at the two medians. After rescaling (multiplying by 4 and subtracting 1), the coefficient turns out to have a range of $[-1,1]$, where 0 occurs under independence.

Like Spearman's rho and Kendall's tau, an estimator based on ranks is easy to provide. First write $\beta_B = 4C(1/2,1/2)-1 = 2\Pr((U_1-1/2)(U_2-1/2))-1$ where $U_1, U_2$ are uniform random variables. Then, define

$$
\hat{\beta}_B = \frac{2}{n} \sum_{i=1}^n I\left( (R(X_{i})-\frac{n+1}{2})(R(Y_{i})-\frac{n+1}{2}) \ge 0 \right)-1 .
$$

See, for example, @joe2014dependence, page 57 or @hougaard2000analysis, page 135, for more details.

Because Blomqvist's parameter is based on the center of the distribution, it is particularly useful when data are censored; in this case, information in extreme parts of the distribution are not always reliable. How does this affect a choice of association measures? First, recall that association measures are based on a bivariate distribution function. So, if one has knowledge of a good approximation of the distribution function, then calculation of an association measure is straightforward in principle. Second, for censored data, bivariate extensions of the univariate Kaplan-Meier distribution function estimator are available. For example, the version introduced in @dabrowska1988kaplan is appealing. However, because of instances when large masses of data appear at the upper range of the data, this and other estimators of the bivariate distribution function are unreliable. This means that, summary measures of the estimated distribution function based on Spearman's rho or Kendall's tau can be unreliable. For this situation, Blomqvist's beta appears to be a better choice as it focuses on the center of the distribution. @hougaard2000analysis, Chapter 14, provides additional discussion.

You can obtain the Blomqvist's beta, using the `betan()` function from the `copula` library in `R`. From below, $\beta_B=0.3$ between the ${\tt Coverage}$ rating variable in millions of dollars and ${\tt Claim}$ amount  variable in dollars. 

```{r include=FALSE}
### Blomqvist's beta correlation between Claim and Coverage ###
library(copula)
n<-length(Claim)
U<-cbind(((n+1)/n*pobs(Claim)),((n+1)/n*pobs(Coverage)))
beta<-betan(U, scaling=FALSE)
round(beta,2)

### Blomqvist's beta correlation between Claim and log(Coverage) ###
n<-length(Claim)
Fx<-cbind(((n+1)/n*pobs(Claim)),((n+1)/n*pobs(log(Coverage))))
beta<-betan(Fx, scaling=FALSE)
round(beta,2)
```

`r HideRCode('beta.1',"R Code for Blomqvist's Beta")`

```{r echo=HtmlEval, eval=FALSE}
### Blomqvist's beta correlation between Claim and Coverage ###
library(copula)
n<-length(Claim)
U<-cbind(((n+1)/n*pobs(Claim)),((n+1)/n*pobs(Coverage)))
beta<-betan(U, scaling=FALSE)
round(beta,2)

Output:
[1]  0.3

### Blomqvist's beta correlation between Claim and log(Coverage) ###
n<-length(Claim)
Fx<-cbind(((n+1)/n*pobs(Claim)),((n+1)/n*pobs(log(Coverage))))
beta<-betan(Fx, scaling=FALSE)
round(beta,2)

Output:
[1]  0.3
```
</div>  


In addition, to show that the Blomqvist's beta is invariant under strictly increasing transformations, $\beta_B=0.3$ between the ${\tt Coverage}$ rating variable in logarithmic millions of dollars and ${\tt Claim}$ amount variable in dollars. 


#### TS 16.A.2. Nonparametric Approach Using Spearman Correlation with Tied Ranks {-}

For the first variable, the average rank of observations in the $s$th row is

\begin{equation*}
r_{1s} = n_{m_1\bullet}+ \cdots+ n_{s-1,\bullet}+ \frac{1}{2} \left(1+ n_{s\bullet}\right)
\end{equation*}

and similarly $r_{2t} = \frac{1}{2} \left[(n_{\bullet m_1}+ \cdots+ n_{\bullet,s-1}+1)+ (n_{\bullet m_1}+ \cdots+ n_{\bullet s})\right]$. With this, we have Spearman's rho with tied rank is

\begin{equation*}
\hat{\rho}_S = \frac{\sum_{s=m_1}^{m_2} \sum_{t=m_1}^{m_2} n_{st}(r_{1s} - \bar{r})(r_{2t} - \bar{r})}
{\left[\sum_{s=m_1}^{m_2}n_{s \bullet}(r_{1s} - \bar{r})^2 \sum_{t=m_1}^{m_2} n_{\bullet t}(r_{2t} - \bar{r})^2
\right]^2}
\end{equation*}

where the average rank is $\bar{r} = (n+1)/2$.



`r HideRCode('Thry.1',"Click to Show Proof for Special Case: Binary Data.")`

*Special Case: Binary Data*.  Here, $m_1=0$ and $m_2=1$. For the first variable ranks, we have $r_{10} = (1+n_{0\bullet})/2$ and $r_{11} = (n_{0\bullet}+1+n)/2$. Thus, $r_{10} -\bar{r}= (n_{0\bullet}-n)/2$ and $r_{11}-\bar{r} = n_{0\bullet}/2$.
This means that we have $\sum_{s=0}^{1}n_{s\bullet}(r_{1s} - \bar{r})^2 = n (n-n_{0\bullet})n_{0\bullet}/4$ and similarly for the second variable. For the numerator, we have
\begin{eqnarray*}
\sum_{s=0}^{1}  \sum_{t=0}^{1} && n_{st}(r_{1s} - \bar{r})(r_{2t} - \bar{r})\\
&=& n_{00} \frac{n_{0\bullet}-n}{2} \frac{n_{\bullet 0}-n}{2}
+n_{01} \frac{n_{0\bullet}-n}{2} \frac{n_{\bullet 0}}{2}
+n_{10} \frac{n_{0\bullet}}{2} \frac{n_{\bullet 0}-n}{2}
+n_{11} \frac{n_{0\bullet}}{2} \frac{n_{\bullet 0}}{2} \\
&=& \frac{1}{4}(n_{00} (n_{0\bullet}-n) (n_{\bullet 0}-n)
+(n_{0\bullet}-n_{00}) (n_{0\bullet}-n)n_{\bullet 0} \\
&&  ~ ~ ~ +(n_{\bullet 0}-n_{00})  n_{0\bullet}(n_{\bullet 0}-n)
+(n-n_{\bullet 0}-n_{0\bullet}+n_{00}) n_{0\bullet}n_{\bullet 0} ) \\
&=& \frac{1}{4}(n_{00} n^2
- n_{0\bullet} (n_{0\bullet}-n)n_{\bullet 0} \\
&& ~ ~ ~ +n_{\bullet 0}  n_{0\bullet}(n_{\bullet 0}-n)
+(n-n_{\bullet 0}-n_{0\bullet}) n_{0\bullet}n_{\bullet 0} ) \\
&=& \frac{1}{4}(n_{00} n^2
- n_{0\bullet}n_{\bullet 0} (n_{0\bullet}-n +n_{\bullet 0}-n
+n-n_{\bullet 0}-n_{0\bullet}) \\
&=& \frac{n}{4}(n n_{00} - n_{0\bullet}n_{\bullet 0}) .
\end{eqnarray*}

This yields
\begin{eqnarray*}
\hat{\rho}_S &=& \frac{n(n n_{00} - n_{0\bullet}n_{\bullet 0})}
{4\sqrt{(n (n-n_{0\bullet})n_{0\bullet}/4)(n (n-n_{\bullet 0})n_{\bullet 0}/4)}} \\
&=& \frac{n n_{00} - n_{0\bullet}n_{\bullet 0}}
{\sqrt{ n_{0\bullet} n_{\bullet 0}(n-n_{0\bullet}) (n-n_{\bullet 0})}} \\
&=& \frac{n_{00} - n (1-\hat{\pi}_X)(1- \hat{\pi}_Y)}
{\sqrt{\hat{\pi}_X(1-\hat{\pi}_X)\hat{\pi}_Y(1-\hat{\pi}_Y) }}
\end{eqnarray*}
where $\hat{\pi}_X = (n-n_{0\bullet})/n$ and similarly for $\hat{\pi}_Y$. Note that this is same form as the Pearson measure. From this, we see that the joint count $n_{00}$ drives this association measure.


</div>  

\bigskip

You can obtain the ties-corrected Spearman correlation statistic $r_S$ using the `cor()` function in `R` and selecting the `spearman` method.  From below $\hat{\rho}_S=-0.09$.

```{r include=FALSE}
rs_ties<-cor(AlarmCredit,NoClaimCredit, method = c("spearman"))
round(rs_ties,2)
```

`r HideRCode('spearT.1',"R Code for Ties-corrected Spearman Correlation")`

```{r echo=HtmlEval, eval=FALSE}
rs_ties<-cor(AlarmCredit,NoClaimCredit, method = c("spearman"))
round(rs_ties,2)

Output:
[1] -0.09
```
</div>  


