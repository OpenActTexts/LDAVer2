
<!-- # Chap 1 -->

<!-- # Chap 2 -->

<!-- # Chap 3 -->

<!-- # Chap 4 -->

<!-- # Chap 5 -->

<!-- # Chap 6 -->

<!-- # Chap 7 -->

<!-- # Chap 8 -->

<!-- # Chap 9 -->

<!-- # Chap 10 -->

<!-- # Chap 11 -->

<!-- # Chap 12 -->

# Insurance Portfolio Management including Reinsurance {#ChapPortMgt}


*Chapter Preview*. An insurance portfolio is simply a collection of insurance contracts. To help manage the uncertainty of the portfolio, this chapter 

- quantifies unusually large obligations by examining the tail of the distribution, 
- quantifies the overall riskiness by introducing summaries known as risk measures, and 
- discusses options of spreading portfolio risk through reinsurance, the purchase of insurance protection by an insurer.

## Introduction to Insurance Portfolios

In previous chapters, our analyses primarily focused on the contract level, which represents agreements between policyholders and insurers. Insurers maintain and manage `r Gloss('portfolios')`, which are essentially collections of these individual contracts. Conceptually, one can liken an insurance company to nothing more than a collection, or portfolio, of insurance contracts. Similar to banking and investments, there are management decisions that are made exclusively at the portfolio level. Within this chapter, we address three crucial actuarial tasks: quantifying the impact of extreme events, determining overall portfolio risk, and managing insurance portfolios through reinsurance.

`r Gloss('Insurance portfolios')`, representing the obligations of insurers, pique our interest primarily due to the probabilities associated with significant outcomes. These outcomes often translate to unusually large obligations. To illustrate, within property and casualty insurance, large obligations frequently stem from unforeseen consequences of **climate-related risks**. For instance, consider the freezing rain event of 1998 that swept through eastern Ontario and southwestern Quebec, lasting six days. This calamity resulted in double the typical precipitation for the region during an ice storm and gave rise to a catastrophe, triggering over 840,000 insurance claims. Astonishingly, this number exceeded the claims filed in the wake of Hurricane Andrew, one of North America's most extensive natural disasters. The catastrophe led to insurance settlements exceeding 1.44 billion Canadian dollars, marking the highest loss burden in Canada's history. Such incidents are not isolated; similar catastrophic events, like Hurricane Harvey, Superstorm Sandy, the 2011 Japanese earthquake and tsunami, have also caused extreme insurance losses. In our exploration of extreme events in insurance, we introduce the concept of heavy-tailed distributions in Section \@ref(S:Sec132).

Insurance companies engage in the buying and selling of risks as if they were commodities. As we explored in Chapter \@ref(ChapPremiumFoundations), greater uncertainty associated with risks typically translates into higher prices. In that chapter, pricing principles were introduced to quantify the magnitude of these risks. Furthermore, insurance portfolios represent the obligations of a company and, although they are not traded on a marketplace, they require careful management. One crucial aspect of this management is aligning the size of the obligations with an equivalent amount of assets. The subsequent Chapter \@ref(ChapLossReserves) on loss reserves offers practical methods for achieving this alignment. Additionally, insurers need to assess the extent of their obligations for purposes such as capacity planning, policy formulation, and maintaining a balanced product portfolio that fosters revenue growth while managing volatility. To facilitate these tasks, Section \@ref(S:Sec133) introduces risk measures that succinctly capture the uncertainty inherent in the distribution of an insurance portfolio.

Similar to individuals, insurance companies manage their risk portfolios by acquiring insurance, in this case, risk protection from `r Gloss('reinsurers')`, which are insurance companies serving insurers. Just as individuals can structure the amount of risk they retain through mechanisms like deductibles and policy limits, insurers employ similar strategies to structure their risk portfolios. This practice of sharing insurance portfolio risk is detailed in Section \@ref(S:Sec134), where we delve into the concept of reinsurance.

These three actuarial tasks, quantifying the impact of extreme events, determining overall portfolio risk, and managing insurance portfolios through reinsurance, are based on the distribution of insurance portfolios. In Chapter \@ref(ChapAggLossModels), we delved into modeling the distribution of insurance portfolios as the sum of individual contracts where we used $S$ for aggregate losses. Now, this chapter is dedicated to the direct exploration of portfolio distributions and so we revert to the traditional $X$ notation.



```{r child = './Quizzes/Quiz13A1.html', eval = QUIZ}
```


## Tails of Distributions {#S:Sec132}

***
In this section, you learn how to:

- Describe a heavy tail distribution intuitively.
- Classify the heaviness of a distribution's tails based on moments.
- Compare the tails of two distributions.

***


For extreme events such as those due to climate risks, a few major events hitting a portfolio and then converting into losses usually represent the greatest part of the indemnities paid by insurance companies. The aforementioned losses, also called 'extremes', are quantitatively modeled by the tails of the associated probability distributions.  From the quantitative modeling standpoint, relying on probabilistic models having lengthy tails can be daunting.  For instance, periods of financial stress may appear with a higher frequency than expected, and insurance losses may occur with worse severity. Therefore, the study of probabilistic behavior in the tail portion of actuarial models is important in quantitative risk management. For this reason, this section introduces a few mathematical notions that describe the tail weight of random variables. These notions will benefit us in the construction and selection of appropriate models with desired mathematical properties in the tail portion.

Formally, define $X$ to be the random obligations that arise from a collection (portfolio) of insurance contracts. At the portfolio level, we are particularly interested in studying the right tail of the distribution of $X$ which represents the occurrence of large losses. Informally, *a random variable is said to be heavy-tailed if high probabilities are assigned to large values.*  This does not imply that the probability density/mass function increases as the value of $X$ goes to infinity.  Indeed, for a real-valued random variable, the `r Gloss('pdf')`/`r Gloss('pmf')` must diminish at infinity in order to guarantee the total probability to be equal to one.  Instead, what we are concerned about is the *rate* of decay of the pdf/pmf. Unwelcome outcomes are more likely to occur for an insurance portfolio that is described by a loss random variable possessing a heavier (right) tail.  Tail weight can be an absolute or a relative concept.  Specifically, for the former, we may consider a random variable to be heavy-tailed if certain mathematical properties of the probability distribution are met.  For the latter, we can say the tail of one distribution is heavier/lighter than the other if some tail measures are larger/smaller.

Several quantitative approaches have been proposed to classify and compare tail weights. For most of these approaches, the  `r Gloss('survival function', '10.2')` serves as the building block.  In what follows, we introduce two simple yet useful tail classification methods both of which are based on the behavior of the survival function of $X$.

### Classification Based on Moments

One way of classifying the tail weight of a distribution is by determining whether or not a raw moment is finite. Because our major interest lies in the right tail of a distribution, we henceforth assume the obligation or loss random variable $X$ to be non-negative. At the outset, the $k-$th raw moment of a continuous random variable $X$, introduced in Section \@ref(S:Sec41), can be expressed as
$$
\mu_k' =  \int_0^{\infty} x^k f(x) ~dx = k \int_0^{\infty} x^{k-1} S(x) ~dx, \\
$$
where $S(\cdot)$ denotes the survival function of $X$. This expression emphasizes that the finiteness of the raw moments depends on the asymptotic behavior of the survival function at infinity.  Namely, the faster the survival function decays to zero, the higher is the order ($k$) is which the associated random variable may be finite. To capture this idea, we can formally define $k^{\ast}=\sup\{k > 0:\mu_k'<\infty \}$, where $sup$ represents the supremum operator. You may interpret $k^{\ast}$ to be the largest value of $k$ so that the moment is finite. 

This definition leads us to a moment-based tail weight classification method which is defined as follows.


**Definition  13.1.**  Consider a non-negative loss random variable $X$.

- If all the positive raw moments exist, namely the maximal order of finite moment $k^{\ast}=\infty$, then $X$ is said to be **light tailed** based on the moment method. 
- If $k^{\ast} < \infty$, then $X$ is said to be `r Gloss('heavy tailed')` based on the moment method. 
- Moreover, for two positive loss random variables $X_1$ and $X_2$ with maximal orders of moment $k^{\ast}_1$ and $k^{\ast}_2$ respectively, we say $X_1$ has a **heavier (right) tail** than  $X_2$ if $k^{\ast}_1\leq k^{\ast}_2$.

The first part of Definition  13.1 is an absolute concept of tail weight, while the second part is a relative concept of tail weight which compares the (right) tails between two distributions.  Next, we present a few examples that illustrate the applications of the moment-based method for comparing tail weight. 


**Example 13.2.1. Light tail nature of the gamma distribution.**
Let $X\sim gamma(\alpha,\theta)$, with $\alpha>0$ and $\theta>0$. Show that $\mu_k' < \infty$ for all $k>0$.

`r HideExample('13.2.1','Show Example Solution')`

`r SolnBegin()`  From the probability density functions expression in Section \@ref(S:ContinuousDistributions), the $k$th raw moment is
\begin{eqnarray*}
    \mu_k' &=& \int_0^{\infty} x^k \frac{x^{\alpha-1} e^{-x/\theta}}{\Gamma(\alpha) \theta^{\alpha}} dx \\
    &=& \int_0^{\infty} (y\theta)^k  \frac{(y\theta)^{\alpha-1} e^{-y}}{\Gamma(\alpha) \theta^{\alpha}} \theta dy \\
    &=& \frac{\theta^k}{\Gamma(\alpha)} \Gamma(\alpha+k) < \infty.
\end{eqnarray*}
Because all the positive moments exist, we have $k^{\ast}=\infty$. Thus, in accordance with the moment-based classification method in Definition  13.1, the gamma distribution is light-tailed. 
`r SolnEnd()` 

</div>

*** 


**Example 13.2.2. Light tail nature of the Weibull distribution.** Let $X\sim Weibull(\theta,\tau)$, with $\theta>0$ and $\tau>0$. Show that $\mu_k' < \infty$ for all $k>0$.

`r HideExample('13.2.2','Show Example Solution')`

`r SolnBegin()`  From the probability density functions expression in Section \@ref(S:ContinuousDistributions), the $k$th raw moment is
\begin{eqnarray*}
    \mu_k' &=& \int_0^{\infty} x^k \frac{\tau x^{\tau-1} }{\theta^{\tau}} e^{-(x/\theta)^{\tau}}dx \\
    &=& \int_0^{\infty}  \frac{ y^{k/\tau} }{\theta^{\tau}} e^{-y/\theta^{\tau}}dy \\
    &=& \theta^{k} \Gamma(1+k/\tau) < \infty.
\end{eqnarray*}
Again, due to the existence of all the positive moments, the Weibull distribution is light-tailed.
`r SolnEnd()` 

</div>

*** 

The gamma and Weibull distributions are used extensively in the actuarial practice. Applications of these two distributions are vast which include, but are not limited to, insurance claim severity modeling, solvency assessment, loss reserving, aggregate risk approximation, reliability engineering and failure analysis.   We have thus far seen two examples of using the moment-based method to analyze light-tailed distributions.  We document a heavy-tailed example in what follows.

**Example 13.2.3. Heavy tail nature of the Pareto distribution.** Let $X\sim Pareto(\alpha,\theta)$, with $\alpha>0$ and $\theta>0$. Then, for $k>0$,
\begin{eqnarray*}
    \mu_k^{'} &=& \int_0^{\infty} x^k \frac{\alpha \theta^{\alpha}}{(x+\theta)^{\alpha+1}} dx \\
    &=& \alpha \theta^{\alpha} \int_{\theta}^{\infty} (y-\theta)^k {y^{-(\alpha+1)}} dy.
\end{eqnarray*}
From basic calculus, recall that
\begin{eqnarray*}
INT_k= \int_{\theta}^{\infty} {y^{k-\alpha-1}} dy=\left\{
  \begin{array}{ll}
    <\infty, & \text{for } k<\alpha;\\
    =\infty, & \text{for } k\geq \alpha.
  \end{array}
\right.
\end{eqnarray*}
Also note that:
\[\lim_{y\rightarrow \infty} \frac{(y-\theta)^k {y^{-(\alpha+1)}}}{y^{k-\alpha-1}}=\lim_{y\rightarrow \infty}
(1-\theta/y)^{k}=1.\]
Application of the limit comparison theorem for improper integrals yields $\mu_k'$ is finite if and only if $INT_k$ is finite. Hence we can conclude that the raw moments of Pareto random variables exist only up to $k<\alpha$, i.e., $k^{\ast}=\alpha$, and thus the distribution is heavy-tailed.  

What is more, the maximal order of finite moment depends only on the shape parameter $\alpha$ and it is an increasing function of $\alpha$. 
In other words, based on the moment method, the tail weight of Pareto random variables is solely manipulated by $\alpha$ --  the smaller the value of $\alpha$, the heavier the tail weight becomes.  Since $k^{\ast}<\infty$, the tail of Pareto distribution is heavier than those of the gamma and Weibull distributions.  

*** 

Despite its simple implementation and intuitive interpretation, there are certain circumstances in which the application of the moment-based method is not suitable. 

*  1.  For more complicated probabilistic models, the $k$-th raw moment may not be simple to derive, and thus the identification of the maximal order of finite moment can be challenging.  
*  2.  The moment-based method does not well comply with main body of the well established heavy tail theory in the literature.  Specifically, the existence of moment generating functions is arguably the most popular method for classifying heavy tail  versus light tail within the community of academic actuaries.  However, for some random variables such as the lognormal random variables, their moment generating functions do not exist even though all the positive moments are finite.  In these cases, applications of the moment-based methods can lead to different tail weight assessment. 
*  3.  When we need to compare the tail weight between two light-tailed distributions (where both have all finite positive moments), the moment-based method is no longer informative (see, e.g., Examples 13.2.1 and 13.2.2).


### Comparison Based on Limiting Tail Behavior

In order to resolve the aforementioned issues of the moment-based classification method, an alternative approach for comparing tail weight is to directly study the limiting behavior of the survival functions.

**Definition  13.2.** For two random variables $X$ and $Y$, let
$$
\gamma=\lim_{t\rightarrow \infty}\frac{S_X(t)}{S_Y(t)}.
$$
We say that

* $X$ has a **heavier right tail** than $Y$ if $\gamma=\infty$, 
* $X$ and $Y$ are **proportionally equivalent in the right tail** if $\gamma =c \in (0, \infty)$, and
* $X$ has a **lighter right tail** than $Y$ if $\gamma=0$. 

**Example 13.2.4. Comparison of Pareto to Weibull distributions.** Let $X\sim Pareto(\alpha, \theta)$ and $Y\sim Weibull(\tau, \theta)$, for $\alpha>0$, $\tau>0$, and $\theta>0$. Show that the Pareto has a heavier right tail than the Weibull.

`r HideExample('13.2.4','Show Example Solution')`

`r SolnBegin()` 
\begin{eqnarray*}
    \lim_{t\rightarrow \infty}\frac{S_X(t)}{S_Y(t)} &=& \lim_{t\rightarrow \infty}\frac{(1+t/\theta)^{-\alpha}}{\exp\{-(t/\theta)^{\tau}\}} \\
    &=& \lim_{t\rightarrow \infty}\frac{\exp\{t/\theta^{\tau} \}}{(1+t^{1/\tau}/\theta)^{\alpha}} \\
    &=& \lim_{t\rightarrow \infty}\frac{\sum_{i=0}^{\infty}\left(\frac{t}{\theta^{\tau}}\right)^{i}/i!}{(1+t^{1/\tau}/\theta)^{\alpha}}\\
    &=& \lim_{t\rightarrow \infty} \sum_{i=0}^{\infty} \left(t^{-i/\alpha}+\frac{t^{(1/\tau-i/\alpha)}}{\theta} \right)^{-\alpha}/\theta^{\tau i}i!\\
    &=& \infty.
\end{eqnarray*}
Therefore, the Pareto distribution has a heavier tail than the Weibull distribution.  One may also realize that exponentials go to infinity faster than polynomials, thus the aforementioned limit must be infinite.
`r SolnEnd()` 

</div>

*** 


For some distributions  of which the survival functions do not admit explicit expressions, we may find the following alternative formula useful:
\begin{eqnarray*}
    \lim_{t\to \infty} \frac{S_X(t)}{S_Y(t)} &=& \lim_{t \to \infty} \frac{S_X^{'}(t)}{S_Y^{'}(t)} \\
    &=& \lim_{t \to \infty} \frac{-f_X(t)}{-f_Y(t)}\\
 &=& \lim_{t\to \infty} \frac{f_X(t)}{f_Y(t)} ,
\end{eqnarray*}
given that the density functions exist. This is an application of L'Hôpital's Rule from calculus.


**Example 13.2.5. Comparison of Pareto to gamma distributions.** Let $X\sim Pareto(\alpha, \theta)$ and $Y\sim gamma(\alpha, \theta)$, for $\alpha>0$ and $\theta>0$. Show that the Pareto has a heavier right tail than the gamma.

`r HideExample('13.2.5','Show Example Solution')`

`r SolnBegin()` 
\begin{eqnarray*}
    \lim_{t\to \infty} \frac{f_{X}(t)}{f_{Y}(t)} &=& \lim_{t \to \infty} \frac{\alpha \theta^{\alpha} (t+ \theta)^{-\alpha-1}}{t^{\tau-1} e^{-t/\lambda} \lambda^{-\tau} \Gamma(\tau)^{-1}} \\
 &\propto&  \lim_{t\to \infty} \frac{e^{t/\lambda}}{(t+\theta)^{\alpha+1} t^{\tau-1}} \\
    &=& \infty,
\end{eqnarray*}
as exponentials go to infinity faster than polynomials.
`r SolnEnd()` 


</div>

*** 


```{r child = './Quizzes/Quiz13A2.html', eval = QUIZ}
```


## Risk Measures {#S:Sec133}

***
In this section, you learn how to:

- Define the value-at-risk and calculate this quantity for a given distribution.
- Define the expected shortfall and calculate this quantity for a given distribution.
- Define the idea of *coherence* and determine whether or not a risk measure is coherent.

***

In the previous section, we studied two methods for classifying the weight of distribution tails.  We may claim that the risk associated with one distribution is more dangerous (asymptotically) than the other if the tail is heavier.  However, knowing that one risk is more dangerous than the other may not provide sufficient information for risk management purposes and, in addition, one is also interested in quantifying how much more. In fact, the magnitude of risk associated with a given loss distribution is an essential input for many insurance applications, such as actuarial pricing, reserving, hedging, insurance regulatory oversight, and so forth.


The literature on risk measures has been growing rapidly in popularity and importance. In the next two subsections, we introduce two indices which have earned interest among theoreticians, practitioners, and regulators.  They are namely the *Value-at-Risk* ($VaR$) and the *Expected Shortfall* ($ES$) measures.  The rationale underpinning these two risk measures is similar to that for the tail classification methods -- we hope to capture the uncertainty of extreme losses. 


### Value-at-Risk

In Section \@ref(S:MS:QuantileEstimator), we defined the quantile of a distribution. We now look to a special case of this and offer the formal definition of the `r Gloss("value-at-risk")`, or *VaR*.  

**Definition  13.3.** Consider an insurance loss random variable $X$.  The value-at-risk measure of $X$ with confidence level $q\in (0,1)$ is formulated as
\begin{eqnarray}
VaR_q[X]=\inf\{x:F_X(x)\geq q\}.
(\#eq:Value-at-Risk)
\end{eqnarray}
Here, $inf$ is the infimum operator so that the $VaR$ measure outputs the smallest value of $x$ such that the associated `r Gloss('cdf')`  exceeds or equates to $q$. This is simply the `r Gloss('quantile')` that was introduced in Section \@ref(S:Sec412).

Here is how we should interpret $VaR$ in the context of actuarial applications. The $VaR$ is a measure of the 'maximal' probable loss for an insurance product/portfolio or a risky investment occurring $q \times 100\%$ of times, over a specific time horizon (typically, one year).  For instance, if we let $X$ be the annual loss random variable of an insurance product, then $VaR_{0.95}[X]=100$ million means that there is no more than a $5 \%$ chance that the loss will exceed 100 million over a given year.  Owing to this meaningful interpretation, $VaR$ has become the industry standard for measuring financial and insurance risks since the 1990's.  Financial conglomerates, regulators, and academics often utilize $VaR$ to measure risk capital, ensure the compliance with regulatory rules, and disclose the financial positions.

Next, we present a few examples concerning the computation of $VaR$.  


**Example 13.3.1. $VaR$ for the exponential distribution.** Consider an insurance loss random variable $X$ with an exponential distribution having parameter $\theta$ for $\theta>0$, then the *cdf* of $X$ is given by 
$$
F_X(x)=1-e^{-x/\theta}, \text{ for } x>0.
$$
Give a closed-form expression for the $VaR$.

`r HideExample('13.3.1','Show Example Solution')`

`r SolnBegin()` Because exponential distribution is a continuous distribution, the smallest value such that the $cdf$ first exceeds or equates to $q \in (0,1)$ must be at the point $x_q$ satisfying
$$
q=F_X(x_q)=1-\exp\{-x_q/\theta \}.
$$
Thus
$$
VaR_q[X]=F_X^{-1}(q)=-\theta[\log(1-q)].
$$
`r SolnEnd()` 

</div>

*** 

The result reported in Example 13.3.1 can be generalized to any continuous random variables having a strictly increasing *cdf*.  Specifically, the $VaR$ of any continuous random variables is simply the inverse of the corresponding *cdf*.  Let us consider another example of continuous random variable which has the support from negative infinity to positive infinity.


**Example 13.3.2. $VaR$ for the normal distribution.** Consider an insurance loss random variable $X\sim Normal(\mu,\sigma^2)$ with $\sigma>0$.  In this case, one may interpret the negative values of $X$ as profit or revenue.  Give a closed-form expression for the $VaR$.

`r HideExample('13.3.2','Show Example Solution')`

`r SolnBegin()` Because normal distribution is a continuous distribution, the $VaR$ of $X$ must satisfy
\begin{eqnarray*}
 q &=& F_X(VaR_q[X])\\
&=&\Pr\left[(X-\mu)/\sigma\leq (VaR_q[X]-\mu)/\sigma\right]\\
&=&\Phi((VaR_q[X]-\mu)/\sigma).
\end{eqnarray*}
Therefore, we have
$$
VaR_q[X]=\Phi^{-1}(q)\ \sigma+\mu.
$$
`r SolnEnd()` 

</div>

*** 

In many insurance applications, we have to deal with transformations of random variables.  For instance, in Example 13.3.2, the loss random variable $X\sim Normal(\mu, \sigma^2)$ can be viewed as a linear transformation of a standard normal random variable $Z\sim Normal(0,1)$, namely $X=Z\sigma+\mu$.  By setting $\mu=0$ and $\sigma=1$, it is straightforward for us to check $VaR_q[Z]=\Phi^{-1}(q).$  A useful finding revealed from Example 13.3.2 is that the $VaR$ of a linear transformation of the normal random variables is equivalent to the linear transformation of the $VaR$ of the original random variables.  This finding can be further generalized to any random variables as long as the transformations are strictly increasing. 

**Example 13.3.3. $VaR$ for transformed variables.** Consider an insurance loss random variable $Y$ with a lognormal distribution with parameters $\mu \in \mathbf{R}$ and $\sigma^2>0$. Give an expression of the $VaR$ of $Y$ in terms of the standard normal inverse *cdf*.

`r HideExample('13.3.3','Show Example Solution')`

`r SolnBegin()` Note that $\log Y\sim Normal(\mu,\sigma^2)$, or equivalently let $X\sim Normal(\mu,\sigma^2)$, then $Y\overset{d}{=}e^{X}$ which is strictly increasing transformation.  Here, the notation `$\overset{d}{=}$' means equality in distribution.  The $VaR$ of $Y$ is thus given by the exponential transformation of the $VaR$ of $X$.  Precisely, for $q\in (0,1)$,
$$
VaR_{q}[Y]= e^{VaR_q[X]}=\exp\{\Phi^{-1}(q)\ \sigma+\mu\}.
$$
`r SolnEnd()` 

</div>

*** 

We have thus far seen a number of examples about the $VaR$ for continuous random variables, let us consider an example concerning the $VaR$ for a discrete random variable.


**Example 13.3.4. $VaR$ for a discrete random variable.** Consider an insurance loss random variable with the following probability distribution:
$$
{\small
\Pr[X=x] = \left\{
                  \begin{array}{ll}
                    0.75, & \text{for }x=1 \\
                    0.20, & \text{for }x=3 \\
                    0.05, & \text{for }x=4.
                  \end{array}
                \right.
}
$$
Determine the $VaR$ at $q = 0.6, 0.9, 0.95, 0.95001$.

`r HideExample('13.3.4','Show Example Solution')`

`r SolnBegin()` The corresponding $cdf$ of $X$ is
$$
F_X(x)=\left\{
         \begin{array}{lc}
           0, & x<1 \\
           0.75, &  1\leq x<3 \\
           0.95, & 3\leq x<4 \\
           1, & 4\leq x.
         \end{array}
       \right.
$$
By the definition of $VaR$, we thus have 

$VaR_{0.6}[X]=1$ , $VaR_{0.9}[X]=3$, $VaR_{0.95}[X]=3$, and
$VaR_{0.950001}[X]=4$.
`r SolnEnd()` 

</div>

*** 

Let us now conclude the current subsection by an open discussion of the $VaR$ measure.  Some advantages of utilizing $VaR$ include  

* possessing a practically meaningful interpretation, and
* relatively simple to compute for many distributions with closed-form distribution functions.

On the other hand, the limitations of $VaR$ can be particularly pronounced for some risk management practices.  We report some of them herein:  

* the selection of the confidence level $q\in (0,1)$ is highly subjective, while the $VaR$ can be very sensitive to the choice of $q$ (e.g., in Example 13.3.4, $VaR_{0.95}[X]=3$ and $VaR_{0.950001}[X]=4$);
* the scenarios/loss information that are above the $(1-q)\times 100\%$ worst event, are completely neglected;
* as will be seen in Section \@ref(S:Sec1333), the $VaR$ is not a coherent risk measure.

The $VaR$ represents the $(1-q)\times100\%$ chance maximal loss. One major drawback of the $VaR$ measure is that it does not reflect the extremal losses occurring beyond the $(1-q)\times100\%$ chance worst scenario.  For illustrative purposes, let us consider the following slightly unrealistic yet inspiring example.  

**Example 13.3.5.** Consider two loss random variable's $X\sim Uniform [0,100]$, and $Y$ with an exponential distribution having parameter $\theta=31.71$.  We use $VaR$ at $95\%$ confidence level to measure the riskiness of $X$ and $Y$.  Simple calculation yields (see, also, Example 13.3.1),
$$
VaR_{0.95}[X]=VaR_{0.95}[Y]=95,
$$
and thus these two loss distributions have the same level of risk according to $VaR_{0.95}$.  However, $Y$ is riskier than $X$ if extremal losses are of major concern since $X$ is bounded above while $Y$ is unbounded. Simply quantifying risk by using $VaR$ at a specific confidence level could be misleading and may not reflect the true nature of risk.


### Expected Shortfall

Another commonly used risk measure is the **expected shortfall**, $ES$. Mathematically, we can express this as
\begin{equation}
ES_{q}(X) = \frac{1}{1-q} \int_{q}^{1} VaR_{a}(X) d a .
(\#eq:ESDefined)
\end{equation}
That is, the $ES$ is the average of $VaR_{\alpha}[X]$ with varying degree of confidence level over $\alpha\in [q,1]$. Thus, it is also known as the *average value at risk.*  In this respect, one can see that for any given $q \in (0,1)$ 
$$
ES_q[X] \geq VaR_q[X].
$$
The $ES$ effectively resolves most of the limitations of $VaR$ outlined in the previous subsection.  First, due to the averaging effect, the $ES$ may be less sensitive to the change of confidence level compared with $VaR$.  Second, all the extremal losses that are above the $(1-q)\times 100\%$ worst probable event are taken in account.


There are a few other forms of the $ES$ that will be useful to us. For notional convenience, we write $\pi_q = VaR_q[X]$ and have
\begin{equation}
ES_{q}(X) = 
\left\{\begin{array}{cl}
\frac{1}{1-q} \int_{q}^{1} VaR_{a}(X) d a & \text{Expected Shortfall}\\
\pi_q  + \frac{1}{1- q} 
\left\{ \mathrm{E} [X] - \mathrm{E} [X \wedge \pi_q ] \right\} & \text{Tail VaR}\\
  \mathrm{E} (X | X > \pi_q ) & \text{Conditional VaR} .
\end{array} \right.
(\#eq:ESExpressions)  
\end{equation}
The different expressions in Display \@ref(eq:ESExpressions) hold under some additional (mild) assumptions on the continuity of the distribution function at the point $\pi_q$. As we are interested in applications to portfolios, we employ such assumptions in this chapter which allows us to describe alternative ways of thinking about these measures. For example, from the third expression, we see that $ES$ can also be interpreted to be the expected amount given that the loss exceeds the $VaR_{q}$. 

Naturally, analysts may work with distributions where the assumptions of continuity do not hold, such as discrete distributions (see the examples Chapter 3). For these distributions, 
Display \@ref(eq:ESExpressions) provides a definition for some alternative risk measures, the *Tail value-at-risk* and the *Conditional value-at-risk*. You can learn more about these alternative risk measures in the references given in Section \@ref(S:Sec136). 

`r HideProofTheory('Section21ES.Hide',"Show Development of the ES")`

`r LObjBegin()` 

To see the connections between the second and third equalities, use a variable substitution, $z=VaR_{a}(X)=F^{-1}(a)$ so that $F(z)=a$ and $f(z)dz=da$. With this, we have
$$
\begin{array}{ll}
  \int_{a}^{b} VaR_{a}(X) ~d a &=  \int_{a}^{b} F^{-1}_{a} ~d a    =   \int_{F^{-1}_a}^{F^{-1}_b} z f(z) dz \\ 
  & =   \left.-z[1-F(z)]\right|_{F^{-1}_a}^{F^{-1}_b} + \int_{F^{-1}_a}^{F^{-1}_b} [1-F(z)] dz \\ 
    & =   F^{-1}_{a}(1-a) -F^{-1}_{b}(1-b) +  \\
    &   \ \ \ \ \ \ \ \ \ [\mathrm{E} (X \wedge F^{-1}_{b})- \mathrm{E} (X \wedge F^{-1}_{a})]  .\\ 
  \end{array}
$$
Thus, 
$$
\begin{array}{ll}
  \frac{1}{1-q} \int_{q}^{1} VaR_{a}(X) d a   
    & =   \frac{1}{1-q} \left\{\pi_q (1-q) + 
    [\mathrm{E} (X ) - \mathrm{E} (X \wedge \pi_q )] \right\} \\ 
        & =  \pi_q + \frac{1}{1-q} 
    [\mathrm{E} (X ) - \mathrm{E} (X \wedge \pi_q )]  ,\\ 
  \end{array}
$$
as claimed.
`r LObjEnd()` 

***


</div>

Using the third expression in Display \@ref(eq:ESExpressions), the computation of $ES$ consists of two major steps - the $VaR$ and the average of losses that are above the $VaR$. From this and a change of variables, the $ES$ can be computed via
\begin{eqnarray}
ES_{q}[X]=\frac{1}{(1-q)}\int_{\pi_q}^{\infty}xf_X(x)dx.
(\#eq:cte-pdf)
\end{eqnarray}


**Example 13.3.6. $ES$ for a normal distribution.** Consider an insurance loss random variable\ $X\sim Normal (\mu,\sigma^2)$ with $\mu\in \mathbf{R}$ and $\sigma>0$. Give an expression for $ES$.

`r HideExample('13.3.6','Show Example Solution')`

`r SolnBegin()` Let $Z$ be the standard normal random variable. For $q\in(0,1)$, the $ES$ of $X$ can be computed via
\begin{eqnarray*}
ES_q[X] &=& \mathrm{E}[X|X>VaR_q[X]]\\
&=&\mathrm{E}[\sigma Z+\mu|\sigma Z+\mu>VaR_q[X]]\\
&=& \sigma\mathrm{E}[Z|Z>(VaR_q[X]-\mu)/\sigma]+\mu\\
&\overset{(1)}{=}& \sigma\mathrm{E}[Z|Z>VaR_q[Z]]+\mu,
\end{eqnarray*}
where `$\overset{(1)}{=}$' holds because of the results reported in Example 13.3.2.  Next, we turn to study $ES_q[Z]=\mathrm{E}[Z|Z>VaR_q[Z]]$.  Let $\omega(q)=[\Phi^{-1}(q)]^2/2$, we have
\begin{eqnarray*}
  (1-q)\ ES_q[Z] &=& \int_{\Phi^{-1}(q)}^{\infty} z \frac{1}{\sqrt{2\pi}} e^{-z^2/2}dz\\
&=& \int_{\omega(q)}^{\infty}  \frac{1}{\sqrt{2\pi}} e^{-x}dx\\
&=& \frac{1}{\sqrt{2\pi}} e^{-\omega(q)}\\
&=& \phi[\Phi^{-1}(q)].
\end{eqnarray*}
Thus,
$$
ES_q[X]=\sigma\frac{\phi[\Phi^{-1}(q)]}{1-q}+\mu.
$$
`r SolnEnd()` 

</div>

*** 

We mentioned earlier in the previous subsection that the $VaR$ of a strictly increasing function of random variable is equal to the function of $VaR$ of the original random variable.  Motivated by the results in Example 13.3.6, one can show that the $ES$ of a strictly increasing linear transformation of random variable is equal to the function of $VaR$ of the original random variable. This is due to the linearity property of expectations.  However, the aforementioned  finding cannot be extended to non-linear functions.  The following example of lognormal random variable serves as a counter example.  

**Example 13.3.7. $ES$ of a lognormal distribution.** Consider an insurance loss random variable $X$ with a lognormal distribution having parameters $\mu \in \mathbf{R}$ and $\sigma>0$. Show that
$$
ES_q[X] = \frac{e^{\mu+\sigma^2/2}}{(1-q)} \Phi(\Phi^{-1}(q)-\sigma).
$$

`r HideExample('13.3.7','Show Example Solution')`

`r SolnBegin()` Recall that the $pdf$ of lognormal distribution is formulated as
$$
f_X(x)=\frac{1}{\sigma\sqrt{2\pi} x}\exp\{-(\log x-\mu )^2/2\sigma^2 \}, \text{ for } x>0.
$$
Fix $q \in (0,1)$, then the expected shortfall can be computed via
\begin{eqnarray}
  ES_q[X] &=& \frac{1}{(1-q)} \int_{\pi_q}^{\infty} x f_X(x)dx \nonumber\\
&=&\frac{1}{(1-q)} \int_{\pi_q}^{\infty} \frac{1}{\sigma \sqrt{2\pi}} \exp\left\{ -\frac{(\log x-\mu)^2}{2\sigma^2}
\right\}dx\nonumber\\
&\overset{(1)}{=}&\frac{1}{(1-q)} \int_{\omega(q)}^{\infty} \frac{1}{\sqrt{2\pi}} e^{ -\frac{1}{2}w^2+\sigma w+\mu}dw\nonumber\\
&=&\frac{e^{\mu+\sigma^2/2}}{(1-q)} \int_{\omega(q)}^{\infty} \frac{1}{\sqrt{2\pi}} e^{ -\frac{1}{2}(w-\sigma)^2}dw\nonumber\\
&=&\frac{e^{\mu+\sigma^2/2}}{(1-q)} \Phi(\omega(q)-\sigma),
(\#eq:cte-normal)
\end{eqnarray}
where $\overset{(1)}{=}$ holds by applying change of variable $w=(\log x-\mu)/\sigma$, and $\omega(q)=(\log \pi_q-\mu)/\sigma$.  Evoking the formula of $VaR$ for lognormal random variable reported in Example  13.3.2, we can simplify the expression \@ref(eq:cte-normal) into
\begin{eqnarray*}
  ES_q[X] &=& \frac{e^{\mu+\sigma^2/2}}{(1-q)} \Phi(\Phi^{-1}(q)-\sigma).
\end{eqnarray*}
`r SolnEnd()` 

</div>

*** 

Clearly, the $ES$ of lognormal random variable is not the exponential of the $ES$ of normal random variable.

For distributions of which the survival distribution functions are more tractable to work with, we may apply the integration by parts technique (assuming the mean is finite) to rewrite equation \@ref(eq:cte-pdf) as
\begin{eqnarray*}
ES_{q}[X]&=&\left[-x S_X(x)\big |_{\pi_q}^{\infty}+\int_{\pi_q}^{\infty}S_X(x)dx\right]\frac{1}{(1-q)}\\
&=& \pi_q +\frac{1}{(1-q)}\int_{\pi_q}^{\infty}S_X(x)dx.
\end{eqnarray*}


**Example  13.3.8. $ES$ of an exponential distribution.** Consider an insurance loss random variable $X$ with an exponential distribution having parameter $\theta$ for $\theta>0$. Give an expression for the $ES$.

`r HideExample('13.3.8','Show Example Solution')`

`r SolnBegin()` We have seen from the previous subsection that
$$
\pi_q=-\theta[\log(1-q)].
$$
Let us now consider the $ES$:
\begin{eqnarray*}
  ES_q[X] &=& \pi_q+\int_{\pi_q}^{\infty} e^{-x/\theta}dx/(1-q)\\
&=& \pi_q+\theta e^{-\pi_q/\theta}/(1-q)\\
&=& \pi_q+\theta.
\end{eqnarray*}
`r SolnEnd()` 

</div>

*** 

The second expression in Display \@ref(eq:ESExpressions) shows how to express the $ES$ in terms of limited expected values. For many commonly used parametric distributions, the formulas for calculating $\mathrm{E}[X]$ and $\mathrm{E}[X\wedge\pi_q]$ can be found in a table of distributions.


**Example  13.3.9. $ES$ of a Pareto distribution.** Consider a loss random variable $X\sim Pareto(\theta,\alpha)$ with $\theta>0$ and $\alpha>0$.  The *cdf* of $X$ is given by
$$
F_X(x)=1-\left(\frac{\theta}{\theta+x} \right)^{\alpha}, \text{ for } x>0 .
$$ 
Fix $q\in (0,1)$ and set $F_X(\pi_q)=q$, we readily obtain
\begin{eqnarray}
\pi_q=\theta\left[(1-q)^{-1/\alpha}-1 \right].
(\#eq:var-pareto)
\end{eqnarray}
From Section \@ref(S:ContinuousDistributions), we know that $\mathrm{E}[X]=\frac{\theta}{\alpha-1}$, and 
$$
\mathrm{E}[X\wedge \pi_q]=
\frac{\theta}{\alpha-1}
\left[ 1-\left(\frac{\theta}{\theta+\pi_q}\right)^{\alpha-1} \right].
$$
The second expression in Display \@ref(eq:ESExpressions) yields
\begin{eqnarray*}
ES_q[X] &=& \pi_q+\frac{\theta}{\alpha-1} \frac{[\theta/(\theta+\pi_q)]^{\alpha-1}}
{(\theta/(\theta+\pi_q))^{\alpha}}\\
&=&\pi_q +\frac{\theta}{\alpha-1}\left( \frac{\pi_q+\theta}{\theta} \right)\\
&=& \pi_q+\frac{\pi_q+\theta}{\alpha-1},
\end{eqnarray*}
where $\pi_q$ is given by \@ref(eq:var-pareto). 


### Coherent Risk Measures {#S:Sec1333}

The $VaR$ and $ES$ are widely used risk measures but how does the analyst know which one to employ? Broadly speaking, we seek a function that maps the loss random variable of interest to a numerical value indicating the level of riskiness, which is termed the `r Gloss('risk measure')`.  Put mathematically, the risk measure simply summarizes the distribution function of a random variable as a single number. 

The $VaR$ and $ES$ are risk measures but one might also consider two simpler alternatives, the mean $\mathrm{E}[X]$ and the standard deviation $\mathrm{SD}(X)$ $=\sqrt{\mathrm{Var}(X)}$. In addition, other classical special cases include the *standard deviation principle*
\begin{equation}
H_{\mathrm{SD}}(X)=\mathrm{E}[X]+\alpha \mathrm{SD}(X),\text{ for } \alpha\geq 0,
(\#eq:SD-principle) 
\end{equation}
and the *variance principle*
$$
H_{\mathrm{Var}}(X)=\mathrm{E}[X]+\alpha \mathrm{Var}(X),\text{ for } \alpha\geq 0.
$$
One can check that all the aforementioned functions are risk measures in which we input the loss random variable and the functions output a numerical value.  In contrast, the function $H^{\ast}(X)=\alpha X^{\beta}$ for any real-valued $\alpha,\beta\neq 0$, is not a risk measure because $H^{\ast}$ produces another random variable rather than a single numerical value.

Because risk measures are scalar measures which aim to describe the stochastic uncertainty of loss random variables distributions, it is not surprising that no risk measure can capture all the risk information of the associated random variables.  Therefore, when seeking useful risk measures, it is important for us to keep in mind that the measures should be:

* interpretable practically,  
* computable conveniently, and  
* able to reflect the most critical information of risk underpinning the loss distribution.  

Several risk measures have been developed in the literature. Unfortunately, there is no best risk measure that can outperform the others, and the selection of appropriate risk measure depends on the application questions at hand.  In this respect, there are multiple approaches to assess the uncertainty.  However, for many risk management applications, there is a wide agreement that economically grounded risk measures should satisfy four major axioms, described as follows. 

Consider a risk measure $H(\cdot)$. It is said to be a `r Gloss('coherent risk measure')` for two random variables $X$ and $Y$ if the following axioms are satisfied.  

*  **Axiom 1.** *Subadditivity:* $H(X+Y)\leq H(X)+H(Y)$.  
   *  The economic implication of this axiom is that diversification benefits exist if different risks are combined.  
*  **Axiom 2.** *Monotonicity:* if $\Pr[X\leq Y]=1$, then $H(X)\leq H(Y)$.
   *  Recall that $X$ and $Y$ are random variables representing losses, the underlying economic implication is that higher losses essentially leads to a higher level of risk.    
*  **Axiom 3.** *Positive homogeneity:* $H(cX)=cH(X)$ for any positive constant $c$.  
   *  A potential economic implication about this axiom is that risk measure should be independent of the monetary units in which the risk is measured.  For example, let $c$ be the currency exchange rate between the US and Canadian dollars, then the risk of random losses measured in terms of US dollars (i.e., $X$) and Canadian dollars (i.e., $cX$) should be different only up to the exchange rate $c$ (i.e., $cH(x)=H(cX)$).  
*  **Axiom 4.** *Translation invariance:* $H(X+c)=H(X)+c$ for any positive constant $c$.  
   *  If the constant $c$ is interpreted as risk-free cash and $X$ is an insurance portfolio, then adding cash to a portfolio only increases the portfolio risk by the amount of cash.

Verifying these properties can be straightforward but can be also be challenging at times.  For example, it is a simple matter to check that the mean is a coherent risk measure.

**Special Case. The Mean is a Coherent Risk Measure.**

For any pair of random variables $X$ and $Y$ having finite means and constant $c>0$,

* validation of *subadditivity*: $\mathrm{E}[X+Y]=\mathrm{E}[X]+\mathrm{E}[Y]$;
* validation of *monotonicity*: if $\Pr[X\leq Y]=1$, then $\mathrm{E}[X]\leq \mathrm{E}[Y]$;
* validation of *positive homogeneity*: $\mathrm{E}[cX]=c\mathrm{E}[X]$;
* validation of *translation invariance*: $\mathrm{E}[X+c]=\mathrm{E}[X]+c$

***

With a little more effort, we can determine the following.

**Special Case. The Standard Deviation is not a Coherent Risk Measure.**

`r HideExample('PortMgt.3.1', 'Show Verification of the Special Case')`

`r LObjBegin()`
$\textbf{Verification of the Special Case}$. To see that the standard deviation is not a coherent risk measure, start by checking that the standard deviation satisfies

Validation of $\textit{Subadditivity}$. 
\begin{eqnarray*} 
\mathrm{SD}[X+Y]&=&\sqrt{\mathrm{Var}(X)+\mathrm{Var}(Y)+2\mathrm{Cov}(X,Y)}\\
      &\leq& \sqrt{\mathrm{SD}(X)^2+\mathrm{SD}(Y)^2+2\mathrm{SD}(X)\mathrm{SD}(Y)}\\
      &=& \mathrm{SD}(X)+\mathrm{SD}(Y);
\end{eqnarray*}
Validation of $\textit{Positive Homogeneity}$: $\mathrm{SD}[cX]=c~\mathrm{SD}[X]$.  
However, the standard deviation does not comply with translation invariance property as for any positive constant $c$,
$$
\mathrm{SD}(X+c)=\mathrm{SD}(X)<\mathrm{SD}(X)+c.
$$
Moreover, the standard deviation also does not satisfy the monotonicity property.  To see this, consider the following two random variables:
\begin{eqnarray}
X=\left\{
    \begin{array}{ll}
      0, & \text{with probability }0.25 \\
      4, & \text{with probability }0.75,
    \end{array}
  \right.
(\#eq:special-x)
\end{eqnarray}
and $Y$ is a degenerate random variable such that
\begin{eqnarray}
\Pr[Y = 4] = 1.
(\#eq:special-y)
\end{eqnarray}
You can check that $\Pr[X\leq Y]=1$, but $\mathrm{SD}(X)=\sqrt{4^2\cdot 0.25\cdot 0.75}=\sqrt{3}>\mathrm{SD}(Y)=0$.
`r LObjEnd()` 

</div>

*** 

We have so far checked that $\mathrm{E}[\cdot]$ is a coherent risk measure and that $\mathrm{SD}(\cdot)$ is not. [Exercise 13.1](Exer:13.1) asks you to study the coherent property for the standard deviation principle \@ref(eq:SD-principle) which is a linear combination of coherent and incoherent risk measures. 

It turns out that the $VaR$ is not a coherent risk measure. Specifically, the $VaR$ measure does not satisfy the subadditivity axiom, meaning that diversification benefits may not be fully reflected.

In contrast, $ES$ is a coherent risk measure and thus is able to more accurately capture the diversification effects of insurance portfolio. Herein, we do not intend to provide the proof of the coherent feature for $ES$, which is considered to be challenging technically.



```{r child = './Quizzes/Quiz13A3.html', eval = QUIZ}
```



## Reinsurance {#S:Sec134}


***

In this section, you learn how to:

- Define basic reinsurance treaties including proportional, quota share, non-proportional, stop-loss, excess of loss, and surplus share.
- Interpret the optimality of quota share for reinsurers and compute optimal quota share agreements. 
- Interpret the optimality of stop-loss for insurers.
- Interpret and calculate optimal excess of loss retention limits.

***


Recall from Section \@ref(S:Sec514) that `r Gloss('reinsurance', '1.1, 10.4')` is simply insurance purchased by an insurer. Insurance purchased by non-insurers is sometimes known as  `r Gloss('primary insurance')` to distinguish it from reinsurance. Reinsurance differs from personal insurance purchased by individuals, such as auto and homeowners insurance, in contract flexibility. Like insurance purchased by major corporations, reinsurance programs are generally tailored more closely to the buyer. For contrast, in personal insurance buyers typically cannot negotiate on the contract terms although they may have a variety of different options (contracts) from which to choose.

The two broad types are proportional and non-proportional reinsurance. A 
`r Gloss('proportional reinsurance')` contract is an agreement between a reinsurer and a `r Gloss('ceding company')` (also known as the `r Gloss('reinsured')`) in which the reinsurer assumes a given percent of losses and premium. A reinsurance contract is also known as a `r Gloss('treaty')`. Non-proportional agreements are simply everything else. As examples of non-proportional agreements, this chapter focuses on stop-loss and excess of loss contracts. For all types of agreements, we split the total risk $X$ into the portion taken on by the reinsurer, $Y_{reinsurer}$, and that retained by the insurer, $Y_{insurer}$, that is, $X= Y_{insurer}+Y_{reinsurer}$.

The mathematical structure of a basic reinsurance treaty is the same as the coverage modifications of personal insurance introduced in Chapter \@ref(ChapClaimSeverity). For a proportional reinsurance, the transformation $Y_{insurer} = c X$ is identical to a coinsurance adjustment in personal insurance. For stop-loss reinsurance, the transformation $Y_{reinsurer} = \max(0,X-M)$ is the same as an insurer's payment with deductible $M$ and $Y_{insurer} = \min(X,M) = X \wedge M$ is equivalent to what a policyholder pays with deductible $M$. For practical applications of the mathematics, in personal insurance the focus is generally upon the expectation as this is a key ingredient used in pricing. In contrast, for reinsurance the focus is on the entire distribution of the risk, as the extreme events are a primary concern of the financial stability of the insurer and reinsurer.

This section describes the foundational and most basic of reinsurance treaties: Section \@ref(S:Sec1341) for proportional and Section \@ref(S:Sec1342) for non-proportional reinsurance. Section \@ref(S:Sec1343) gives a flavor of more complex contracts.


### Proportional Reinsurance {#S:Sec1341}

The simplest example of a proportional treaty is called `r Gloss('quota share')`.

-   In a quota share treaty, the reinsurer receives a flat percent, say 50%, of the premium for the book of business reinsured.
-   In exchange, the reinsurer pays 50% of losses, including allocated loss adjustment expenses.
-   The reinsurer also pays the ceding company a ceding commission which is designed to reflect the differences in underwriting expenses incurred.
        
The amounts paid by the primary insurer and the reinsurer are summarized as
$$
Y_{insurer} = c X \ \ \text{and} \ \ \ Y_{reinsurer} = (1-c) X,
$$
where $c\in (0,1)$ denotes the proportion retained by the insurer. Note that $Y_{insurer}+Y_{reinsurer}=X$.

**Example 13.4.1. Distribution of losses under quota share.** To develop an intuition for the effect of quota-share agreement on the distribution of losses, the following is a short `R` demonstration using simulation. The accompanying figure provides the relative shapes of the distributions of total losses, the retained portion (of the insurer), and the reinsurer's portion.

```{r   echo=FALSE, fig.width=10, fig.height=4}
set.seed(2018)
theta = 1000
alpha = 3
nSim = 10000
library(actuar)
X <-  rpareto(nSim, shape = alpha, scale = theta)

par(mfrow=c(1,3))
plot(density(X), xlim=c(0,3*theta), ylim=c(0,0.008), main="Total Loss", xlab="Losses")
plot(density(0.75*X), xlim=c(0,3*theta), ylim=c(0,0.008), main="Insurer (75%)", xlab="Losses")
plot(density(0.25*X), xlim=c(0,3*theta), ylim=c(0,0.008), main="Reinsurer (25%)", xlab="Losses")
```

`r HideRCode('QuotaShare', 'Show the R Code')`

```{r  echo=HtmlEval,  eval=FALSE, fig.width=10, fig.height=4}
set.seed(2018)
theta = 1000
alpha = 3
nSim = 10000
library(actuar)
X <-  rpareto(nSim, shape = alpha, scale = theta)

par(mfrow=c(1,3))
plot(density(X), xlim=c(0,3*theta), ylim=c(0,0.008), main="Total Loss", xlab="Losses")
plot(density(0.75*X), xlim=c(0,3*theta), ylim=c(0,0.008), main="Insurer (75%)", xlab="Losses")
plot(density(0.25*X), xlim=c(0,3*theta), ylim=c(0,0.008), main="Reinsurer (25%)", xlab="Losses")
```


</div>

#### Quota Share is Desirable for Reinsurers {-}

The quota share contract is particularly desirable for the reinsurer. To see this, suppose that an insurer and reinsurer wish to enter a contract to share total losses $X$ such that 
$$
Y_{insurer}=g(X) \ \ \ \text{and} \ \ \ \ Y_{reinsurer}=X-g(X),
$$
for some generic function $g(\cdot)$ (known as the *retention* function). So that the insurer does not retain more than the loss, we consider only functions so that $g(x) \le x$. Suppose further that the insurer only cares about the variability of retained claims and is indifferent to the choice of $g$ as long as $\mathrm{Var}(Y_{insurer})$ stays the same and equals, say, $Q$. Then, the following result shows that the quota share reinsurance treaty minimizes the reinsurer's uncertainty as measured by $\mathrm{Var}(Y_{reinsurer})$.

**Proposition**. Suppose that $\mathrm{Var}(Y_{insurer})=Q$ and assume that $Q \le \mathrm{Var}(X)$. Then, $\mathrm{Var} [(1-c)X] \le \mathrm{Var}(Y_{reinsurer})$ for all $g(\cdot)$.

`r HideProofTheory('proof', 'Show the Justification of the Proposition')`

`r LObjBegin()` 
$\textbf{Proof of the Proposition}$. With $Y_{reinsurer} = X - Y_{insurer}$ and the law of total variation
$$
\begin{array}{ll}
\mathrm{Var} (Y_{reinsurer}) &= \mathrm{Var} (X-Y_{insurer}) \\
&= \mathrm{Var} (X) + \mathrm{Var} (Y_{insurer})  - 2 Cov (X,Y_{insurer}) \\
&=\mathrm{Var} (X) + Q - 2 Corr (X,Y_{insurer}) \times \sqrt{Q} \sqrt{\mathrm{Var} (X)} .
\end{array}
$$
In this expression, we see that $Q$ and $\mathrm{Var}(X)$ do not change with the choice of the retention function $g$. Thus, we can minimize $\mathrm{Var} (Y_{reinsurer})$ by maximizing the correlation $Corr (X,Y_{insurer})$. If we use a quota share reinsurance agreement, then $Corr (X,Y_{insurer})=Corr (X,c X)=1$, the maximum possible correlation. This establishes the proposition.
`r LObjEnd()`

</div>

***

The proposition is intuitively appealing - with quota share insurance, the insurer and reinsurer share the responsibility for very large claims in the tail of the distribution. This is in contrast to non-proportional agreements where reinsurers take responsibility for the very large claims.

#### Optimizing Quota Share Agreements for Insurers {-}

Now assume $n$ risks in the portfolio, $X_1, \ldots, X_n,$ so that the portfolio sum is $X= X_1 + \cdots + X_n$. For simplicity, we focus on the case of independent risks (extensions to dependence is the subject of Chapter \@ref(ChapDependenceModel)). Each risk $X_i$ may represent risk of an individual policy, claim, or a sub-portfolio, depending on the application. As an example of the latter, the insurer may subdivide its portfolio into subportfolios consisting of lines of business such as (1) personal auto, (2) commercial auto, (3) homeowners, (4) workers' compensation, and so forth.

In general, let us consider a variation of the basic quota share agreement where the amount retained by the insurer may vary with each risk, say $c_i$. Thus, the insurer's portion of the portfolio risk is $Y_{insurer} = \sum_{i=1}^n c_i X_i$. What is the best choice of the proportions $c_i$?

To formalize this question, we seek to find those values of $c_i$ that minimize $\mathrm{Var}  (Y_{insurer})$ subject to the constraint that $\mathrm{E} (Y_{insurer}) = K.$ The requirement that $\mathrm{E} (Y_{insurer}) = K$ suggests that the insurers wishes to retain a revenue in at least the amount of the constant $K$. Subject to this revenue constraint, the insurer wishes to minimize the uncertainty of the retained risks as measured by the variance.

`r HideProofTheory('derivationProof', 'Show the Optimal Retention Proportions')`

`r LObjBegin()`

$\textbf{The Optimal Retention Proportions}$. Minimizing $\mathrm{Var}(Y_{insurer})$ subject to  $\mathrm{E}(Y_{insurer}) = K$ is a constrained optimization problem. We can use the method of Lagrange multipliers, a calculus technique, to solve this. To this end, define the Lagrangian
$$
\begin{array}{ll}
L &= \mathrm{Var} (Y_{insurer}) - \lambda (\mathrm{E} (Y_{insurer}) - K) \\
&= \sum_{i=1}^n c_i^2 ~\mathrm{Var}(X_i) - \lambda (\sum_{i=1}^n c_i ~\mathrm{E}(X_i) - K) 
\end{array}
$$
Taking a partial derivative with respect to $\lambda$ and setting this equal to zero simply means that the constraint, $\mathrm{E}(Y_{insurer}) = K$, is enforced and we have to choose the proportions $c_i$ to satisfy this constraint. Moreover, taking the partial derivative with respect to each proportion $c_i$ yields
$$
\frac{\partial}{\partial c_i} L = 2 c_i ~\mathrm{Var}(X_i) - \lambda ~\mathrm{E}(X_i) = 0 
$$
so that
$$
c_i  =  \frac{\lambda}{2} \frac{\mathrm{E}(X_i)}{\mathrm{Var}(X_i)} .
$$
With our constraint, we may determine $\lambda$ as the solution of
$$
\begin{array}{ll}
K &= \sum_{i=1}^n c_i \mathrm{E}(X_i) \\
&= \frac{\lambda}{2} \sum_{i=1}^n \frac{\mathrm{E}(X_i)^2}{\mathrm{Var}(X_i)} 
\end{array}
$$
and use this value of $\lambda$ to determine the proportions.
`r LObjEnd()` 

***

</div>


From the math, it turns out that the constant for the $i$th risk, $c_i$ is proportional to $\frac{\mathrm{E}(X_i)}{\mathrm{Var} (X_i)}$. This is intuitively appealing. Other things being equal, a higher revenue as measured by $\mathrm{E} (X_i)$ means a higher value of $c_i$. In the same way, a higher value of uncertainty as measured by $\mathrm{Var}(X_i)$ means a lower value of $c_i$. The proportional scaling factor is determined by the revenue requirement $\mathrm{E}(Y_{insurer}) = K$. The following example helps to develop a feel for this relationship.

**Example 13.4.2. Three Pareto risks.** Consider three risks that have a Pareto distribution, each having a different set of parameters (so they are independent but non-identical). Specifically, use the parameters:

*  $\alpha_1 =3$, $\theta_1=1000$ for the first risk $X_1$,
*  $\alpha_2 =3$, $\theta_2=2000$ for the second risk $X_2$, and
*  $\alpha_3 =4$, $\theta_3=3000$ for the third risk $X_3$.

Provide a graph that gives values of $c_1$, $c_2$, and $c_3$ for a required revenue $K$. Note that these values increase linearly with $K$.

`r HideRCode('ParetoRisksProp', 'Show an Example with Three Pareto Risks')`

**Solution.**

```{r   echo=HtmlEval, fig.width=8, fig.height=4}

theta1 = 1000; theta2 = 2000; theta3 = 3000;
alpha1 = 3; alpha2 = 3; alpha3 = 4;
library(actuar)
propnfct <- function(alpha,theta){
  mu    <- mpareto(shape=alpha, scale=theta, order=1)
  var   <- mpareto(shape=alpha, scale=theta, order=2) - mu^2
  mu/var
}
temp <- propnfct(alpha1, theta1)*mpareto(shape=alpha1, scale=theta1, order=1)+
        propnfct(alpha2, theta2)*mpareto(shape=alpha2, scale=theta2, order=1)+
        propnfct(alpha3, theta3)*mpareto(shape=alpha3, scale=theta3, order=1)  
KVec <- seq(100, 2500, length.out=20)
Lambdavec <- 2*KVec/temp
c1 <- propnfct(alpha1, theta1)
c2 <- propnfct(alpha2, theta2)
c3 <- propnfct(alpha3, theta3)
c1Vec <- c2Vec <- c3Vec <- 0*KVec 
for (j in 1:20) {
  c1Vec[j] <- (Lambdavec[j]/2) * propnfct(alpha1, theta1)
  c2Vec[j] <- (Lambdavec[j]/2) * propnfct(alpha2, theta2)
  c3Vec[j] <- (Lambdavec[j]/2) * propnfct(alpha3, theta3)
  }
plot(KVec, c1Vec, type="l", ylab="proportion", xlab="required revenue (K)", ylim=c(0,1))
lines(KVec, c2Vec)
lines(KVec, c3Vec)
text(1200,0.80, expression(c[1]))
text(2000,0.75, expression(c[2]))
text(1500,0.30, expression(c[3]))

```

</div>

### Non-Proportional Reinsurance {#S:Sec1342}

#### The Optimality of Stop-Loss Insurance {-}

Under a `r Gloss('stop-loss')` arrangement, the insurer sets a retention level $M (>0)$ and pays in full total claims for which $X  \le M$. Further, for claims for which $X > M$, the primary insurer pays $M$ and the reinsurer pays the remaining amount $X-M$. That is, the insurer retains an amount $M$ of the risk and the reinsurer pays the excess. Summarizing this mathematically, the amounts paid by the primary insurer and the reinsurer are
$$
Y_{insurer} =
\begin{cases}
X & \text{for } X \le M\\
M & \text{for } X >M \\
\end{cases} \ \ \ \ = \min(X,M) = X \wedge M
$$
and
$$
Y_{reinsurer} =
\begin{cases}
0 & \text{for } X \le M\\
X- M &  \text{for } X >M \\
\end{cases} \ \ \ \  = \max(0,X-M) .
$$
As before, note that $Y_{insurer}+Y_{reinsurer}=X$.

The stop-loss type of contract is particularly desirable for the insurer. Similar to earlier, suppose that an insurer and reinsurer wish to enter a contract so that $Y_{insurer}=g(X)$ and $Y_{reinsurer}=X-g(X)$ for some generic `r Gloss('retention function')` $g(\cdot)$. Suppose further that the insurer only cares about the variability of retained claims and is indifferent to the choice of $g$ as long as $\mathrm{Var}(Y_{insurer})$ can be minimized. Again, we impose the constraint that $\mathrm{E}(Y_{insurer}) = K$; the insurer needs to retain a revenue $K$. Subject to this revenue constraint, the insurer wishes to minimize uncertainty of the retained risks (as measured by the variance). Then, the following result shows that the stop-loss reinsurance treaty minimizes the insurer's uncertainty.


**Proposition**. Suppose that $\mathrm{E}(Y_{insurer})=K$ and choose $M$ such that $\mathrm{E}(X \wedge M)=K$. Then, $\mathrm{Var} (X \wedge M) \le \mathrm{Var}[g(X)]$ for all $g(.)$ such that $\mathrm{E}[g(X)]= K$.

`r HideProofTheory('ProofStopLoss', 'Show the Justification of the Proposition')`

`r LObjBegin()`
$\textbf{Proof of the Proposition}$. Add and subtract a constant $M$ and expand the square to get
$$
\begin{array}{ll}
\mathrm{Var}[g(X)] &= \mathrm{E} [g(X) - K]^2 = \mathrm{E} (g(X) -M +M- K)^2 \\
&= \mathrm{E} [g(X) -M]^2 +  (M- K)^2 +2 \mathrm{E} [g(X) -M](M- K) \\
&= \mathrm{E} [g(X) -M]^2 -  (M- K)^2 ,
\end{array}
$$
because $\mathrm{E}[g(X)]= K.$

Now, for any retention function, we have $g(X) \le X$, that is, the insurer's retained claims are less than or equal to total claims. Using the notation $g_{SL}(X) = X \wedge M$ for stop-loss insurance, we have
$$
\begin{array}{ll}
M- g_{SL}(X) &= M-(X \wedge M) \\
&= \max(M-X, 0) \\
&\le \max(M-g(X), 0) .
\end{array}
$$
Squaring each side yields 
$$
[M- g_{SL}(X)]^2 \le \max([M-g(X)]^2, 0)  \le [M-g(X)]^2.
$$
Returning to our expression for the variance, we have
$$
\begin{array}{ll}
\mathrm{Var}[g_{SL}(X)] &= \mathrm{E} [g_{SL}(X) -M]^2 -  (M- K)^2 \\
&\le \mathrm{E} [g(X) -M]^2 -  (M- K)^2 = \mathrm{Var}[g(X)] ,
\end{array}
$$
for any retention function $g$. This establishes the proposition.
`r LObjEnd()`

</div>

The proposition is intuitively appealing - with stop-loss insurance, the reinsurer takes the responsibility for very large claims in the tail of the distribution, not the insurer. 

#### Excess of Loss {-}

A closely related form of non-proportional reinsurance is the `r Gloss('excess of loss')` coverage. Under this contract, we assume that the total risk $X$ can be thought of as composed as $n$ separate risks $X_1, \ldots, X_n$ and that each of these risks are subject to an upper limit, say, $M_i$. So the insurer retains
$$
 Y_{insurer} = \sum_{i=1}^n Y_{i,insurer}, \ \ \ \ \text{where} \ \ \ \ \ Y_{i,insurer} = X_i \wedge M_i.
$$
and the reinsurer is responsible for the excess, $Y_{reinsurer}=X - Y_{insurer}$. The retention limits may vary by risk or may be the same for all risks, that is, $M_i =M$, for all $i$.

#### Optimal Choice for Excess of Loss Retention Limits {-}

What is the best choice of the excess of loss retention limits $M_i$? To formalize this question, we seek to find those values of $M_i$ that minimize $\mathrm{Var}(Y_{insurer})$ subject to the constraint that $\mathrm{E}(Y_{insurer}) = K.$ Subject to this revenue constraint, the insurer wishes to minimize the uncertainty of the retained risks (as measured by the variance).

`r HideProofTheory('DerivationProofExcess', 'Show the Optimal Retention Proportions')`

`r LObjBegin()`
$\textbf{The Optimal Retention Limits}$. Minimizing $\mathrm{Var}(Y_{insurer})$ subject to  $\mathrm{E}(Y_{insurer}) = K$ is a constrained optimization problem. We can use the method of Lagrange multipliers, a calculus technique, to solve this. As before, define the Lagrangian
$$
\begin{array}{ll}
L &= \mathrm{Var} (Y_{insurer}) - \lambda (\mathrm{E}(Y_{insurer}) - K) \\
&= \sum_{i=1}^n ~\mathrm{Var} (X_i \wedge M_i) - \lambda (\sum_{i=1}^n ~\mathrm{E}(X_i \wedge M_i)- K).
\end{array}
$$
We first recall the relationships
$$
\mathrm{E}(X \wedge M) = \int_0^M ~(1- F(x))dx
$$
and
$$
\mathrm{E}(X \wedge M)^2 = 2\int_0^M ~x(1- F(x))dx.
$$
Taking a partial derivative of $L$ with respect to $\lambda$ and setting this equal to zero simply means that the constraint, $\mathrm{E}(Y_{insurer}) = K$, is enforced and we have to choose the limits $M_i$ to satisfy this constraint. Moreover, taking the partial derivative with respect to each limit $M_i$ yields
$$
\begin{array}{ll}
\frac{\partial}{\partial M_i} L 
&= \frac{\partial}{\partial M_i}  ~\mathrm{Var}(X_i \wedge M_i)  - \lambda \frac{\partial}{\partial M_i} ~\mathrm{E}(X_i \wedge M_i) \\
&= \frac{\partial}{\partial M_i} \left(\mathrm{E}(X_i \wedge M_i)^2 -(\mathrm{E}(X_i \wedge M_i))^2\right) - \lambda (1-F_i(M_i)) \\
&= 2 M_i (1-F_i(M_i)) - 2 \mathrm{E}(X_i \wedge M_i) (1-F_i(M_i))-
\lambda (1-F_i(M_i)).
\end{array}
$$
Setting $\frac{\partial}{\partial M_i} L =0$ and solving for $\lambda$, we get
$$
\lambda = 2 (M_i - \mathrm{E}(X_i \wedge M_i)) .
$$
`r LObjEnd()` 

</div>

From the math, it turns out that the retention limit less the expected insurer's claims, $M_i - \mathrm{E}(X_i \wedge M_i)$, is the same for *all* risks. This is intuitively appealing.


**Example 13.4.3. Excess of loss for three Pareto risks.** Consider three risks that have a Pareto distribution, each having a different set of parameters (so they are independent but non-identical). Use the same set of parameters as in Example 13.4.2. For this example:

a.  Show numerically that the optimal retention limits $M_1$, $M_2$, and $M_3$ resulting retention limit minus expected insurer's claims, $M_i - \mathrm{E}(X_i \wedge M_i)$, is the same for all risks, as we derived theoretically. 
b.  Further, graphically compare the distribution of total risks to that retained by the insurer and by the reinsurer.

`r HideRCode('ParetoRisksExcess', 'Show an Example with Three Pareto Risks')`

**Solution**

**a**. We first optimize the Lagrangian using the `R` package `alabama` for *Augmented Lagrangian Adaptive Barrier Minimization Algorithm*. 


```{r  echo=HtmlEval, fig.width=8, fig.height=4}

theta1 = 1000;theta2 = 2000;theta3 = 3000;
alpha1 = 3;   alpha2 = 3;   alpha3 = 4;
Pmin <- 2000
library(actuar)
VarFct <- function(M){
  M1=M[1];M2=M[2];M3=M[3]
  mu1    <- levpareto(limit=M1,shape=alpha1, scale=theta1, order=1)
  var1   <- levpareto(limit=M1,shape=alpha1, scale=theta1, order=2)-mu1^2
  mu2    <- levpareto(limit=M2,shape=alpha2, scale=theta2, order=1)
  var2   <- levpareto(limit=M2,shape=alpha2, scale=theta2, order=2)-mu2^2
  mu3    <- levpareto(limit=M3,shape=alpha3, scale=theta3, order=1)
  var3   <- levpareto(limit=M3,shape=alpha3, scale=theta3, order=2)-mu3^2
  varFct <- var1 +var2+var3
  meanFct <- mu1+mu2+mu3
  c(meanFct,varFct)
  }
f <- function(M){VarFct(M)[2]}
h <- function(M){VarFct(M)[1] - Pmin}
library(alabama)
par0=rep(1000,3)
op <- auglag(par=par0,fn=f,hin=h,control.outer=list(trace=FALSE))
```


The optimal retention limits $M_1$, $M_2$, and $M_3$ resulting retention limit minus expected insurer's claims, $M_i - \mathrm{E}(X_i \wedge M_i)$, is the same for all risks, as we derived theoretically. 

```{r  fig.width=8, fig.height=4, echo=HtmlEval}

M1star = op$par[1];M2star = op$par[2];M3star = op$par[3]
M1star -levpareto(M1star,shape=alpha1, scale=theta1,order=1)
M2star -levpareto(M2star,shape=alpha2, scale=theta2,order=1)
M3star -levpareto(M3star,shape=alpha3, scale=theta3,order=1)
```

**b**. We graphically compare the distribution of total risks to that retained by the insurer and by the reinsurer.

```{r  fig.width=8, fig.height=4, echo=HtmlEval}
set.seed(2018)
nSim = 10000
library(actuar)
Y1 <- rpareto(nSim, shape = alpha1, scale = theta1)
Y2 <- rpareto(nSim, shape = alpha2, scale = theta2)
Y3 <- rpareto(nSim, shape = alpha3, scale = theta3)
YTotal <- Y1 + Y2 + Y3
Yinsur <-  pmin(Y1,M1star)+pmin(Y2,M2star)+pmin(Y3,M3star)
Yreinsur <- YTotal - Yinsur

par(mfrow=c(1,3))
plot(density(YTotal),   xlim=c(0,10000), main="Total Loss", xlab="Losses")
plot(density(Yinsur),   xlim=c(0,10000), main="Insurer",    xlab="Losses")
plot(density(Yreinsur), xlim=c(0,10000), main="Reinsurer",  xlab="Losses")
```

</div>

***

### Additional Reinsurance Treaties  {#S:Sec1343}

#### Surplus Share Proportional Treaty {-}

Another proportional treaty is known as `r Gloss('surplus share')`; this type of contract is common in commercial property insurance.

-   A surplus share treaty allows the reinsured to limit its exposure on a risk to a given amount (the `r Gloss('retained line')`).
-   The reinsurer assumes a part of the risk in proportion to the amount that the insured value exceeds the retained line, up to a given
    limit (expressed as a multiple of the retained line, or number of lines).

For example, let the retained line be 100,000 and the given limit be 4 lines (400,000). Then, if $X$ is the loss, the reinsurer's portion is $\min(400000, (X-100000)_+)$.


#### Layers of Coverage {-}

One can also extend non-proportional stop-loss treaties by introducing additional parties to the contract. For example, instead of simply an insurer and reinsurer or an insurer and a policyholder, think about the situation with all three parties, a policyholder, insurer, and reinsurer, who agree on how to share a risk. More generally, we consider $k$ parties. If $k=3$, it could be an insurer and two different reinsurers.


**Example 13.4.4. Layers of coverage for three parties.**

-   Suppose that there are $k=3$ parties. The first party is responsible for the first 100 of claims, the second responsible for claims from 100 to 3000, and the third responsible for claims above 3000.
-   If there are four claims in the amounts 50, 600, 1800 and 4000, then they would be allocated to the parties as follows:

  Layer               Claim 1   Claim 2   Claim 3   Claim 4  Total
  ------------------ --------- --------- --------- --------- -------
  (0, 100\]             50        100       100       100    350
  (100, 3000\]           0        500      1700      2900    5100
  (3000, $\infty$)       0         0         0       1000    1000
  Total                 50        600      1800      4000    6450


***

To handle the general situation with $k$ groups, partition the positive real line into $k$ intervals using the cut-points
$$
0 = M_0 < M_1 < \cdots < M_{k-1} < M_k = \infty.
$$
Note that the $j$th interval is $(M_{j-1}, M_j]$. Now let $Y_j$ be the amount of risk shared by the $j$th party. To illustrate, if a loss $x$ is such that $M_{j-1} <x \le M_j$, then
$$
\left(\begin{array}{c}
    Y_1\\ Y_2 \\ \vdots \\ Y_j \\Y_{j+1} \\ \vdots \\Y_k
    \end{array}\right)
    =\left(\begin{array}{c}
    M_1-M_0 \\ M_2-M_1  \\ \vdots \\ x-M_{j-1}  \\ 0 \\ \vdots \\0
    \end{array}\right)
$$

More succinctly, we can write
$$
Y_j = \min(X,M_j) - \min(X,M_{j-1}) .
$$
With the expression $Y_j = \min(X,M_j) - \min(X,M_{j-1})$, we see that the $j$th party is responsible for claims in the interval  $(M_{j-1}, M_j].$ With this, you can check that $X = Y_1 + Y_2 + \cdots + Y_k.$ As emphasized in the following example, we also remark that the parties need not be different.


**Example 13.4.5.**

-   Suppose that a policyholder is responsible for the first 100 of claims and all claims in excess of 100,000. The insurer takes claims between 100 and 100,000.
-   Then, we would use $M_1 = 100$, $M_2 =100000$.
-   The policyholder is responsible for $Y_1 =\min(X,100)$ and  $Y_3 = X - \min(X,100000) = \max(0, X-100000)$.


For additional reading, see the [Wisconsin Property Fund site](https://sites.google.com/a/wisc.edu/local-government-property-insurance-fund/home/reinsurance) for an example on layers of reinsurance.
    

#### Portfolio Management Example {-}

Many other variations of the foundational contracts are possible. For one more illustration, consider the following.


```{r   echo=FALSE}
# For the gamma distributions, use
alpha1 <- 2;      theta1 <- 100
alpha2 <- 2;      theta2 <- 200
# For the Pareto distributions, use
alpha3 <- 2;      theta3 <- 1000
alpha4 <- 3;      theta4 <- 2000
# Deductibles
M1 <- 100
M2 <- 200
```

<a id=Ex:13.4.6></a>  

[Example 13.4.6]: ./ChapPortMgt.html#Ex:13.4.6

**[Example 13.4.6]{#Ex:13.4.6}. Portfolio Management.** You are the Chief Risk Officer of a telecommunications firm. Your firm has several property and liability risks. We will consider:

- $X_1$ - buildings, modeled using a gamma distribution with mean `r alpha1*theta1` and scale parameter `r theta1`.
- $X_2$ - motor vehicles, modeled using a gamma distribution with mean `r alpha2*theta2` and scale parameter `r theta2`.
- $X_3$ - directors and executive officers risk, modeled using a Pareto distribution with mean `r round(theta3/(alpha3-1),digits=8)` and scale parameter `r theta3`.
- $X_4$ - cyber risks, modeled using a Pareto distribution with mean `r theta4/(alpha4-1)` and scale parameter `r theta4`.

Denote the total risk as $X = X_1 + X_2 + X_3 + X_4$. For simplicity, you assume that these risks are independent. (Later, in Section \@ref(S:Sec166), we will consider the more complex case of dependence.)

To manage the risk, you seek some insurance protection. You wish to manage internally small building and motor vehicles amounts, up to $M_1$ and $M_2$, respectively. You seek insurance to cover all other risks. Specifically, the insurer's portion is
$$ Y_{insurer} = (X_1 - M_1)_+ + (X_2 - M_2)_+ + X_3 + X_4 ,$$
so that your retained risk is $Y_{retained}= X- Y_{insurer} =$ $\min(X_1,M_1) +  \min(X_2,M_2)$. Using deductibles $M_1=$ `r M1` and $M_2=$ `r M2`:

a. Determine the expected claim amount of (i) that retained, (ii) that accepted by the insurer, and (iii) the total overall amount.
b. Determine the 80th, 90th, 95th, and 99th percentiles for (i) that retained, (ii) that accepted by the insurer, and (iii) the total overall amount. 
c. Compare the distributions by plotting the densities for (i) that retained, (ii) that accepted by the insurer, and (iii) the total overall amount.

`r HideRCode('PortMgt', 'Show Example Solution with R Code')`

**Solution**.

In preparation, here is the code needed to set the parameters.

```{r   eval=FALSE, echo=HtmlEval}
# For the gamma distributions, use
alpha1 <- 2;      theta1 <- 100
alpha2 <- 2;      theta2 <- 200
# For the Pareto distributions, use
alpha3 <- 2;      theta3 <- 1000
alpha4 <- 3;      theta4 <- 2000
# Limits
M1     <- 100
M2     <- 200
```

With these parameters, we can now simulate realizations of the portfolio risks.

```{r  fig.width=8, fig.height=4,  echo=HtmlEval}
# Simulate the risks
nSim <- 10000  #number of simulations
set.seed(2017) #set seed to reproduce work 
X1 <- rgamma(nSim,alpha1,scale = theta1)  
X2 <- rgamma(nSim,alpha2,scale = theta2)  
# For the Pareto Distribution, use
library(actuar)
X3 <- rpareto(nSim,scale=theta3,shape=alpha3)
X4 <- rpareto(nSim,scale=theta4,shape=alpha4)
# Portfolio Risks
X         <- X1 + X2 + X3 + X4
Yretained <- pmin(X1,M1) + pmin(X2,M2)
Yinsurer  <- X - Yretained
```

**(a)** Here are the results for the expected claim amounts.

```{r  fig.width=8, fig.height=4,  echo=HtmlEval}
# Expected Claim Amounts
ExpVec <- t(as.matrix(c(mean(Yretained),mean(Yinsurer),mean(X))))
colnames(ExpVec) <- c("Retained", "Insurer","Total")
round(ExpVec,digits=2)
```

**(b)** Here are the results for the quantiles.

```{r  fig.width=8, fig.height=4,  echo=HtmlEval}
# Quantiles
quantMat <- rbind(
  quantile(Yretained, probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(Yinsurer,  probs=c(0.80, 0.90, 0.95, 0.99)),
  quantile(X       ,  probs=c(0.80, 0.90, 0.95, 0.99)))
rownames(quantMat) <- c("Retained", "Insurer","Total")
round(quantMat,digits=2)
```

**(c)** Here are the results for the density plots of the retained, insurer, and total portfolio risk.

```{r  fig.width=8, fig.height=4,  echo=HtmlEval}

par(mfrow=c(1,3))
plot(density(Yretained), xlim=c(0,500), main="Retained Portfolio Risk", xlab="Loss (Note the different horizontal scale)", ylab = "Density (Note different vertical scale)")
plot(density(Yinsurer), xlim=c(0,15000), main="Insurer Portfolio Risk", xlab="Loss")
plot(density(X), xlim=c(0,15000), main="Total Portfolio Risk", xlab="Loss")
```

</div>


## Exercises

#### Theoretical Exercise  {-}


<a id=Exer:13.1></a>  

[Exercise 13.1]: ./ChapPortMgt.html#Exer:13.1

**[Exercise 13.1]{#Exer:13.1}**. In this exercise, you will demonstrate that only under specific circumstances can the standard deviation principle \@ref(eq:SD-principle) be considered a coherent risk measure.

*  **a.**   Show that subadditivity, positive homogeneity, and translation invariance, hold for the standard deviation principle.
*  **b.**   Assume that $0 \leq \alpha \leq 1/\sqrt{3}$. Show that for these values of $\alpha$ that monotonicity holds for standard deviation principle. Thus, for these values of  $\alpha$, the standard deviation principle is coherent.
*  **c.**   For $\alpha > 1/\sqrt{3}$, show that monotonicity does not hold and so the standard deviation principle can not be considered coherent in general.


`r HideExample('PortMgt.13.1', 'Show Exercise 13.1 Solution')`

`r LObjBegin()`
$\textbf{Verification of the Special Case}$. To this end, for a given $\alpha>0$, we check the four axioms for $H_{\mathrm{SD}}(X+Y)$ one by one:

$\textbf{a1}$  $\textit{validation of subadditivity:}$
$$
\begin{array}{lll}
  H_{\mathrm{SD}}(X+Y) &=& \mathrm{E}[X+Y]+\alpha \mathrm{SD}(X+Y) \\
  &\leq& \mathrm{E}[X]+\mathrm{E}[Y]+\alpha [\mathrm{SD}(X) +\mathrm{SD}(Y)]\\
  &=& H_{\mathrm{SD}}(X)+ H_{\mathrm{SD}}(Y);
\end{array}
$$
$\textbf{a2}$  $\textit{validation of positive homogeneity:}$
$$
H_{\mathrm{SD}}(cX) = c ~\mathrm{E}[X] + c~\alpha~\mathrm{SD}(X)=c~H_{\mathrm{SD}}(X);
$$
$\textbf{a3}$  $\textit{validation of translation invariance:}$
$$
H_{\mathrm{SD}}(X+c)=\mathrm{E}[X]+c+\alpha~\mathrm{SD}(X)=H_{\mathrm{SD}}(X)+c.
$$


$\textbf{b/c}$  $\textit{validation of monotonicity}$


It only remains to verify the monotonicity property, which may or may not be satisfied depending on the value of $\alpha$.  To see this, consider again the setup of \@ref(eq:special-x) and \@ref(eq:special-y) in which $\Pr[X\leq Y]=1$.  Let $\alpha=0.1\cdot \sqrt{3}$, then $H_{\mathrm{SD}}(X)=3+0.3=3.3< H_{\mathrm{SD}}(Y)=4$ and the monotonicity condition is met.  On the other hand, let $\alpha=\sqrt{3}$, then $H_{\mathrm{SD}}(X)=3+3=6> H_{\mathrm{SD}}(Y)=4$ and the monotonicity condition is not satisfied. More precisely, by setting
$$
  H_{\mathrm{SD}}(X) = 3+\alpha\sqrt{3} \leq4= H_{\mathrm{SD}}(Y),
$$
we find that the monotonicity condition is only satisfied for $0\leq\alpha\leq 1/\sqrt{3}$, and thus the standard deviation principle $H_{\mathrm{SD}}$ is coherent.  

This result appears to be very intuitive since the standard deviation principle $H_{\mathrm{SD}}$ is a linear combination of two risk measures of which one is coherent and the other is incoherent.  If $\alpha\leq 1/\sqrt{3}$, then the coherent measure dominates the incoherent one, thus the resulting measure $H_{\mathrm{SD}}$ is coherent and vice versa. Note that the aforementioned conclusion may not be generalized to any pair of random variables $X$ and $Y$.
`r LObjEnd()`

*** 

</div>


#### Exercises with a Practical Focus {-}

**Exercise 13.2. Property Fund**. Consider commercial property claims from the Wisconsin Property Fund, introduced in Section \@ref(S:LGPIF). This exercise is based on 1,377 claims from 2010 for damages to state government properties and their building contents. You will use these data to estimate an empirical distribution function, without reference to a parametric model. 

*  **a.**  Use the empirical distribution function to estimate $VaR$ over several confidence levels. Produce a graph similar to the left-hand panel of Figure \@ref(fig:Exercise132).
*  **b.**  Use the empirical distribution function to estimate $ES$ over several confidence levels. Produce a graph similar to the middle panel of Figure \@ref(fig:Exercise132).
*  **c.**  Compare the two measures from parts (a) and (b) to produce a graph similar to the right-hand panel of Figure \@ref(fig:Exercise132). This comparison shows, for any given level of confidence, that the $ES$ measure far exceeds the $VaR$.

(ref:Exercise132) **Property Fund VaR and ES Plots.** The left-hand panel shows the value at risk $VaR$ for several confidence levels and the middle panel gives similar information for the expected shortfall ($ES$). The confidence level $\alpha = 0.80$ is marked with a blue dashed vertical line. Note that the vertical axes differ. This is emphasized by direct comparison in the right-hand panel where the 45 degree solid line falls below the empirical values.

```{r Exercise132,  echo=FALSE, fig.cap='(ref:Exercise132)',  fig.asp=0.5}

ClaimLev  <- read.csv("Data/CLAIMLEVEL.csv", header=TRUE)
ClaimData <-subset(ClaimLev,Year==2010);     #2010 subset

ClaimsBC  <- ClaimData$Claim
prob.seq = seq(0.50, 0.99, by=0.001)
VaR = quantile(ClaimsBC, prob.seq)
CTE <- VaR
for (i in (1:length(prob.seq)))
   {CTE[i] = sum(ClaimsBC*(ClaimsBC>VaR[i]))/sum(ClaimsBC>VaR[i])
}
par(mfrow=c(1, 3))
options(scipen=10)
plot(prob.seq, VaR, log = "y", xlab = 'Confidence Level')
abline(v = 0.8, col = "blue", lty = 2)
plot(prob.seq, CTE, log = "y", xlab = 'Confidence Level', ylab = 'ES')
abline(v = 0.8, col = "blue", lty = 2)
plot(VaR, CTE, log = "xy", ylab = 'ES')
abline(0,1)
```


`r HideRCode('PortMgt.13.2',"Solutions for Exercise 13.2")`

```{r echo=HtmlEval, ref.label = 'Exercise132', eval = FALSE}
```

***

</div>

**Exercise 13.3. Risk Measures with Stop-Loss**. Consider the stop-loss arrangement with retention level $M$ described in Section \@ref(S:Sec1342).

**a.** Show that the value at risk for the retained portion can be expressed as
$$
VaR_{\alpha}[X \wedge M]  =\left\{
\begin{array}{cl}
  F^{-1}_{\alpha}  & \text{if } \alpha < F(M) \\
 M                 & \text{if } \alpha \ge F(M)
\end{array} \right. ,
$$
where $F^{-1}_{\alpha}=VaR_{\alpha}(X)$ is a quantile for a random variable $X$. 

**b.** Show that the expected shortfall for the retained portion can be expressed as
$$
{\small
ES_{\alpha}[X \wedge M]=\left\{
\begin{array}{cl}
F_{\alpha}^{-1}+\frac{1}{1-\alpha} \left\{ \mathrm{E} (X \wedge M)- \mathrm{E} (X \wedge F_{\alpha}^{-1}) \right\} 
&  \text{if }  \alpha < F(M)  \\ 
M & \text{if } \alpha \ge F(M)\\
\end{array} \right. .
}
$$


```{r echo = FALSE}
ClaimsBC  <- ClaimData$Claim
alpha = 0.99
VaR.alpha = quantile(ClaimsBC, alpha)
```


**c.** Let us continue Exercise 13.2 where we examined empirical estimates of the distribution using 1,377 property damage claims. We now impose an upper limit $M$. A confidence level of $\alpha = 0.99$ is used for this illustration. Provide a plot of the value at risk for retained losses under the stop-loss arrangement versus the upper limit $M$. The plot should be comparable to the left-hand panel of Figure \@ref(fig:Exercise133) where a blue dashed vertical line marks the $\widehat{VaR}_{0.99}  =$ `r round(VaR.alpha,digits=0)`.

**d**. Provide a plot of the expected shortfall for retained losses under the stop-loss arrangement versus the upper limit $M$
The plot should be comparable to the right-hand panel of Figure \@ref(fig:Exercise133).

By displaying the figures side-by-side in Figure \@ref(fig:Exercise133), we learn that the $ES$ is smoother at this point when compared to the $VaR$.


(ref:Exercise133) **Property Fund VaR and ES Plots for Various Upper Limits.** The left-hand panel shows the retained risk $VaR$ over different upper limits and the right-hand panel gives similar information for the expected shortfall ($ES$). The blue dashed vertical line marks $\widehat{VaR}_{\alpha}$.

```{r Exercise133, echo=FALSE, fig.cap='(ref:Exercise133)', out.width='80%', fig.asp=0.6}
ClaimsBC  <- ClaimData$Claim
alpha = 0.99
VaR.alpha = quantile(ClaimsBC, alpha)

maxClaim <- max(ClaimsBC)
sortClaims <- sort(ClaimsBC, decreasing = TRUE)
maxClaimN <- sortClaims[1]
maxClaimN.1 <- sortClaims[2]
u.seq = seq(0.01*maxClaimN.1, 0.25*maxClaimN.1, length.out = 110)
VaR.u <- pmin(VaR.alpha, u.seq)

LEV <- mean(pmin(ClaimsBC,VaR.alpha))
CTE.u <- VaR.u
for (i in (1:length(u.seq)))
   {
  LEVi <- mean(pmin(ClaimsBC,u.seq[i]))
  CTE.u[i] <-  ((LEVi - LEV + (1-alpha)*VaR.alpha)/(1-alpha))*
                (u.seq[i] >= VaR.alpha) +
                 u.seq[i]*(u.seq[i] < VaR.alpha) 
}

par(mfrow=c(1, 2))
options(scipen=10)
plot(u.seq, VaR.u, xlab = 'Upper Limit', 
     ylab = "VaR", type = 'l', ylim = c(40000,600000))
abline(v = VaR.alpha, col = "blue", lty = 2)
plot(u.seq, CTE.u, xlab = 'Upper Limit', 
     ylab = "ES", type = 'l', ylim = c(40000,600000))
abline(v = VaR.alpha, col = "blue", lty = 2)

```


`r HideRCode('PortMgt.13.3',"Solutions for Exercise 13.3")`

`r SolnBegin()` 
$\textbf{a.}$ The distribution function for the limited random variable $X \wedge M$ is
$$
\Pr[X \wedge M \le z ]=F_{X \wedge M}(z) = \left\{
\begin{array}{cl}
F(z) &  \text{if }  z < M  \\ 
1 & \text{if } z \ge M\\
\end{array} \right. .
$$
(Draw a graph of this function.) From this, if $\alpha \ge F(M)$, then $F_{X \wedge M}^{-1}(\alpha) = M$. In the same way, if $\alpha < F(M)$, then $F_{X \wedge M}^{-1}(\alpha) = F_{\alpha}^{-1}$. This is sufficient for part (a). 

$\textbf{b.}$ From equation \@ref(eq:ESExpressions), the expected shortfall for retained risks can be expressed as
$$
\begin{array}{ll}
ES_{\alpha}[X \wedge M]
& = F_{X \wedge M}^{-1}(\alpha) + \frac{1}{1-\alpha} \left\{
\mathrm{E}[X \wedge M] - \mathrm{E} [X \wedge M \wedge F_{X \wedge M}^{-1}(\alpha)]
\right\} \\
&=\left\{
\begin{array}{cl}
F^{-1}_{\alpha} + \frac{1}{1-\alpha} \left\{
\mathrm{E}[X \wedge M] - \mathrm{E} [X \wedge M \wedge F^{-1}_{\alpha}]
\right\} 
&  \text{if }  \alpha < F(M)  \\ 
M + \frac{1}{1-\alpha} \left\{
\mathrm{E}[X \wedge M] - \mathrm{E} [X \wedge M \wedge M]
\right\}  & \text{if } \alpha \ge F(M)\\
\end{array} \right. .\\
&=\left\{
\begin{array}{cl}
F^{-1}_{\alpha} + \frac{1}{1-\alpha} \left\{
\mathrm{E}[X \wedge M] - \mathrm{E} [X  \wedge F^{-1}_{\alpha}]
\right\} 
&  \text{if }  \alpha < F(M)  \\ 
M   & \text{if } \alpha \ge F(M)\\
\end{array} \right. ,
 \end{array}
$$
as desired.

$\textbf{c/d.}$

`r SolnEnd()` 

```{r echo=HtmlEval, ref.label = 'Exercise133', eval = FALSE}
```

`r Refer()` 

***

</div>


```{r child = './Quizzes/Quiz13A4.html', eval = QUIZ}
```



## Further Resources and Contributors {#S:Sec136}

We refer the interested reader to @denuit2006actuarial and @hardy2006 for more comprehensive discussions of alternative risk measures for both discrete and continuous random variables. Note, however, that the definition in @denuit2006actuarial of "expected shortfall" differs from the one in this text. We use the definition of expected shortfall from @Wang2022. 

As summarized in @Wang2022, both $VaR$ and $ES$ have solid axiomatic foundations and “appear in the banking regulation frameworks of Basel III/IV, as well as in the insurance regulation frameworks of Solvency II and the Swiss Solvency Test.” In addition to the coherence properties introduced in Section \@ref(S:Sec1333), this paper introduces economic axioms to motivate the use of $ES$. Thus, their usefulness in determining adequate solvency for banks and insurers motivates our emphasis in Section \@ref(S:Sec133) of these measures. 

Concepts of pricing individual risks were introduced in Chapter \@ref(ChapPremiumFoundations). For a comprehensive treatment of pricing portfolios, we refer to @Mildenhall2022.

There are many superb treatments of reinsurance in the literature. An outstanding book-long introduction is @albrecher2017reinsurance.

Some of the examples from this chapter were borrowed from @clark1996basics, @klugman2012, and @bahnemann2015distributions. These resources provide excellent sources for additional discussions and examples.


-  **Edward (Jed) Frees**, University of Wisconsin-Madison, and **Jianxi Su**, Purdue University were the principal authors of the initial version of this chapter. 
   - Chapter reviewers include: Fei Huang, Hirokazu (Iwahiro) Iwasawa, Peng Shi, Ranee Thiagarajah, Ping Wang, and Chengguo Weng.
-  **Edward (Jed) Frees**, University of Wisconsin-Madison and Australian National University, is the author of the second edition of this chapter. Email: jfrees@bus.wisc.edu for chapter comments and suggested improvements.
   - Chapter reviewers include Chengguo Weng.



```{js echo=FALSE}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show";}
      else {ele.style.display = "block"; text.innerHTML = "Hide";}}
```
