
```{r include = FALSE}
numAnimation <- 10
options(digits = 9)
options(scipen = 9)
```

<!-- #  Chap 1 -->

<!-- #  Chap 2 -->

<!-- #  Chap 3 -->

<!-- #  Chap 4 -->

<!-- #  Chap 5 -->

<!-- #  Chap 6 -->

<!-- #  Chap 7 -->


# Simulation and Resampling {#ChapSimulation}

*Chapter Preview.* Simulation is a computationally intensive method used to solve difficult problems. Instead of creating physical processes and experimenting with them in order to understand their operational characteristics, a simulation study is based on a computer representation - it considers various hypothetical conditions as inputs and summarizes the results. Through simulation, a vast number of hypothetical conditions can be quickly and inexpensively examined. Section \@ref(S:Sec81) introduces simulation as a valuable computational tool, particularly effective in complex, multivariate settings.

Analysts find simulation especially useful for computing measures that summarize intricate distributions, as discussed in Section \@ref(S:Sec82). This encompasses all the examples mentioned in the book thus far, such as measures that summarize the frequency and severity of losses, along with many additional cases. Simulation can also be used to compute complex distributions necessary for hypothesis testing. In addition, we can also use simulation to draw from an empirical distribution - this process is known as *resampling*. Resampling allows us to assess the uncertainty of estimates in complex models. Section \@ref(S:Sec83) introduces resampling in the context of bootstrapping to determine the precision of estimators. Section \@ref(S:Sec84) on cross-validation shows how to use it for model selection and validation. 


## Random Number Generation {#S:Sec81}

***

In this section, you learn how to:

-   Generate approximately independent realizations that are uniformly distributed
-   Transform the uniformly distributed realizations to observations
    from a probability distribution of interest
-   Generate simulated values directly from common distributions using ready-made random number generators
-   Generate simulated values from complex distributions by combining simulated values from common distributions
-   Generate simulated values from distributions whose domain is restricted to specific regions of interest, such as with deductible and long-tailed actuarial applications.

***


### Generating Independent Uniform Observations {#S:Sec811}

The `r Gloss('simulations')` that we consider are generated by computers. A major strength of this approach is that they can be replicated, allowing us to check and improve our work. Naturally, this also means that they are not really random. Nonetheless, algorithms have been produced so that results appear to be random for all practical purposes. Specifically, they pass sophisticated tests of independence and can be designed so that they come from a single distribution - our `r Gloss('iid')` assumption, identically and independently distributed. 

To get a sense as to what these algorithms do, we consider a historically prominent method.

**Linear Congruential Generator.** To generate a sequence of random numbers, start with $B_0$, a starting value that is known as a *seed*. This value is updated using the recursive relationship
$$
B_{n+1} = (a B_n + c)  \text{ modulo }m, ~~ n=0, 1, 2, \ldots .
$$
This algorithm is called a `r Gloss('linear congruential generator')`. The case of $c=0$ is called a *multiplicative* congruential generator; it is
particularly useful for really fast computations.

For illustrative values of $a$ and $m$, Microsoft's Visual Basic uses $m=2^{24}$, $a=1,140,671,485$, and $c = 12,820,163$ (see
<https://en.wikipedia.org/wiki/Linear_congruential_generator>). This is the engine underlying the random number generation in Microsoft's Excel program.

The sequence used by the analyst is defined as $U_n=B_n/m.$ The analyst
may interpret the sequence {$U_{i}$} to be (approximately) identically
and independently uniformly distributed on the interval (0,1). To
illustrate the algorithm, consider the following.

**Example 8.1.1. Illustrative Sequence.**
Take $m=15$, $a=3$, $c=2$ and $B_0=1$. Then we have:

   step $n$  $B_n$                                      $U_n$
  ---------- --------------------------------- ------------------------
      0      $B_0=1$                           
      1      $B_1 =\mod(3 \times 1 +2) = 5$      $U_1 = \frac{5}{15}$
      2      $B_2 =\mod(3 \times 5 +2) = 2$      $U_2 = \frac{2}{15}$
      3      $B_3 =\mod(3 \times 2 +2) = 8$     $U_3 = \frac{8}{15}$
      4      $B_4 =\mod(3 \times 8 +2) = 11$    $U_4 = \frac{11}{15}$
  ---------- --------------------------------- ------------------------



The linear congruential generator is just one method of producing pseudo-random outcomes. It is easy to understand and is widely used. The linear congruential generator does have limitations, including the fact that it is possible to detect long-run patterns over time in the sequences generated (recall that we can interpret *independence* to mean a total lack of functional patterns). Not surprisingly, advanced techniques have been developed that address some of this method's drawbacks. The random number generated by `R` utilizes such advanced techniques.

Sometimes computer generated random results are known as `r Gloss("pseudo-random numbers")` to reflect the fact that they are machine generated and can be replicated. That is, despite the fact that {$U_{i}$} appears to be
i.i.d, it can be reproduced by using the same seed number (and the same
algorithm). 

<a id=Ex:8.1.2></a>  

[Example 8.1.2]: ./ChapSimulation.html#Ex:8.1.2

**[Example 8.1.2]{#Ex:8.1.2}. Generating Uniform Random Numbers in `R`.**
The following code shows how to generate three uniform (0,1) numbers in `R` using the `runif` command. The `set.seed()` function sets the initial seed. In many computer packages, the initial seed is set using the system clock unless specified otherwise.


##### Three Uniform Random Variates {-}

`r CodeFontSmall()`

```{r }
set.seed(2017)
U <- runif(3)
knitr::kable(U, digits=5, align = "c", col.names = "Uniform")  %>%
     kableExtra::kable_classic(full_width = F) %>%
     kable_styling(latex_options = "hold_position", font_size = 10) 

```

`r CodeFontLarge()`

### Inverse Transform Method {#S:Sec812}

With the sequence of uniform random numbers, we next transform them to a distribution of interest, say $F$. A prominent technique is the `r Gloss('inverse transform method')`, defined as
$$
X_i=F^{-1}\left( U_i \right) .
$$
Here, recall from Section 4.1.1 that we introduced the inverse of the distribution function, $F^{-1}$, and referred to it also as the `r Gloss('quantile function')`. Specifically, it is defined to be 
$$
F^{-1}(y) = \inf_x ~ \{ F(x) \ge y \} .
$$
Recall that $\inf$ stands for *infimum* or the `r Gloss('greatest lower bound')`. It is essentially the smallest value of *x* that satisfies the inequality $\{F(x) \ge y\}$. The result is that the sequence {$X_{i}$} is *iid* with distribution function $F$ if the {$U_i$} are *iid* with uniform on $(0,1)$ distribution function.

The inverse transform result is available when the underlying random variable is continuous, discrete or a hybrid combination of the two. We now present a series of examples to illustrate its scope of applications.

<a id=Ex:8.1.3></a>  

[Example 8.1.3]: ./ChapSimulation.html#Ex:8.1.3

**[Example 8.1.3]{#Ex:8.1.3}. Generating Exponential Random Numbers.**
Suppose that we would like to generate observations from an exponential distribution with scale
parameter $\theta$ so that $F(x) = 1 - e^{-x/\theta}$. To compute the
inverse transform, we can use the following steps: 
$$
\begin{aligned}
 y = F(x) &\Leftrightarrow  y = 1-e^{-x/\theta} \\
  &\Leftrightarrow -\theta \ln(1-y) = x = F^{-1}(y) .
\end{aligned}
$$
Thus, if $U$ has a uniform (0,1) distribution, then $X = -\theta \ln(1-U)$ has an exponential distribution with parameter $\theta$.

The following `R` code shows how we can start with the same three uniform random numbers as in [Example 8.1.2](#Ex:8.1.2) and transform them to independent exponentially distributed random variables with a mean of 10. Alternatively, you can directly use the `rexp` function in `R` to generate random numbers from the exponential distribution. The algorithm built into this routine is different so even with the same starting seed number, individual realizations will differ.

`r CodeFontSmall()`

```{r }
set.seed(2017)
U  <- runif(3)
X1 <- -10*log(1-U)
set.seed(2017)
X2 <- rexp(3, rate = 1/10)
```

`r CodeFontLarge()`

##### Three Uniform Random Variates {-}

```{r echo = FALSE}
outmat           <- cbind(U,X1,X2)
colnames(outmat) <- c("Uniform","Exponential 1", "Exponential 2")
knitr::kable(outmat, digits=5, align = "c")  %>%
     kableExtra::kable_classic(full_width = F) %>%
     kable_styling(latex_options = "hold_position", font_size = 10)
```


***

**Example 8.1.4. Generating Pareto Random Numbers.**
Suppose that we would like to generate observations from a Pareto distribution with parameters $\alpha$ and
$\theta$ so that $F(x) = 1 - \left(\frac{\theta}{x+\theta} \right)^{\alpha}$. To compute
the inverse transform, we can use the following steps: 
$$
\begin{aligned}
 y = F(x) &\Leftrightarrow 1-y = \left(\frac{\theta}{x+\theta} \right)^{\alpha} \\
  &\Leftrightarrow \left(1-y\right)^{-1/\alpha} = \frac{x+\theta}{\theta} = \frac{x}{\theta} +1 \\
    &\Leftrightarrow \theta \left((1-y)^{-1/\alpha} - 1\right) = x = F^{-1}(y) .\end{aligned}
$$
Thus, $X = \theta \left((1-U)^{-1/\alpha} - 1\right)$ has a Pareto distribution with parameters $\alpha$ and $\theta$.

***


**Inverse Transform Justification.** Why does the random variable $X = F^{-1}(U)$ have a distribution function $F$?

`r HideExample('Snippet.Hide','Show A Snippet of Theory')`

`r LObjBegin()`

This is easy to establish when $F$ is strictly increasing, where the distribution is continuous. Because $U$ is a uniform random variable on (0,1), we know that $\Pr(U \le y) = y$, for $0 \le y \le 1$. Thus, 
$$
\begin{aligned}
\Pr[X \le x] &= \Pr[F^{-1}(U) \le x] \\
 &= \Pr[F(F^{-1}(U)) \le F(x)] \\
&= \Pr[U \le F(x)] = F(x),
\end{aligned}
$$
as required. The key step is that $F[F^{-1}(u)] = u$ for each $u$, which is true when $F$ is strictly increasing.

`r LObjEnd()`

</div>

We now consider some discrete examples.

**Example 8.1.5. Generating Bernoulli Random Numbers.**
Suppose that we wish to simulate random variables from a Bernoulli distribution with parameter $q=0.85$.

(ref:Fig81) **Distribution Function of a Binary Random Variable**

```{r Fig81, fig.cap='(ref:Fig81)', out.width='50%', fig.asp=.75, echo=FALSE}
time  <- seq(-1,2,0.01)
Ftime <- c(rep(0,100), rep(.85,100), rep(1,101) )

plot(time,Ftime, ylim=c(0,1), xlab="x",ylab="",pch=19, cex=.2)#,type="l")
mtext("F(x)", side=2, at=1.1, las=1, cex=1.2, adj=1.6)
segments(0,0,0,0.85)#,code=4)
segments(1,0.85,1,1)#,code=4)
symbols(0,.85,circles=.03, add=TRUE,bg="black",inches=FALSE)
symbols(0,0,  circles=.03, add=TRUE,inches=FALSE)
symbols(1,.85,circles=.03, add=TRUE,inches=FALSE)
symbols(1,1,  circles=.03, add=TRUE,bg="black",inches=FALSE)
```


A graph of the cumulative distribution function in Figure \@ref(fig:Fig81) shows that the quantile function can be written as
$$
\begin{aligned}
F^{-1}(y) = \left\{ \begin{array}{cc}
              0 & 0<y \leq 0.85 \\
              1 & 0.85 < y  \leq  1.0 .
            \end{array} \right.
\end{aligned}
$$
Thus, with the inverse transform we may define 
$$
\begin{aligned}
X = \left\{ \begin{array}{cc}
              0 & 0<U \leq 0.85  \\
              1 &  0.85 < U  \leq  1.0
            \end{array} \right.
\end{aligned}
$$
For illustration, we generate three random numbers to get

`r CodeFontSmall()`

```{r }
set.seed(2017)
U <- runif(3)
X <- 1*(U > 0.85)
```

`r CodeFontLarge()`

##### Three Random Variates {-}

```{r echo = FALSE}
outmat           <- cbind(U,X)
colnames(outmat) <- c("Uniform","Binary X")
knitr::kable(outmat, digits=5, align = "c")  %>%
     kableExtra::kable_classic(full_width = F) %>%
     kable_styling(latex_options = "hold_position", font_size = 10)
```

**Example 8.1.6. Generating Random Numbers from a Discrete Distribution.**
Consider the time of a machine failure in the first five years. The distribution of failure times is
given as:

##### Discrete Distribution {-}

```{r echo = FALSE}
time   <- 1:5
probs  <- c(0.1,0.2,0.1, 0.4, 0.2)
df     <- cumsum(probs)
outmat <- rbind(time, probs, df)
colnames(outmat) <- NULL
rownames(outmat) <- c("Time","Probability","Distribution Function")
knitr::kable(outmat, digits=5, align = "c")  %>%
     kableExtra::kable_classic(full_width = F) %>%
     kable_styling(latex_options = "hold_position", font_size = 10)

```



(ref:Fig82) **Distribution Function of a Discrete Random Variable**

```{r Fig82, fig.cap='(ref:Fig82)', out.width='60%', fig.asp=.75,  echo=FALSE}
time  <- seq(0,6,0.01)
Ftime <- c(rep(0,100),  rep(0.1,100),rep(0.3,100),rep(0.4,100),
           rep(0.8,100),rep(1,101) )
plot(time,Ftime, ylim=c(0,1), xlab="x",ylab="",pch=19, cex=.2)#,type="l")
mtext("F(x)", side=2, at=1.1, las=1, cex=1.2, adj=1.6)
segments(1,0,1,0.1)#,code=4)
segments(2,0.1,2,.3)#,code=4)
segments(3,0.3,3,.4)#,code=4)
segments(4,0.4,4,.8)#,code=4)
segments(5,0.8,5, 1)#,code=4)

symbols(1, 0,circles=.05, add=TRUE,inches=FALSE)
symbols(1,.1,circles=.05, add=TRUE,bg="black",inches=FALSE)
symbols(2,.1,circles=.05, add=TRUE,inches=FALSE)
symbols(2,.3,circles=.05, add=TRUE,bg="black",inches=FALSE)
symbols(3,.3,circles=.05, add=TRUE,inches=FALSE)
symbols(3,.4,circles=.05, add=TRUE,bg="black",inches=FALSE)
symbols(4,.4,circles=.05, add=TRUE,inches=FALSE)
symbols(4,.8,circles=.05, add=TRUE,bg="black",inches=FALSE)
symbols(5,.8,circles=.05, add=TRUE,inches=FALSE)
symbols(5, 1,circles=.05, add=TRUE,bg="black",inches=FALSE)
```

Using the graph of the distribution function in Figure \@ref(fig:Fig82), with the inverse transform we may define
$$
\small{
\begin{aligned}
X = \left\{ \begin{array}{cc}
              1 &   0<U  \leq 0.1  \\
              2 &  0.1 < U  \leq  0.3\\
              3 &  0.3 < U  \leq  0.4\\
              4 &  0.4 < U  \leq  0.8  \\
              5 &  0.8 < U  \leq  1.0     .
            \end{array} \right.
\end{aligned}
}
$$

***

For general discrete random variables there may not be an ordering of
outcomes. For example, a person could own one of five types of life
insurance products and we might use the following algorithm to generate
random outcomes: 
$$
{\small
\begin{aligned}
X = \left\{ \begin{array}{cc}
  \textrm{whole life} &   0<U  \leq 0.1  \\
 \textrm{endowment} &  0.1 < U  \leq  0.3\\
\textrm{term life} &  0.3 < U  \leq  0.4\\
  \textrm{universal life} &  0.4 < U  \leq  0.8  \\
  \textrm{variable life} &  0.8 < U  \leq  1.0 .
            \end{array} \right.
\end{aligned}
}
$$ 
Another analyst may use an alternative procedure such as: 
$$
{\small
\begin{aligned}
X = \left\{ \begin{array}{cc}
  \textrm{whole life} &   0.9<U<1.0  \\
 \textrm{endowment} &  0.7 \leq U < 0.9\\
\textrm{term life} &  0.6 \leq U < 0.7\\
  \textrm{universal life} &  0.2 \leq U < 0.6  \\
  \textrm{variable life} &  0 \leq U < 0.2 .
            \end{array} \right.
\end{aligned}
}
$$
Both algorithms produce (in the long-run) the same probabilities, e.g., $\Pr(\textrm{whole life})=0.1$, and so forth. So, neither is incorrect. You should be aware that there is more than one way to accomplish a goal. Similarly, you could use an alternative algorithm for ordered outcomes (such as failure times 1, 2, 3, 4, or 5, above).



### Ready-made Random Number Generators {#S:Sec813}

Sections \@ref(S:Sec811) and \@ref(S:Sec812) showed how one can generate simulated values from the foundations. This approach is important so that analyst can appreciate why the simulation works so well. However, because simulation is used so widely, it is not surprising that packages have been developed as time-saving devices.

For example, we have already seen in [Example 8.1.3](#Ex:8.1.3) that one can generate exponentially distributed random variates through the `rexp` function. This function means that analyst need not generate uniform random variates and then transform them using the inverse exponential distribution function. Instead, this is done in a single step using the `rexp` function.

[Table 8.2](#tab:8.2) summarizes a few of the standard random number generators in `R`; the **r** at the beginning of each function refers to a **r**andom number generator. Additional documentation for these functions are in Appendix Chapter \@ref(ChapSummaryDistributions). Note that the Pareto distribution requires the package `actuar`.


<a id=tab:8.2></a>  

[Table 8.2]: ./ChapSimulation.html#tab:8.2

[Table 8.2]{#tab:8.2}. **Random Number Generators (RNGs)**

$$
{\small
\begin{array}{ll|ll}
\hline
\textbf{Discrete Distributions} &
&\textbf{Continuous  Distributions} & \\
\textit{Distribution}& \textit{RNG Function}&
\textit{Distribution}& \textit{RNG Function}\\
\hline
\text{Binomial}& \text{rbinom}  &\text{Exponential}& \text{rexp}\\
\text{Poisson} & \text{rpoisson}&\text{Gamma}      & \text{rgamma}\\
\text{Negative Binomial}& \text{rnbinom} & \text{Pareto}& \text{actuar::rpareto}\\
& &\text{Normal}& \text{rnorm}\\
& &\text{Weibull}& \text{rweibull}\\
\hline
\end{array}
}
$$

### Simulating from Complex Distributions {#S:Sec814}

In statistical software programs such as `R`, analysts will find several ready-made random number generators. However, for many complex actuarial applications, it is likely that ready-made generators will not be available and so one must return to the foundations.

To illustrate, consider the aggregate claims distributions introduced in Chapter \@ref(ChapAggLossModels). There, in Section \@ref(S:Sec742), we have already seen how to simulate aggregate loss distributions. As we saw in [Example 7.4.2], the process is to first simulate the number of losses and then simulate individual losses.

As another example of a complex distribution, consider the following example.


**Example 8.1.7. Generating Random Numbers from a Hybrid Distribution.**
Consider a random variable that is 0 with probability 70% and is exponentially distributed with parameter $\theta= 10,000$ with probability 30%. In an insurance application, this might correspond to a 70% chance of having no insurance claims and a 30% chance of a claim - if a claim occurs, then it is exponentially distributed. The distribution function, depicted in Figure \@ref(fig:Fig83), is given as
$$
\begin{aligned}
F(y) = \left\{ \begin{array}{cc}
              0                      &  x<0  \\
              1 - 0.3 \exp(-x/10000) & x \ge 0 .
            \end{array} \right.
\end{aligned}
$$
      
(ref:Fig83) **Distribution Function of a Hybrid Random Variable**            

```{r Fig83, fig.cap='(ref:Fig83)', out.width='60%', fig.asp=.75,  echo=FALSE}
time  <- seq(-1000,40000,10)
Ftime <- 1 - .3*exp(-0.0001*time)
Ftime <- Ftime*(time>0)
plot(time,Ftime, ylim=c(0,1), xlab="x",ylab="",pch=19, cex=.2)#,xaxt="n")
#axis(1, at=seq(0,40000, by=10000), font=10, cex=0.005, tck=0.01)
mtext("F(x)", side=2, at=1.1, las=1, cex=1.2, adj=1.6)
segments(0,0,0,0.7)#,code=4)
symbols(0, 0,circles=500, add=TRUE,inches=FALSE)
symbols(0,.7,circles=500, add=TRUE,bg="black",inches=FALSE)
```


From Figure \@ref(fig:Fig83), we can see that the inverse transform for generating random variables with this distribution function is
$$
\begin{aligned}
X = F^{-1}(U) = \left\{ \begin{array}{cc}
              0 &  0< U  \leq  0.7  \\
              -1000 \ln (\frac{1-U}{0.3}) & 0.7 < U < 1 .
            \end{array} \right.
\end{aligned}
$$
For discrete and hybrid random variables, the key is to draw a graph of the distribution function that allows you to visualize potential values of the inverse function.

***

You can think of this hybrid distribution as a special case of a mixture model that was introduced in Sections \@ref(S:Sec36) and \@ref(S:Sec432). Mixture models are straight-forward to evaluate using simulation. In the first stage, one simulates a variable indicating the subpopulation. In the second stage, one simulates from that subpopulation. The resulting variate is a realization from the mixture model. To illustrate, let's revisit [Example 4.3.5].

**Example 4.3.5. Continued.** In this problem, we can label draws from the Type 1 subpopulation as $X_1$ from an exponential distribution with mean 200, and those from the Type 2 subpopulation as $X_2$ from a Pareto distribution with parameters $\alpha=3$ and $\theta=200$. Here, 25% of policies are Type 1 and 75% of policies are Type 2.

We can use simulation to find the probability that the annual loss will be less than 100, and find the average loss. The illustrative code uses the ready-made random number generator functions `rbinom`, `rexp`, and `actuar::pareto`.


`r HideRCode('Example4.3.5.Continued','Show R Code for Example 4.3.5 Continued')`

`r CodeFontSmall()`

```{r}
nsim <- 100000
Z    <- rbinom(nsim, prob=0.75, size=1)
X1   <- rexp(nsim, rate=1/200)
X2   <- actuar::rpareto(nsim, shape = 3, scale = 200)
X    <- (1-Z)*X1 + Z*X2
#sum(X<100)/nsim
#mean(X)

```

`r CodeFontLarge()`

</div>


### Importance Sampling {#S:Sec815}

Another class of important problems utilize distributions that are from a limited region. For example, when a loss has a deductible, the resulting claim represents the payment by an insurer that is not observed for amounts less than the deductible. This type of problem was considered extensively in Chapter \@ref(ChapClaimSeverity). As another example, for claims that are extremely large, one may wish to restrict an analysis to only extremely large outcomes - discussions of *tails* of distributions will be taken up in Section \@ref(S:Sec132). To address both types of problems, we now suppose that we wish to draw according to $X$, conditional on $X\in[a,b]$.

To this end, one can use an `r Gloss('accept-reject mechanism')` : draw $x$ from distribution $F$

- if $x\in[a,b]$ : keep it ("*accept*")
- if $x\notin[a,b]$ : draw another one ("*reject*")

Observe that from $n$ values initially generated, we keep here only $[F(b)-F(a)]\cdot n$ draws, on average.



**Example 8.1.8. Draws from a Normal Distribution.** Suppose that we draw from a normal distribution with mean 2.5 and variance 1, $N(2.5,1)$, but are only interested in draws greater that $a = 2$ and less than $b = 4$. That is, we can only use $F(4)-F(2) = \Phi(4-2.5)-\Phi(2-2.5)$ = `r round(pnorm(4,2.5,1),digits=4)` - `r round(pnorm(2,2.5,1),digits=4)` = `r round(pnorm(4,2.5,1)-pnorm(2,2.5,1),digits=4)` proportion of the draws. Figure \@ref(fig:Fig84) demonstrates that some draws lie with the interval $(2,4)$ and some are outside.

`r HideRCode('ImportanceSampling.1','Show R Code for Accept-Reject Mechanism')`

```{r echo=HtmlEval}
mu    <- 2.5
sigma <- 1
a     <- 2
b     <- 4
Fa    <- pnorm(a,mu,sigma)
Fb    <- pnorm(b,mu,sigma)
pic_ani <- function(){
  u <- seq(0,5,by=.01)
  plot(u,pnorm(u,mu,sigma),col="white",ylab="",xlab="")
  rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA)
  rect(a,Fa,b,Fb,col="white",border=NA)
  lines(u,pnorm(u,mu,sigma),lwd=2)
  abline(v=c(a,b),lty=2,col="red")
  ru  <- runif(1)
  clr <- "red"
  if ( (qnorm(ru,mu,sigma)>=a) & (qnorm(ru,mu,sigma)<=b) ) {clr <- "blue"}
  segments(-1,ru,qnorm(ru,mu,sigma),ru,col=clr,lwd=2)
  arrows(qnorm(ru,mu,sigma), ru, qnorm(ru,mu,sigma), 0,
         col=clr, lwd=2, length = .1)
  }

```

</div>

(ref:Fig84) **Demonstration of Draws In and Outside of (2,4)**

```{r Fig84, animation.hook=ANIMATIONHOOK, fig.cap='(ref:Fig84)',   out.width='50%', fig.asp=.75, echo=FALSE}
if (PdfEval){ numAnimation = 1 }
for (i in 1:numAnimation) {pic_ani()}
```


***

Instead, one can draw according to the conditional distribution $F^{\star}$ defined as
$$
F^{\star}(x) = \Pr(X \le x | a < X \le b) =\frac{F(x)-F(a)}{F(b)-F(a)}, \ \  \ \text{for } a < x \le b .
$$
Using the inverse transform method in Section \@ref(S:Sec812), we have that the draw
$$
X^\star=F^{\star-1}\left( U \right) = F^{-1}\left(F(a)+U\cdot[F(b)-F(a)]\right)
$$
has distribution $F^{\star}$. Expressed another way, define
$$
\tilde{U} = (1-U)\cdot F(a)+U\cdot F(b)
$$
and then use $F^{-1}(\tilde{U})$. With this approach, each draw counts.

This can be related to the `r Gloss('importance sampling mechanism')` : we draw more frequently in regions where we expect to have quantities that have some interest. This transform can be considered as a "a change of measure."

`r HideRCode('ImportanceSampling.2','Show R Code for Importance Sampling by the Inverse Transform Method')`

```{r echo=HtmlEval}
pic_ani <- function(){
  u <- seq(0,5,by=.01)
  plot(u,pnorm(u,mu,sigma),col="white",ylab="",xlab="")
  rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA)
  rect(a,Fa,b,Fb,col="white",border=NA)
  lines(u,pnorm(u,mu,sigma),lwd=2)
  abline(h=pnorm(c(a,b),mu,sigma) ,lty=2, col="red")
  ru      <- runif(1)
  rutilde <- (1-ru)*Fa+ru*Fb
  segments(-1,rutilde,qnorm(rutilde,mu,sigma),rutilde,col="blue",lwd=2)
  arrows(qnorm(rutilde,mu,sigma), rutilde, qnorm(rutilde,mu,sigma) ,0,
         col="blue", lwd=2, length = .1 )
  }

```

</div>

```{r  out.width='50%', fig.asp=.75, echo=FALSE, animation.hook=ANIMATIONHOOK}
for (i in 1:numAnimation) {pic_ani()}
```

In Example 8.1.8, the inverse of the normal distribution is readily available (in `R`, the function is `qnorm`). However, for other applications, this is not always the case. Then, one simply uses numerical methods to determine $X^\star$ as the solution of the equation $F(X^\star) =\tilde{U}$ where $\tilde{U}=(1-U)\cdot F(a)+U\cdot F(b)$. 
`r HideRCode('ImportanceSampling.3','Show R Code for Importance Sampling via Numerical Solutions')`

```{r  echo=HtmlEval}
pic_ani <- function(){
  u <- seq(0,5,by=.01)
  plot(u,pnorm(u,mu,sigma),col="white",ylab="",xlab="")
  rect(-1,-1,6,2,col=rgb(1,0,0,.2),border=NA)
  rect(2,-1,4,2,col="white",border=NA)
  lines(u,pnorm(u,mu,sigma),lty=2)
  pnormstar <- Vectorize(function(x){
    y <- (pnorm(x,mu,sigma)-Fa)/(Fb-Fa)
    if(x<=a) {y <- 0}
    if(x>=b) {y <- 1}
    return(y)
    })
  qnormstar <- function(u){as.numeric(uniroot((function (x) pnormstar(x) - u), 
                                              lower = 2, upper = 4)[1]) }
  lines(u,pnormstar(u),lwd=2)
  abline(v=c(2,4),lty=2,col="red")
  ru <- runif(1)
  segments(-1,ru,qnormstar(ru),ru,col="blue",lwd=2)
  arrows(qnormstar(ru), ru, qnormstar(ru), 0,
         col="blue", lwd=2, length = .1 )
}

```

</div>

```{r  out.width='50%', fig.asp=.75, echo=FALSE, animation.hook=ANIMATIONHOOK}
for (i in 1:numAnimation) {pic_ani()}

```


## Computing Distribution Parameters {#S:Sec82}

***

In this section, you learn how to:

-   Calculate quantities of interest and determine the precision of the calculated quantities
-   Determine the appropriate number of replications for a simulation study
-   Calculate complex distributions needed for hypothesis testing.

***

### Simulating Parameters {#S:Sec821}

One use of the term **parameter** is as a quantity that serves as an index for a known parametric family. For example, one usually thinks of a mean $\mu$ and standard deviation $\sigma$ as parameters of a normal distribution. Statisticians also use the term *parameter* to mean any quantity that summarizes a distribution. In this sense, a parameter can be written as $\theta(F)$, that is, if one knows the distribution function $F(\cdot)$, then one can compute the summary measure $\theta$.

In the previous subsection, we learned how to generate independent simulated realizations from a distribution of interest. With these realizations, we can construct an empirical distribution and approximate the underlying distribution as precisely as needed. As we introduce more actuarial applications in this book, you will see that simulation can be applied in a wide variety of contexts.

Many of these applications can be reduced to the problem of approximating a parameter $\mathrm{E~}[h(X)]$, where $h(\cdot)$ is some known function.  Based on $R$ simulations (replications), we get $X_1,\ldots,X_R$. From this simulated sample, we calculate an average 
$$
\overline{h}_R=\frac{1}{R}\sum_{i=1}^{R} h(X_i)
$$
that we use as our simulated approximate (estimate) of $\mathrm{E~}[h(X)]$. To estimate the precision of this approximation, we use the simulation variance
$$
s_{h,R}^2 = \frac{1}{R-1} \sum_{i=1}^{R}\left( h(X_i) -\overline{h}_R
\right) ^2.
$$
From the independence, the standard error of the estimate is $s_{h,R}/\sqrt{R}$. This can be made as small as we like by increasing the number of replications $R$.



```{r  Example821SetUp, echo=FALSE}
# For the gamma distributions, use
alpha1 <- 2;      theta1 <- 100
alpha2 <- 2;      theta2 <- 200
# Deductibles
M <- 400
nSim <- 1e6  #number of simulations

# For the gamma distributions, use
alpha1 <- 2;      theta1 <- 100
alpha2 <- 2;      theta2 <- 200
# Deductibles
M <- 400
# Simulate the risks
nSim <- 1e6  #number of simulations
set.seed(2017) #set seed to reproduce work 
X1 <- rgamma(nSim,alpha1,scale = theta1)  
X2 <- rgamma(nSim,alpha2,scale = theta2)  

# Portfolio Risks
X         <- X1 + X2 
Yretained <- pmin(X, M)
Yinsurer  <- X - Yretained
```

**Example 8.2.1. Portfolio Management.** In Section \@ref(S:Sec51), we learned how to calculate the expected value of policies with deductibles. For an example of something that cannot be done with closed form expressions, we now consider two risks. This is a variation of a more complex example that will be covered as [Example 13.4.6](#Ex13.4.6).

We consider two property risks of a telecommunications firm:

- $X_1$ - buildings, modeled using a gamma distribution with mean `r alpha1*theta1` and scale parameter `r theta1`.
- $X_2$ - motor vehicles, modeled using a gamma distribution with mean `r alpha2*theta2` and scale parameter `r theta2`.

Denote the total risk as $X = X_1 + X_2.$ For simplicity, you assume that these risks are independent. 

To manage the risk, you seek some insurance protection. You are willing to retain internally small building and motor vehicles amounts, up to $M$, say. Random amounts in excess of $M$ will have an unpredictable affect on your budget and so for these amounts you seek insurance protection. Stated mathematically, your retained risk is $Y_{retained}=$ $\min(X_1 + X_2,M)$ and the insurer's portion is $Y_{insurer} =  X- Y_{retained}$.

To be specific, we use $M=$ `r M` as well as $R=$ `r nSim` simulations. 

**a.** With these settings, we wish to determine the expected claim amount and the associated standard deviation of (i) that retained by you, (ii) that accepted by the insurer, and (iii) the total overall amount.
**b.** For insured claims, the standard error of the simulation approximation is $s_{h,R}/\sqrt{1000000} =$ `r 
round(sd(Yinsurer),digits=2)` $/\sqrt{1000000} =$  `r round(sd(Yinsurer)/sqrt(1000000), digits=3)`. For this example, simulation is quick and so a large value such as 1000000 is an easy choice. However, for more complex problems, the simulation size may be an issue. 


```{r  Example821ExpAmounts, echo=FALSE}
# a. Expected Claim Amounts
ExpVec <- t(as.matrix(c(mean(Yretained),mean(Yinsurer),mean(X))))
sdVec  <- t(as.matrix(c(sd(Yretained),sd(Yinsurer),sd(X))))
outMat <- rbind(ExpVec, sdVec)
colnames(outMat)  <- c("Retained", "Insurer","Total")
row.names(outMat) <- c("Mean","Standard Deviation")
#round(outMat,digits=2)
```



```{r  Example821FigSetUp, echo=FALSE}
# b. Figure Set Up
Yinsurefct <- function(numSim){
  X1       <- rgamma(numSim, alpha1, scale = theta1)  
  X2       <- rgamma(numSim, alpha2, scale = theta2)  
  # Portfolio Risks
  X        <- X1 + X2 
  Yinsurer <- X - pmin(X, M)
  return(Yinsurer)
  }
R          <- 1e3
nPath      <- 20
set.seed(2017)
simU       <- matrix(Yinsurefct(R*nPath), R, nPath)
sumP2      <- apply(simU, 2, cumsum)/(1:R)
```


**Example Solution**. For part (a), the results of these calculations are:

`r CodeFontSmall()`

```{r  echo=FALSE}
round(outMat,digits=2)
```

`r CodeFontLarge()`

For part (b), Figure \@ref(fig:Fig85) allows us to visualize the development of the approximation as the number of simulations increases.

`r Refer()`

***

(ref:Fig85) **Estimated Expected Insurer Claims versus Number of Simulations**

```{r Fig85,  fig.width=8, fig.height=4,   fig.cap = '(ref:Fig85)', echo=FALSE}
matplot(1:R,sumP2[,1:20], type="l", col=rgb(1,0,0,.2), ylim=c(100, 400),
        xlab=expression( paste("Number of Simulations (", italic('R'), ")") ), 
        ylab="Expected Insurer Claims" )
abline(h=mean(Yinsurer), lty=2)
bonds <- cbind( 1.96*sd(Yinsurer)*sqrt(1/(1:R)), -1.96*sd(Yinsurer)*sqrt(1/(1:R)) )
matlines(1:R, bonds+mean(Yinsurer), col="red", lty=1)
```




`r HideRCode('Example821.Hide', 'Show R Code for Example 8.2.1')`


```{r ref.label = 'Example821SetUp', eval = FALSE,  echo=HtmlEval}
```

```{r ref.label = 'Example821ExpAmounts', eval = FALSE,  echo=HtmlEval}
```

```{r ref.label = 'Example821FigSetUp', eval = FALSE,  echo=HtmlEval}
```

```{r ref.label = 'Fig85', eval = FALSE,  echo=HtmlEval}
```

</div>


### Determining the Number of Simulations {#S:Sec822}

How many simulated values are recommended? 100? 1,000,000? We can use the `r Gloss('central limit theorem', '6.1')` to respond to this question. 

As one criterion for your confidence in the result, suppose that you wish to be within 1% of the mean with 95% certainty. That is, you want $\Pr \left( |\overline{h}_R - \mathrm{E~}[h(X)]| \le 0.01 \mathrm{E~}[h(X)] \right) \ge 0.95$. According to the central limit theorem, your estimate should be approximately normally distributed and so we want to have $R$ large enough to satisfy $0.01 \mathrm{E~}[h(X)]/\sqrt{\mathrm{Var~}[h(X)]/R}) \ge 1.96$. (Recall that 1.96 is the 97.5th percentile from the standard normal distribution.) Replacing $\mathrm{E~}[h(X)]$ and $\mathrm{Var~}[h(X)]$ with estimates, you continue your simulation until 
$$
\frac{0.01~\overline{h}_R}{s_{h,R}/\sqrt{R}}\geq 1.96
$$
or equivalently 
\begin{equation}
R \geq 38,416\frac{s_{h,R}^2}{\overline{h}_R^2}.
(\#eq:NumSimulations)
\end{equation}
This criterion is a direct application of the approximate normality. Note that $\overline{h}_R$ and $s_{h,R}$ are not known in advance, so you will have to come up with estimates, either by doing a small pilot study in advance or by interrupting your procedure intermittently to see if the criterion is satisfied.


**Example 8.2.1. Portfolio Management - continued**. For this example, the average insurance claim is `r round(mean(Yinsurer), digits=3)` and the corresponding standard deviation is `r round(sd(Yinsurer), digits=3)`. Using equation \@ref(eq:NumSimulations), to be within 1% of the mean, we would only require at least `r round(38416*(sd(Yinsurer)/mean(Yinsurer))^2/1000,digits=2)` thousand simulations. In addition, to be within 0.1% we would want at least `r round(100*38416*(sd(Yinsurer)/mean(Yinsurer))^2/1000000,digits=2)` million simulations.

***

**Example 8.2.2. Approximation Choices.** 
An important application of simulation is the approximation of $\mathrm{E~}[h(X)]$. In this example, we show that the choice of the $h(\cdot)$ function and the distribution of $X$ can play a role.

Consider the following question : what is $\Pr[X>2]$ when $X$ has a `r Gloss('Cauchy distribution')`, with density $f(x) =\left[\pi(1+x^2)\right]^{-1}$, on the real line? The true value is 
$$
\Pr\left[X>2\right] = \int_2^\infty \frac{dx}{\pi(1+x^2)} .
$$
One can use an `R` numerical integration function (which usually works well on improper integrals)

```{r, echo=HtmlEval}
true_value <- integrate(function(x) 1/(pi*(1+x^2)),lower=2,upper=Inf)$value
```

which is equal to `r round(true_value,digits=5)`.

**Approximation 1.** Alternatively, one can use simulation techniques to approximate that quantity. From calculus, you can check that the quantile function of the Cauchy distribution is $F^{-1}(y) = \tan \left[\pi(y-0.5) \right]$. Then, with simulated uniform (0,1) variates, $U_1, \ldots, U_R$, we can construct the estimator
$$
p_1 = \frac{1}{R}\sum_{i=1}^R \mathrm{I}(F^{-1}(U_i)>2) 
= \frac{1}{R}\sum_{i=1}^R \mathrm{I}(\tan \left[ \pi(U_i-0.5) \right]>2) .
$$

`r HideRCode('Approximationchoices.1.9.1', 'Show the R Code')`

```{r  echo=HtmlEval}
Q     <- function(u) { tan(pi*(u-.5)) }
R     <- 1e6
set.seed(1)
X     <- Q(runif(R))
p1    <- mean(X>2)
se.p1 <- sd(X>2)/sqrt(R)
#p1
#se.p1
```

</div>

With one million simulations, we obtain an estimate of `r round(p1,digits=5)` with standard error `r round(se.p1*1000,digits=3)` (divided by 1000). The estimated variance of $p_1$ can be written as  $0.127/R$.

<!--It can be visualized below -->
```{r eval=FALSE, echo = FALSE}
true_value <- integrate(function(x) 1/(pi*(1+x^2)),lower=2,upper=Inf)$value
n     <- 1e3
ns    <- 20
set.seed(1)
simU  <- matrix(Q(runif(n*ns))>2,n,ns)
sumP1 <- apply(simU, 2, cumsum)/(1:n)
matplot(1:n,sumP1[,1:20], type="l", col=rgb(1,0,0,.2), ylim=c(.1,.2),
        xlab="Number of simulations (n)", ylab="Approximation of the integral")
abline(h=true_value,lty=2)
bonds <- cbind( 1.96*sqrt(.127/(1:n)),-1.96*sqrt(.127/(1:n)) )
matlines(1:n,bonds+true_value, col="red", lty=1)
```

**Approximation 2.** With other choices of $h(\cdot)$ and $F(\cdot)$ it is possible to reduce uncertainty even using the same number of simulations $R$. To begin, one can use the symmetry of the Cauchy distribution to write $\Pr[X>2]=0.5\cdot\Pr[|X|>2]$. With this, can construct a new estimator,
$$
p_2 = \frac{1}{2R}\sum_{i=1}^R \mathrm{I}(|F^{-1}(U_i)|>2) .
$$


```{r echo = FALSE}
set.seed(1)
p2    <- mean(abs(Q(runif(R)))>2)/2
se.p2 <- sd(abs(Q(runif(R)))>2)/(2*sqrt(R))
```

With one million simulations, we obtain an estimate of `r round(p2,digits=5)` with standard error `r round(se.p2*1000,digits=3)` (divided by 1000). The estimated variance of $p_2$ can be written as $0.052/R$.

<!--It can be visualized below -->
```{r eval=FALSE, echo = FALSE}
n     <- 1e3
ns    <- 20
set.seed(1)
simU  <- matrix(abs(Q(runif(n*ns)))>2, n, ns)
sumP2 <- apply(simU, 2, cumsum)/(1:n)/2
matplot(1:n,sumP2[,1:20], type="l", col=rgb(1,0,0,.2), ylim=c(.1,.2),
        xlab="Number of simulations (n)", ylab="Approximation of the integral")
abline(h=true_value, lty=2)
bonds <- cbind( 1.96*sqrt(.052/(1:n)),-1.96*sqrt(.052/(1:n)) )
matlines(1:n,bonds+true_value,col="red",lty=1)
```

**Approximation 3.** But one can go one step further. The improper integral can be written as a proper one by a simple symmetry property (since the function is symmetric and the integral on the real line is equal to $1$)
$$
\int_2^\infty \frac{dx}{\pi(1+x^2)}=\frac{1}{2}-\int_0^2\frac{dx}{\pi(1+x^2)} .
$$
From this expression, a natural approximation would be
$$
p_3 = \frac{1}{2}-\frac{1}{R}\sum_{i=1}^R h_3(2U_i), ~~~~~~\text{where}~h_3(x)=\frac{2}{\pi(1+x^2)} .
$$

```{r echo = FALSE}
h <- function(x) 2/(pi*(1+x^2))
set.seed(1)
p3 <- .5-mean(h(2*runif(R)))
se.p3 <- sd(h(2*runif(R)))/sqrt(R)
```

With one million simulations, we obtain an estimate of `r round(p3,digits=5)` 
with standard error `r signif(se.p3*1000,digits=3)` (divided by 1000). The estimated variance of $p_3$ can be written as  $0.0285/R$.

<!--It can be visualized below -->
```{r eval=FALSE, echo = FALSE}
n     <- 1e3
ns    <- 20
set.seed(1)
simU  <- matrix((h(2*runif(n*ns))),n,ns)
sumP3 <- .5-apply(simU, 2, cumsum)/(1:n)
matplot(1:n,sumP3[,1:20],type="l",col=rgb(1,0,0,.2),ylim=c(.1,.2),
        xlab="Number of simulations (n)",ylab="Approximation of the integral")
abline(h=true_value,lty=2)
bonds <- cbind(1.96*sqrt(.0285/(1:n)),-1.96*sqrt(.0285/(1:n)))
matlines(1:n,bonds+true_value,col="red",lty=1)
```

**Approximation 4.** Finally, one can also consider some change of variable in the integral
$$
\int_2^\infty \frac{dx}{\pi(1+x^2)}=\int_0^{1/2}\frac{y^{-2}dy}{\pi(1-y^{-2})} .
$$
From this expression, a natural approximation would be
$$
p_4 = \frac{1}{R}\sum_{i=1}^R h_4(U_i/2),~~~~~\text{where}~h_4(x)=\frac{1}{2\pi(1+x^2)} .
$$
The expression seems rather similar to the previous one. 

```{r echo = FALSE}
set.seed(1)
h4    <- function(x) {1/(2*pi*(1+x^2))}
p4    <- mean(h4(runif(R)/2))
se.p4 <- sd(h4(runif(R)/2))/sqrt(R)
```

With one million simulations, we obtain an estimate of `r round(p4,digits=5)` with standard error `r round(se.p4*1000,digits=3)` (divided by 1000). The estimated variance of $p_4$ can be written as  $0.00009/R$, which is much smaller than what we had so far!

<!--It can be visualized below -->
```{r eval=FALSE, echo = FALSE}
n     <- 1e3
ns    <- 20
set.seed(1)
simU  <- matrix((h(runif(n*ns)/2)),n,ns)
sumP4 <- apply(simU, 2, cumsum)/(1:n)
matplot(1:n,sumP4[,1:20],type="l",col=rgb(1,0,0,.2),ylim=c(.1,.2),
        xlab="Number of simulations (n)", ylab="Approximation of the integral")
abline(h=true_value, lty=2)
bonds <- cbind( 1.96*sqrt(.00009/(1:n)),-1.96*sqrt(.00009/(1:n)) )
matlines(1:n,bonds+true_value, col="red",lty=1)
```

[Table 8.1](#tab:8.1) summarizes the four choices of $h(\cdot)$ and $F(\cdot)$ to approximate $\Pr[X>2] =$ `r round(true_value,digits=5)`. The standard error varies dramatically. Thus, if we have a desired degree of accuracy, then the *number of simulations* depends strongly on how we write the integrals we try to approximate.

[Table 8.1]: ./ChapSimulation.html#tab:8.1

<a id=tab:8.1></a>

[Table 8.1]{#tab:8.1}. **Summary of Four Choices to Approximate** $\Pr[X>2]$

<div align="center">
```{r echo = FALSE}
library(htmlTable)
x1 <- c(
  "$p_1$", 
  "$\\frac{1}{R}\\sum_{i=1}^R \\mathrm{I}(F^{-1}(U_i)>2)$",
   "$~~~~~~F^{-1}(u)=\\tan \\left( \\pi(u-0.5) \\right)~~~~~~~$",round(c(p1,se.p1),digits=6))
x2 <- c(
  "$p_2$", 
  "$\\frac{1}{2R}\\sum_{i=1}^R \\mathrm{I}(|F^{-1}(U_i)|>2)$",
   "$F^{-1}(u)=\\tan \\left( \\pi(u-0.5) \\right)$",round(c(p2,se.p2),digits=6))
x3 <- c(
  "$p_3$", 
  "$\\frac{1}{2}-\\frac{1}{R}\\sum_{i=1}^R h_3(2U_i)$",
"$h_3(x)=\\frac{2}{\\pi(1+x^2)}$",round(c(p3,se.p3),digits=6))
x4 <- c(
  "$p_4$", 
  "$\\frac{1}{R}\\sum_{i=1}^R h_4(U_i/2)$",
  "$h_4(x)=\\frac{1}{2\\pi(1+x^2)}$",format(round(c(p4,se.p4),digits=6), scientific = FALSE))
outMat <- rbind(x1,x2,x3,x4)
rownames(outMat) <- NULL

colnames(outMat) <- c("Estimator", "Definition",
                            "Support \n Function", "Estimate", " Standard \n Error")
if (PdfEval) {knitr::kable(outMat, "latex", booktabs=TRUE, escape = F, align = "cccrr") %>%
  kable_styling(font_size = 10, latex_options="scale_down") %>%
     kable_styling(latex_options = "hold_position") 
}
if (HtmlEval) {knitr::kable(outMat, "html", booktabs=TRUE, escape = F, align = "cccrr")   

  }
```
</div>




### Simulation and Statistical Inference {#S:Sec823}

Simulations not only help us approximate expected values but are also useful in calculating other aspects of distribution functions. As described in Section \@ref(S:Sec821), the logic is that one wishes to calculate a parameter $\theta(F)$, use the same rule for calculating the parameter but replace the distribution function $F(\cdot)$ with an empirical one from a simulated sample. For example, in addition to expected values, analysts can use simulation to compute quantiles from complex distributions.


In addition, simulation is very useful when distributions of test statistics are too complicated to derive; in this case, one can use simulations to approximate the reference distribution. We now illustrate this with the `r Gloss("Kolmogorov-Smirnov test")` which we learned about in Section \@ref(S:Sec612). 

**Example 8.2.3. Kolmogorov-Smirnov Test of Distribution.** 
Suppose that we have available $n=100$ observations $\{x_1,\cdots,x_n\}$ that, unknown to the analyst, were generated from a gamma distribution with parameters $\alpha = 6$ and $\theta=2$. The analyst believes that the data come from a lognormal distribution with parameters 1 and 0.4 and would like to test this assumption.

The first step is to visualize the data. 

`r HideRCode('KSTest.1.10.1','Show R Code To Set up The Visualization')`

```{r   echo=HtmlEval}
set.seed(1)
n  <- 100
x  <- rgamma(n, 6, 2)
u  <- seq(0,7,by=.01)
vx <- c(0,sort(x))
vy <- (0:n)/n
```

</div>

With this set-up, Figure \@ref(fig:Fig86) provides a graph of a histogram and empirical distribution. For reference, superimposed are red dashed lines from the lognormal distribution. 

(ref:Fig86) **Histogram and Empirical Distribution Function of Data used in Kolmogorov-Smirnov Test**. The red dashed lines are fits based on (incorrectly) hypothesized lognormal distribution.


```{r Fig86,  fig.cap = '(ref:Fig86)', echo=FALSE}
par(mfrow=c(1,2))
hist(x,probability = TRUE,main="Histogram", col="light blue",
     border="white",xlim=c(0,7),ylim=c(0,.4))
lines(u,dlnorm(u,1,.4),col="red",lty=2)
plot(vx,vy,type="l",xlab="x",ylab="Cumulative Distribution",main="Empirical cdf")
lines(u,plnorm(u,1,.4),col="red",lty=2)
```

`r HideRCode('Example823.Hide', 'Show R Code for Plotting Figure 8.6')`

```{r ref.label = 'Fig86', eval = FALSE,  echo=HtmlEval}

```

</div>

Recall that the Kolmogorov-Smirnov statistic equals the largest discrepancy between the empirical and the hypothesized distribution. This is $\max_x |F_n(x)-F_0(x)|$, where $F_0$ is the hypothesized lognormal distribution. We can calculate this directly.

`r HideRCode('KSTest.1.10.2','Show R Code for a Direct Calculation of the KS Statistic')`

```{r  echo=HtmlEval}
# test statistic
D <- function(data, F0){
   F <- Vectorize( function(x) mean( (data<=x) ) )
   n <- length(data)
   x <- data
   d1 = abs(F(x) - F0(x))
   d2 = abs(F(x) - 1/n - F0(x))   
   return(max(c(d1,d2)))
  }
#D(x,function(x) plnorm(x,1,.4))
```

</div>

Fortunately, for the lognormal distribution, `R` has built-in tests that allow us to determine this without complex programming:

`r CodeFontSmall()`

```{r }
ks.test(x, plnorm, mean=1, sd=0.4)
```

`r CodeFontLarge()`

However, for many distributions of actuarial interest, pre-built programs are not available. We can use simulation to test the relevance of the test statistic. Specifically, to compute the $p$-value, let us generate thousands of random samples from a $LN(1,0.4)$ distribution (with the same size), and compute empirically the distribution of the statistic,

`r HideRCode('KSTest.1.10.3','Show R Code for the Simulation Distribution of the KS Statistic')`

`r CodeFontSmall()`

```{r }
ns <- 1e4
d_KS <- rep(NA,ns)
# compute the test statistics for a large (ns) number of simulated samples
for(s in 1:ns) d_KS[s] <- D(rlnorm(n,1,.4),function(x) plnorm(x,1,.4))
mean(d_KS>D(x,function(x) plnorm(x,1,.4)))
```

`r CodeFontLarge()`

</div>

(ref:Fig87) **Simulated Distribution of the Kolmogorov-Smirnov Test Statistic**. The vertical red dashed line marks the test statistic for the sample of 100.

```{r Fig87,   fig.cap = '(ref:Fig87)', echo=FALSE}
hist(d_KS,probability = TRUE,col="light blue",border="white",xlab="Test Statistic",main="")
lines(density(d_KS),col="red")
abline(v=D(x,function(x) plnorm(x,1,.4)),lty=2,col="red")

```


The simulated distribution based on 10,000 random samples is summarized in Figure \@ref(fig:Fig87). Here, the statistic exceeded the empirical value (`r format(D(x,function(x) plnorm(x,1,.4)),digits=4)`) in `r 100*mean(d_KS>D(x,function(x) plnorm(x,1,.4)))`% of the scenarios, while the *theoretical* $p$-value is `r round(ks.test(x,plnorm,mean=1,sd=.4)$p.value, digits=4)`. For both the simulation and the theoretical $p$-values, the conclusions are the same; the data do not provide sufficient evidence to reject the hypothesis of a lognormal distribution.


Although only an approximation, the simulation approach works in a variety of distributions and test statistics without needing to develop the nuances of the underpinning theory for each situation. We summarize the procedure for developing simulated distributions and *p*-values as follows:

1. Draw a sample of size *n*, say, $X_1, \ldots, X_n$, from a known distribution function $F$. Compute a statistic of interest, denoted as $\hat{\theta}(X_1, \ldots, X_n)$. Call this $\hat{\theta}^r$ for the *r*th replication.
2. Repeat this $r=1, \ldots, R$ times to get a sample of statistics, $\hat{\theta}^1, \ldots,\hat{\theta}^R$.
3. From the sample of statistics in Step 2, $\{\hat{\theta}^1, \ldots,\hat{\theta}^R\}$, compute a summary measure of interest, such as a *p*-value.
 


## Bootstrapping and Resampling {#S:Sec83}

***

In this section, you learn how to:

-   Generate a nonparametric bootstrap distribution for a statistic of interest
-   Use the bootstrap distribution to generate estimates of precision for the statistic of interest, including bias, standard deviations, and confidence intervals
-   Perform bootstrap analyses for parametric distributions

***

### Bootstrap Foundations {#S:Sec831}

Simulation presented up to now is based on sampling from a **known** distribution. Section \@ref(S:Sec81) showed how to use simulation techniques to sample and compute quantities from known distributions.  However, statistical  science is dedicated to providing inferences about distributions that are *unknown*. We gather summary statistics based on this unknown population distribution. But how do we sample from an unknown distribution? 

Naturally, we cannot simulate draws from an unknown distribution but we can draw from a sample of observations. If the sample is a good representation from the population, then our simulated draws from the sample should well approximate the simulated draws from a population. The process of sampling from a sample is called *resampling* or *bootstrapping*. The term `r Gloss('bootstrap')` comes from the phrase "pulling oneself up by one's bootstraps" @efron1979bootstrap. With resampling, the original sample plays the role of the population and estimates from the sample play the role of true population parameters.

The resampling algorithm is the same as introduced in Section \@ref(S:Sec823) except that now we use simulated draws from a sample. It is common to use $\{X_1, \ldots, X_n\}$ to denote the original sample and let $\{X_1^*, \ldots, X_n^*\}$ denote the simulated draws. We draw them with replacement so that the simulated draws will be independent from one another, the same assumption as with the original sample. For each sample, we also use *n* simulated draws, the same number as the original sample size. To distinguish this procedure from the simulation, it is common to use *B* (for bootstrap) to be the number of simulated samples. We could also write $\{X_1^{(b)}, \ldots, X_n^{(b)}\}$, $b=1,\ldots, B$ to clarify this.

There are two basic resampling methods, *model-free* and *model-based*, which are, respectively, as *nonparametric* and *parametric*. In the `r Gloss('nonparametric approach')`, no assumption is made about the distribution of the parent population. The simulated draws come from the empirical distribution function $F_n(\cdot)$, so each draw comes from $\{X_1, \ldots, X_n\}$ with probability 1/*n*. 

In contrast, for the `r Gloss('parametric approach')`, we assume that we have knowledge of the distribution family *F*. The original sample $X_1, \ldots, X_n$ is used to estimate parameters of that family, say, $\hat{\theta}$. Then, simulated draws are taken from the $F(\hat{\theta})$. Section \@ref(S:Sec834) discusses this approach in further detail.


#### Nonparametric Bootstrap {-}

The idea of the nonparametric bootstrap is to use the inverse transform method on $F_n$, the empirical cumulative distribution function, depicted in Figure \@ref(fig:Fig88).

(ref:Fig88) **Inverse of an Empirical Distribution Function**

```{r Fig88, fig.cap='(ref:Fig88)', out.width='60%', fig.asp=.75,  echo=FALSE}
plot.new()
par(cex=1.3)
set.seed(1)
x <- sort(c(0,rexp(10, 1/6)))
y<- (0:10)/10
plot(x,y, xlim=c(0, 10), ylim=c(0, 1), lwd=2,
        xlab="", type="s", ylab="",xaxs="i", yaxs="i", xaxt="n", yaxt="n")
vx <- seq(0, 10, by=.01)
vy<- 1-exp(-vx/6)
lines(vx,vy,lty=2,col="red")
x.6 <- x[9]
y.6 <- (sum(x<x.6))/10-.03
mtext("y=F(x)", side=2, line=2, cex=1.3, las=2, padj=-4, adj=.5) # TO MOVE UPWARD
axis(1, at=x.6, labels=expression("x =" ~ F^{-1} *"(y)"))
segments(x.6,0,x.6,y.6)
segments(0,y.6,x.6,y.6)
axis(1, at=0)
axis(2, at=0)
```

Because $F_n$ is a step-function, $F_n^{-1}$ takes values in $\{x_1,\cdots,x_n\}$. More precisely, as illustrated in Figure \@ref(fig:Fig89).

- if $y\in(0,1/n)$ (with probability $1/n$) we draw the smallest value ($\min\{x_i\}$)
- if $y\in(1/n,2/n)$ (with probability $1/n$) we draw the second smallest value,
- $\vdots \ \ \vdots \ \ \vdots$
- if $y\in((n-1)/n,1)$ (with probability $1/n$) we draw the largest value ($\max\{x_i\}$).

(ref:Fig89) **Inverse of an Empirical Distribution Function**

```{r Fig89, fig.cap='(ref:Fig89)', out.width='60%', fig.asp=.75,  echo=FALSE}
plot.new()
par(cex=1.3)
set.seed(1)
x <- sort(c(0,rexp(10, 1/6)))
y<- (0:10)/10
plot(x,y, xlim=c(0, 10), ylim=c(0, 1), lwd=2,
        xlab="", type="s", ylab="",xaxs="i", yaxs="i", xaxt="n", yaxt="n")
vx <- seq(0, 10, by=.01)
clr <- c(rgb(0,1,0,.2),rgb(0,0,1,.2))
for(i in 1:10){
  rect(-10,(i-1)/10,100,i/10,col=clr[1+i%%2],border="white")
}
abline(v=x,col="white")
lines(x,y,lwd=2,type="s")
vy<- 1-exp(-vx/6)
lines(vx,vy,lty=2,col="red")
```

Using the inverse transform method with $F_n$ means sampling from $\{x_1,\cdots,x_n\}$, with probability $1/n$. Generating a bootstrap sample of size $B$ means sampling from $\{x_1,\cdots,x_n\}$, with probability $1/n$, with replacement. See the following illustrative `R` code.


`r HideRCode('BootStrap.A','Show R Code For Creating a Bootstrap Sample')`

`r CodeFontSmall()`

```{r  }
set.seed(1)
n <- 10
x <- rexp(n, 1/6)
m <- 10
bootvalues <- sample(x, size=m, replace=TRUE)
```

</div>

```{r  echo=FALSE}
round(bootvalues,digits=4)
```

`r CodeFontLarge()`


Observe that value `r round(bootvalues[2],digits=4)` was obtained three times.



### Bootstrap Precision: Bias, Standard Deviation, and Mean Square Error {#S:Sec832}


We summarize the nonparametric bootstrap procedure as follows:

1. From the sample $\{X_1, \ldots, X_n\}$, draw a sample of size *n* (with replacement), say, $X_1^*, \ldots, X_n^*$. From the simulated draws compute a statistic of interest, denoted as $\hat{\theta}(X_1^*, \ldots, X_n^*)$. Call this $\hat{\theta}_b^*$ for the *b*th replicate.
2. Repeat this $b=1, \ldots, B$ times to get a sample of statistics, $\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*$.
3. From the sample of statistics in Step 2, $\{\hat{\theta}_1^*, \ldots, \hat{\theta}_B^*\}$, compute a summary measure of interest.
 
In this section, we focus on three summary measures, the `r Gloss('bias')`, the standard deviation, and the mean square error (*MSE*). [Table 8.3](#tab:8.3) summarizes these three measures. Here, $\overline{\hat{\theta^*}}$ is the average of $\{\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*\}$.


[Table 8.3]: ./ChapSimulation#tab:8.3

<a id=tab:8.3></a>

[Table 8.3]{#tab:8.3}. **Bootstrap Summary Measures**

$$
{\small
\begin{matrix}
\begin{array}{l|c|c|c}
\hline
\textit{Population Measure}& \textit{Population Definition}&\textit{Bootstrap Approximation}&\textit{Bootstrap Symbol}\\
\hline
\text{Bias} & \mathrm{E}(\hat{\theta})-\theta&\overline{\hat{\theta^*}}-\hat{\theta}& Bias_{boot}(\hat{\theta})  \\\hline
\text{Standard Deviation} &   \sqrt{\mathrm{Var}(\hat{\theta})}
& \sqrt{\frac{1}{B-1} \sum_{b=1}^{B}\left(\hat{\theta}_b^* -\overline{\hat{\theta^*}} \right) ^2}&s_{boot}(\hat{\theta})  \\\hline
\text{Mean Square Error} &\mathrm{E}(\hat{\theta}-\theta)^2 & \frac{1}{B} \sum_{b=1}^{B}\left(\hat{\theta}_b^* -\hat{\theta}
\right)^2&MSE_{boot}(\hat{\theta})\\
\hline
\end{array}\end{matrix}
}
$$



***

<a id=Ex:8.3.1></a>  

[Example 8.3.1]: ./ChapSimulation.html#Ex:8.3.1

**[Example 8.3.1]{#Ex:8.3.1}. Bodily Injury Claims and Loss Elimination Ratios.** To show how the bootstrap can be used to quantify the precision of estimators, we return to the [Example 5.3.2] bodily injury claims data where we introduced a nonparametric estimator of the loss elimination ratio.

[Table 8.4](#tab:8.4) summarizes the results of the bootstrap estimation. For example, at $d=14000$, the nonparametric estimate of *LER* is 0.97678. This has an estimated bias of 0.00016 with a standard deviation of 0.00687. For some applications, you may wish to apply the estimated bias to the original estimate to give a `r Gloss('bias-corrected estimator')`. This is the focus of the next example. For this illustration, the bias is small and so such a correction is not relevant.


`r HideRCode('LER8.2.1','Show R Code For Bootstrap Estimates of LER')`

```{r   echo=HtmlEval}
library(boot) # for boot(), boot.ci()
# Example from Derrig et al
BIData <- read.csv("Data/DerrigResampling.csv", header =T)
BIData$Censored <- 1*(BIData$AmountPaid >= BIData$PolicyLimit)
BIDataUncensored <- subset(BIData, Censored == 0)
LER.boot <- function(ded, data, indices){
  resample.data <- data[indices,]
  sumClaims     <- sum(resample.data$AmountPaid)
  sumClaims_d   <- sum(pmin(resample.data$AmountPaid,ded))
  LER           <- sumClaims_d/sumClaims
  return(LER)  
}

##Derrig et al
set.seed(2019)
dVec2 <- c(4000, 5000, 10500, 11500, 14000, 18500)
OutBoot <- matrix(0,length(dVec2),6)
  for (i in 1:length(dVec2)) {
OutBoot[i,1] <- dVec2[i]
results      <- boot(data=BIDataUncensored, statistic=LER.boot, 
                     R=1000, ded=dVec2[i])
OutBoot[i,2] <- results$t0
biasboot     <- mean(results$t)-results$t0 -> OutBoot[i,3]
sdboot       <- sd(results$t) -> OutBoot[i,4]
temp         <- boot.ci(results, type = "norm")
OutBoot[i,5] <- temp$normal[2]
OutBoot[i,6] <- temp$normal[3]
}
```

</div>

[Table 8.4]: ./ChapSimulation#tab:8.4

<a id=tab:8.4></a>

[Table 8.4]{#tab:8.4}. **Bootstrap Estimates of LER at Selected Deductibles**

```{r  echo=FALSE}
library(kableExtra)
OutBoot.latex <- OutBoot
colnames(OutBoot) <- c("d","NP Estimate","Bootstrap Bias", "Bootstrap SD", 
                           "Lower Normal 95% CI", "Upper Normal 95% CI")
if (HtmlEval) {knitr::kable(OutBoot, "html",digits=5) }
if (PdfEval) {
  kbl(OutBoot.latex, booktabs = T, digits=5) %>%
  kable_styling(latex_options="scale_down") %>%
     kable_styling(latex_options = "hold_position", font_size = 10) %>%
    add_header_above(c(" ","Estimate"=1,"Bias"=1,"SD"=1, 
                            "95% CI"=1, "95% CI"=1)) %>%
      add_header_above(c("d","NP"=1,"Bootstrap"=1, "Bootstrap"=1,
                             "Lower Normal"=1, "Upper Normal"=1))
      }
```



The bootstrap standard deviation gives a measure of precision. For one application of standard deviations, we can use the normal approximation to create a confidence interval. For example, the `R` function `boot.ci` produces the normal confidence intervals at 95%. These are produced by creating an interval of twice the length of 1.95994 bootstrap standard deviations, centered about the bias-corrected estimator (1.95994 is the 97.5th quantile of the standard normal distribution). For example, the lower normal 95% CI at $d=14000$ is $(0.97678-0.00016)- 1.95994 \times 0.00687$ $= 0.96316$. We further discuss bootstrap confidence intervals in the next section.

***

**Example 8.3.2. Estimating $\log(\mu)$.** The bootstrap can be used to quantify the bias of an estimator, for instance. Consider here a sample $\mathbf{x}=\{x_1,\cdots,x_n\}$ that is `r Gloss('iid')` with mean $\mu$.

`r CodeFontSmall()`

```{r }
sample_x <- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40,5.22,2.55,
              2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77)
```

`r CodeFontLarge()`

```{r include = FALSE, eval = FALSE}
library(VGAM)
fit.gamma <- vglm(sample_x ~ 1, family = gamma2)
( theta <- exp(coef(fit.gamma)[1]) / exp(coef(fit.gamma)[2]) ) # theta = mu / alpha  
( alpha <- exp(coef(fit.gamma)[2]) )
alpha * theta
```

Suppose that the quantity of interest is $\theta=\log(\mu)$. A natural estimator would be $\widehat{\theta}_1=\log(\overline{x})$. This estimator is biased (due to the `r Gloss('Jensen inequality')`) but is asymptotically unbiased. For our sample, the estimate is as follows.

`r CodeFontSmall()`

```{r  }
(theta_1 <- log(mean(sample_x)))
```

`r CodeFontLarge()`

One can use  a bootstrap strategy to get a correction: given a bootstrap sample, $\mathbf{x}^{\ast}_{b}$, let $\overline{x}^{\ast}_{b}$ denote its mean, and set

$$
\widehat{\theta}_2=\frac{1}{B}\sum_{b=1}^B\log(\overline{x}^{\ast}_{b}) .
$$

To implement this, we have the following code where we now use the function `boot` from the `R` package `boot`.

`r HideRCode('bootstrapdisn.1','Show R Code for Creating Bootstrap Samples')`

`r CodeFontSmall()`

```{r }
library(boot)
results <- boot(data=sample_x, 
                statistic=function(y,indices) { log(mean(y[indices])) }, 
                R=1000)
theta_2 <- 2*theta_1 - mean(results$t)

```

`r CodeFontLarge()`

</div>

\newpage

Then, you can `plot(results)` and `print(results)` to see the following.

(ref:Fig810) **Distribution of Bootstrap Replicates**. The left-hand panel is a histogram of replicates. The right-hand panel is a quantile-quantile plot, comparing the bootstrap distribution to the standard normal distribution.

`r CodeFontSmall()`

```{r Fig810, fig.cap='(ref:Fig810)',    echo=FALSE}
plot(results)
print(results)
```

`r CodeFontLarge()`

This results in two estimators, the raw estimator $\widehat{\theta}_1=$ `r round(theta_1,digits=3)` and the bootstrap estimator $\widehat{\theta}_2=$ `r round(theta_2,digits=3)`.

How does this work with differing sample sizes? We now suppose that the $x_i$'s are generated from a gamma distribution with shape parameter $\alpha = 0.25$ and scale parameter $\theta = 12$. We use simulation to draw the sample sizes but then act as if they were a realized set of observations. See the following illustrative code.

`r HideRCode('bootstrapdisn.2','Show R Code for Creating Bootstrap Samples')`

`r CodeFontSmall()`

```{r  eval = FALSE}
param <- function(x){
  n <- length(x)
  theta_1 <- log( mean(x) )
  results <- boot(data=x, 
                statistic=function(y,indices){log(mean(y[indices]))} , 
                R=999)
  theta_2 <- 2*theta_1 - mean(results$t)
  return( c(theta_1,theta_2) )
  }
set.seed(2074)
ns  <- 200 
est <- function(n){
  call_param <- function(i) { param(rgamma(n,shape=0.25,scale=12)) }
  V <- Vectorize(call_param)(1:ns)
  apply(V, 1, median)
  }
VN  <- seq(15, 100, by=5)
Est <- Vectorize(est)(VN)

save(VN, Est, file= "../IntermediateCalcs/SimulationChapter/Section832Bootstrap.Rdata")

```

`r CodeFontLarge()`


</div>


The results of the comparison are summarized in Figure \@ref(fig:Fig811). This figure shows that the bootstrap estimator is closer to the true parameter value for many of the sample sizes. The bias of both estimators decreases as the sample size increases.


(ref:Fig811) **Comparison of Estimates.** True value of the parameter is given by the solid horizontal line at $\log(3) \approx 1.099$.

```{r Fig811, fig.cap='(ref:Fig811)',    echo=FALSE,  out.width='70%', fig.asp=.70}
load(file= "IntermediateCalcs/SimulationChapter/Section832Bootstrap.Rdata")
matplot(VN,t(Est),type="l", col=2:4, lty=2:4, ylim=log(3) + c(-0.2, 0.2),
        xlab="sample size (n)", ylab="estimator")
abline(h=log(3),lty=1, col=1)
legend("topleft", c("raw estimator", "bootstrap"),
       col=2:4, lty=2:4, bty="n")
```

***

Although successful in this example, we remark that the bootstrap bias adjusted estimator is generally not used in practice because the bias adjustment introduces extra variability into the estimator. Instead, the bias estimate provides information as to whether or not the estimate contains bias; this information gives additional information about the reliability of the estimate.

### Confidence Intervals  {#S:Sec833}

The bootstrap procedure generates *B* replicates $\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*$ of the estimator $\hat{\theta}$. In [Example 8.3.1](#Ex:8.3.1), we saw how to use standard normal approximations to create a confidence interval for parameters of interest. However, given that a major point is to use bootstrapping to avoid relying on assumptions of approximate normality, it is not surprising that there are alternative confidence intervals available.

For an estimator $\hat{\theta}$, the *basic* bootstrap confidence interval is
\begin{equation} 
  \left(2 \hat{\theta} - q_U, 2 \hat{\theta} - q_L \right) ,
(\#eq:basicBootCI)
\end{equation}
where $q_L$ and $q_U$ are lower and upper 2.5% quantiles from the bootstrap sample $\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*$.

To see where this comes from, start with the idea that $(q_L, q_U)$ provides a 95% interval for $\hat{\theta}_1^*, \ldots,\hat{\theta}_B^*$. So, for a random $\hat{\theta}_b^*$, there is a 95% chance that $q_L \le \hat{\theta}_b^* \le q_U$. Reversing the inequalities and adding $\hat{\theta}$ to each side gives a 95% interval 
$$
\hat{\theta} -q_U \le \hat{\theta} - \hat{\theta}_b^* \le  \hat{\theta} -q_L .
$$
So, $\left( \hat{\theta}-q_U,  \hat{\theta} -q_L\right)$ is an 95% interval for $\hat{\theta} - \hat{\theta}_b^*$. The bootstrap approximation idea says that this is also a 95% interval for $\theta - \hat{\theta}$. Adding $\hat{\theta}$ to each side gives the 95% interval in equation \@ref(eq:basicBootCI). 

Many alternative bootstrap intervals are available. The easiest to explain is the `r Gloss('percentile bootstrap interval')` which is defined as $\left(q_L, q_U\right)$. However, this has the drawback of potentially poor behavior in the tails which can be of concern in some actuarial problems of interest.

<a id=8.3.3></a>  

[Example 8.3.3]: ./ChapSimulation.html#8.3.3


**[Example 8.3.3]{#Ex:8.3.3}. Bodily Injury Claims and Risk Measures.** To see how the bootstrap confidence intervals work, we return to the bodily injury auto claims considered in [Example 8.3.1](#Ex:8.3.1). Instead of the loss elimination ratio, suppose we wish to estimate the 95th percentile $F^{-1}(0.95)$ and a measure defined as
$$
ES_{0.95}[X] = \mathrm{E}[X | X > F^{-1}(0.95)] .
$$
This measure is called the `r Gloss('expected shortfall')`. In this formulation, it is the expected value of $X$ conditional on $X$ exceeding the 95th percentile which is also sometimes known as the *conditional value at risk*. Section \@ref(S:Sec132) explains how quantiles and the expected shortfall are the two most important examples of so-called *risk measures*. For now, we will simply think of these as measures that we wish to estimate. For the percentile, we use the nonparametric estimator $F^{-1}_n(0.95)$ defined in Section \@ref(S:MS:QuantileEstimator). For the expected shortfall, we use the plug-in principle to define the nonparametric estimator
$$
ES_{n,0.95}[X] = \frac{\sum_{i=1}^n X_i I[X_i > F^{-1}_n(0.95)]}{\sum_{i=1}^n I[X_i > F^{-1}_n(0.95)]} ~.
$$
In this expression, the denominator counts the number of observations that exceed the 95th percentile $F^{-1}_n(0.95)$. The numerator adds up losses for those observations that exceed $F^{-1}_n(0.95)$. [Table 8.5](#tab:8.5) summarizes the estimator for selected fractions.


`r HideRCode('bootstrapquantiles.1','Show R Code for Creating Quantile Bootstrap Samples')`

```{r   echo=HtmlEval}
# Example from Derrig et al
#BIData <- read.csv("./Data/DerrigResampling.csv", header =T)
BIData$Censored  <- 1*(BIData$AmountPaid >= BIData$PolicyLimit)
BIDataUncensored <- subset(BIData, Censored == 0)

set.seed(2017)
PercentVec      <- c(0.50, 0.80, 0.90, 0.95, 0.98)
OutBoot1        <- matrix(0,5,10)
for (i in 1:length(PercentVec)) {
  OutBoot1[i,1] <- PercentVec[i]
  results <- boot(data=BIDataUncensored$AmountPaid,
                statistic=function(X,indices)
                    quantile(X[indices],PercentVec[i]),
                 R=1000)
  if (i==1){bootreal <- results$t}
  OutBoot1[i,2]  <- results$t0
  OutBoot1[i,3]  <- mean(results$t)-results$t0 
  OutBoot1[i,4]  <- sd(results$t) 
  temp           <- boot.ci(results, type = c("norm", "basic", "perc"))
  OutBoot1[i,5]  <- temp$normal[2]
  OutBoot1[i,6]  <- temp$normal[3]
  OutBoot1[i,7]  <- temp$basic[4]
  OutBoot1[i,8]  <- temp$basic[5]
  OutBoot1[i,9]  <- temp$percent[4]
  OutBoot1[i,10] <- temp$percent[5]
  }

```

</div>

[Table 8.5]: ./ChapSimulation#tab:8.5

<a id=tab:8.5></a>

[Table 8.5]{#tab:8.5}. **Bootstrap Estimates of Quantiles at Selected Fractions**

```{r  echo=FALSE}
OutBoot1.latex <- OutBoot1
colnames(OutBoot1) <- c("Fraction","NP Estimate", "Bootstrap Bias", 
       "Bootstrap SD", "Lower Normal 95% CI", "Upper Normal  95% CI",
       "Lower Basic 95% CI", "Upper Basic 95% CI",
       "Lower Percentile 95% CI", "Upper  Percentile 95% CI")
if (HtmlEval) {knitr::kable(OutBoot1, "html",digits=2) }
if (PdfEval) {kbl(OutBoot1.latex, booktabs = T, digits=2) %>%
  kable_styling(latex_options="scale_down") %>%
    add_header_above(c("","Estimate"=1, "Bias"=1, 
       "SD"=1, "95% CI"=1, "95% CI"=1,
       "95% CI"=1, "95% CI"=1,
       "95% CI"=1, "95% CI"=1)) %>%
    add_header_above(c("Fraction"=1,"NP"=1, "Bootstrap"=1, 
       "Bootstrap"=1, "Lower Normal"=1, "Upper Normal"=1,
       "Lower Basic"=1, "Upper Basic"=1,
       "Lower Percentile"=1, "Upper  Percentile"=1)) %>%
     kable_styling(latex_options = "hold_position", font_size = 10) 
  }
```

For example, when the fraction is 0.50, we see that lower and upper 2.5th quantiles of the bootstrap simulations are $q_L=$ `r quantile(bootreal,.025, type=6)` and $q_u=$ `r quantile(bootreal,.975, type=6)`, respectively. These form the percentile bootstrap confidence interval. With the nonparametric estimator `r quantile(BIDataUncensored$AmountPaid,.5)`, these yield the lower and upper bounds of the basic confidence interval `r 2*quantile(BIDataUncensored$AmountPaid,.5)-quantile(bootreal,.975, type=6)`
and `r 2*quantile(BIDataUncensored$AmountPaid,.5)-quantile(bootreal,.025, type=6)`, respectively. [Table 8.5](#tab:8.5) also shows bootstrap estimates of the bias, standard deviation, and a normal confidence interval, concepts introduced in Section \@ref(S:Sec832).


[Table 8.6](#tab:8.6) shows similar calculations for the expected shortfall. In each case, we see that the bootstrap standard deviation increases as the fraction increases. This is because there are fewer observations to estimate quantiles as the fraction increases, leading to greater imprecision.  Confidence intervals also become wider. Interestingly, there does not seem to be the same pattern in the estimates of the bias.


`r HideRCode('bootstrapquantiles.2','Show R Code for Creating ES Bootstrap Samples')`

```{r   echo=HtmlEval}

ES.boot <- function(data, indices, RiskLevel){
  resample.data <- data[indices,]
  X             <- resample.data$AmountPaid
  cutoff        <- quantile(X, RiskLevel)
  ES           <- sum(X*(X > cutoff))/sum(X > cutoff)
  return(ES) 
}

set.seed(2017)  
PercentVec <- c(0.50, 0.80, 0.90, 0.95, 0.98)
OutBoot1   <- matrix(0,5,10)
for (i in 1:length(PercentVec)) {
  OutBoot1[i,1]  <- PercentVec[i]
  results        <- boot(data=BIDataUncensored, statistic=ES.boot, 
                         R=1000, RiskLevel=PercentVec[i])
  OutBoot1[i,2]  <- results$t0
  OutBoot1[i,3]  <- mean(results$t)-results$t0 
  OutBoot1[i,4]  <- sd(results$t) 
  temp           <- boot.ci( results, type = c("norm", "basic", "perc") )
  OutBoot1[i,5]  <- temp$normal[2]
  OutBoot1[i,6]  <- temp$normal[3]
  OutBoot1[i,7]  <- temp$basic[4]
  OutBoot1[i,8]  <- temp$basic[5]
  OutBoot1[i,9]  <- temp$percent[4]
  OutBoot1[i,10] <- temp$percent[5]
  }

```

</div>

\newpage

[Table 8.6]: ./ChapSimulation#tab:8.6

<a id=tab:8.6></a>

[Table 8.6]{#tab:8.6}. **Bootstrap Estimates of ES at Selected Risk Levels**


```{r  echo=FALSE}
OutBoot1.latex <- OutBoot1
colnames(OutBoot1) <- c("Fraction","NP Estimate", "Bootstrap Bias", 
       "Bootstrap SD", "Lower Normal 95% CI", "Upper Normal  95% CI",
       "Lower Basic 95% CI", "Upper Basic 95% CI",
       "Lower Percentile 95% CI", "Upper  Percentile 95% CI")
if (HtmlEval) {knitr::kable(OutBoot1, "html",digits=2) }
if (PdfEval) {kbl(OutBoot1.latex,  booktabs = T, digits=2) %>%
  kable_styling(latex_options="scale_down") %>%
    add_header_above(c("","Estimate"=1, "Bias"=1, 
       "SD"=1, "95% CI"=1, "95% CI"=1,
       "95% CI"=1, "95% CI"=1,
       "95% CI"=1, "95% CI"=1))  %>%
    add_header_above(c("Fraction"=1,"NP"=1, "Bootstrap"=1, 
       "Bootstrap"=1, "Lower Normal"=1, "Upper Normal"=1,
       "Lower Basic"=1, "Upper Basic"=1,
       "Lower Percentile"=1, "Upper  Percentile"=1)) %>%
     kable_styling(latex_options = "hold_position") 
  }
```



### Parametric Bootstrap {#S:Sec834}

The idea of the nonparametric bootstrap is to resample  by drawing independent variables from the empirical cumulative distribution function $F_n$. In contrast, with parametric bootstrap, we draw independent variables from $F_{\widehat{\theta}}$ where the underlying distribution is assumed to be in a parametric family such as a gamma or lognormal distribution. Typically, parameters from this distribution are estimated based on a sample and denoted as $\hat{\theta}$.


**Example 8.3.4. Lognormal distribution.** Consider again the dataset  

`r CodeFontSmall()`

```{r }
sample_x <- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40,
              5.22,2.55,2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77)

```

`r CodeFontLarge()`

The classical (nonparametric) bootstrap was based on the following samples.

`r CodeFontSmall()`

```{r }
x <- sample(sample_x,replace=TRUE)

```

`r CodeFontLarge()`

Instead, for the parametric bootstrap, we have to assume that the distribution of $x_i$'s is from a specific family. As an example, the following code utilizes a lognormal distribution.

`r CodeFontSmall()`

```{r }
library(MASS)
fit <- fitdistr(sample_x, dlnorm, list(meanlog = 1, sdlog = 1))
fit

```

`r CodeFontLarge()`

Then we draw from that distribution.

`r CodeFontSmall()`

```{r }
x <- rlnorm(length(sample_x), meanlog=fit$estimate[1], sdlog=fit$estimate[2])
```

`r CodeFontLarge()`

`r HideRCode('BootES.1','Show R Code for Parametric Bootstrap Samples')`

```{r echo=HtmlEval}
set.seed(2074)
CV       <- matrix(NA,1e5,2)
for(s in 1:nrow(CV)){
  x1     <- sample(sample_x,replace=TRUE)
  x2     <- rlnorm(length(sample_x), meanlog=fit$estimate[1], sdlog=fit$estimate[2])
  CV[s,] <- c(sd(x1)/mean(x1),sd(x2)/mean(x2))
  }

```

</div>

Figure \@ref(fig:Fig812) compares the bootstrap distributions for the coefficient of variation, one based on the nonparametric approach and the other based on a parametric approach, assuming a lognormal distribution.

(ref:Fig812) **Comparison of Nonparametric and Parametric Bootstrap Distributions for the Coefficient of Variation**

```{r Fig812, fig.cap='(ref:Fig812)', echo=FALSE,  out.width='70%', fig.asp=.70}
plot(density(CV[,1]),col="red",main="",xlab="Coefficient of Variation", lty=1)
lines(density(CV[,2]),col="blue",lty=2)
abline(v=sd(sample_x)/mean(sample_x),lty=3)
legend("topright",c("nonparametric","parametric(LN)"),
       col=c("red","blue"),lty=1:2,bty="n")
```

***

**Example 8.3.5. Bootstrapping Censored Observations.** The parametric bootstrap draws simulated realizations from a parametric estimate of the distribution function. In the same way, we can draw simulated realizations from estimates of a distribution function. As one example, we might draw from smoothed estimates of a distribution function introduced in Section \@ref(S:MS:Density). Another special case, considered here, is to draw an estimate from the Kaplan-Meier estimator introduced in Section \@ref(S:Sec533). In this way, we can handle observations that are censored.

Specifically, return to the bodily injury data in Examples 8.2.1 and 8.2.3 but now we include the `r sum(BIData$Censored)` claims that were censored by policy limits. In Example 4.3.6, we used this full dataset to estimate the Kaplan-Meier estimator of the survival function introduced in Section \@ref(S:Sec533). [Table 8.7](#tab:8.7) presents bootstrap estimates of the quantiles from the Kaplan-Meier survival function estimator. These include the bootstrap precision estimates, bias and standard deviation, as well as the basic 95% confidence interval.


`r HideRCode('KMCode.1','Show R Code For Bootstrap Kaplan-Meier Estimates')`

```{r  echo=HtmlEval}
# Example from Derrig et al
library(survival)                # for Surv(), survfit()
library(Hmisc)                   # for bootkm()
BIData$UnCensored <- 1*(BIData$AmountPaid < BIData$PolicyLimit)
## KM estimate
KM0 <- survfit(Surv(AmountPaid, UnCensored) ~ 1,  
               type="kaplan-meier", data=BIData)

set.seed(2019)
PercentVec <- c(0.50, 0.80, 0.90, 0.95, 0.98)
OutBoot1   <- matrix(NA,5,6)
KM.survobj <- Surv(BIData$AmountPaid, BIData$UnCensored) 
for (i in 1:length(PercentVec)) {
  OutBoot1[i,1] <- PercentVec[i]
  results <- bootkm(KM.survobj, q=1-PercentVec[i], B=1000, pr = FALSE)
  if (i==1){bootreal <- results}
  OutBoot1[i,2] <- quantile(KM0, PercentVec[i])$quantile
  OutBoot1[i,3] <- mean(results)-OutBoot1[i,2]
  OutBoot1[i,4] <- sd(results) 
  # temp <- boot.ci(results, type = c("norm",  "basic","perc"))
  OutBoot1[i,5] <- 2*OutBoot1[i,2]-quantile(results,.975, type=6)
  OutBoot1[i,6] <- 2*OutBoot1[i,2]-quantile(results,.025, type=6)
  }

```

</div>

\newpage

[Table 8.7]: ./ChapSimulation#tab:8.7

<a id=tab:8.7></a>

[Table 8.7]{#tab:8.7}. **Bootstrap Kaplan-Meier Estimates of Quantiles at Selected Fractions**

```{r  echo=FALSE}
OutBoot1.latex <- OutBoot1
colnames(OutBoot1) <- c("Fraction","KM NP Estimate", "Bootstrap Bias",
                        "Bootstrap SD",  "Lower Basic 95% CI", "Upper Basic 95% CI")
if (HtmlEval) {knitr::kable(OutBoot1, "html",digits=2)  }
if (PdfEval) {kbl(OutBoot1.latex, booktabs = T, digits=2) %>%
  kable_styling(latex_options="scale_down") %>%
        add_header_above(c("","Estimate"=1, "Bias"=1,
                      "SD"=1,  "95% CI"=1, "95% CI"=1)) %>%
    add_header_above(c("Fraction"=1,"KM NP"=1, "Bootstrap "=1,
                      "Bootstrap"=1,  "Lower Basic"=1, "Upper Basic"=1)) %>%
     kable_styling(latex_options = "hold_position", font = 10) 
    }
```



Results in [Table 8.7](#tab:8.7) are consistent with the results for the uncensored subsample in [Table 8.5](#tab:8.5). In [Table 8.7](#tab:8.7), we note the difficulty in estimating quantiles at large fractions due to the censoring. However, for moderate size fractions (0.50, 0.80, and 0.90), the Kaplan-Meier nonparametric (KM NP) estimates of the quantile are consistent with those [Table 8.5](#tab:8.5). The bootstrap standard deviation is smaller at the 0.50 (corresponding to the median) but larger at the 0.80 and 0.90 levels. The censored data analysis summarized in [Table 8.7](#tab:8.7) uses more data than the uncensored subsample analysis in [Table 8.5](#tab:8.5) but also has difficulty extracting information for large quantiles.


## Model Selection and Cross-Validation {#S:Sec84}

***

In this section, you learn how to:

-  Compare and contrast cross-validation to simulation techniques and bootstrap methods.
-  Use cross-validation techniques for model selection
-  Explain the jackknife method as a special case of cross-validation and calculate jackknife estimates of bias and standard errors

***

Cross-validation, briefly introduced in Chapter \@ref(ChapDataAnalytics) and Section \@ref(S:Sec65), is a technique based on simulated outcomes that is especially useful for selecting an appropriate model.  We now compare and contrast cross-validation to other simulation techniques already introduced in this chapter.

- Simulation, or Monte-Carlo, introduced in Section \@ref(S:Sec81), allows us to compute expected values and other summaries of statistical distributions, such as $p$-values, readily.
- Bootstrap, and other resampling methods introduced in Section \@ref(S:Sec83), provides estimators of the precision, or variability, of statistics.
- Cross-validation is important when assessing how accurately a predictive model will perform in practice.

Overlap exists but nonetheless it is helpful to think about the broad goals associated with each statistical method.

To discuss cross-validation, let us recall from Chapter \@ref(ChapDataAnalytics) some of the key ideas of model validation. When assessing, or validating, a model, we look to  performance measured on *new* data, or at least not those that were used to fit the model. A classical approach is to split the sample in two: a subpart (the *training* dataset) is used to fit the model and the other one (the *testing* dataset) is used to validate. However, a limitation of this approach is that results depend on the split; even though the overall sample is fixed, the split between training and test subsamples varies randomly. A different training sample means that model estimated parameters will differ. Different model parameters and a different test sample means that validation statistics will differ. Two analysts may use the same data and same models yet reach different conclusions about the viability of a model (based on different random splits), a frustrating situation.

### k-Fold Cross-Validation {#S:Sec841}

To mitigate this difficulty, it is common to use a cross-validation approach as introduced in Section 4.2.4. The key idea is to emulate the basic test/training approach to model validation by repeating it many times through averaging over different splits of the data. A key advantage is that the validation statistic is not tied to a specific parametric (or nonparametric) model - one can use a nonparametric statistic or a statistic that has economic interpretations - and so this can be used to compare models that are not nested (unlike likelihood ratio procedures).

```{r echo = FALSE}
library(MASS) # for gamma.dispersion()
library(VGAM) # for vglm()
library(goftest)

## Read in data and get number of claims.  
claim_lev      <- read.csv("Data/CLAIMLEVEL.csv", header = TRUE) 
# 2010 subset 
claim_data     <- subset(claim_lev, Year == 2010); 

# Fit a Pareto distribution to the full dataset
fit.pareto     <- vglm(Claim ~ 1, paretoII, loc = 0, data = claim_data)
ksResultPareto <- ks.test( claim_data$Claim, "pparetoII", loc = 0, shape = exp(coef(fit.pareto)[2]), 
                          scale = exp(coef(fit.pareto)[1]) )
# Fit a gamma distribution to the full dataset
fit.gamma      <- glm(Claim ~ 1, data = claim_data, family = Gamma(link = log)) 
gamma_theta    <- exp(coef(fit.gamma)) * gamma.dispersion(fit.gamma) 
alpha          <- 1 / gamma.dispersion(fit.gamma)
ksResultGamma  <- ks.test(claim_data$Claim, "pgamma", shape = alpha, scale = gamma_theta)

```

**Example 8.4.1. Wisconsin Property Fund.** For the 2010 property fund data introduced in Section \@ref(S:LGPIF), we fit gamma and Pareto distributions to the 1,377 claims data. For details of the related goodness of fit, see Appendix Section 15.4.4. We now consider the Kolmogorov-Smirnov statistic introduced in Section \@ref(S:Sec612). When the entire dataset was fit, the Kolmogorov-Smirnov goodness of fit statistic for the gamma distribution turns out to be `r round(ksResultGamma$statistic,digits=4)` and for the Pareto distribution is `r round(ksResultPareto$statistic,digits=4)`. The lower value for the Pareto distribution indicates that this distribution is a better fit than the gamma.

To see how `r Gloss('k-fold cross-validation')` works, we randomly split the data into $k=8$ groups, or folds, each having about $1377/8 \approx 172$ observations. Then, we fit gamma and Pareto models to a data set with the first seven folds (about $172 \times 7 = 1,204$ observations), determine estimated parameters, and then used these fitted models with the held-out data to determine the Kolmogorov-Smirnov statistic. 

`r HideRCode('KFoldCV.1','Show R Code for Kolmogorov-Smirnov Cross-Validation')`

```{r  echo=HtmlEval}
# Randomly re-order the data - "shuffle it"
n      <- nrow(claim_data)
set.seed(12347)
cvdata <- claim_data[sample(n), ]
# Number of folds
k      <- 8
cvalvec <- matrix(0,2,k)
for (i in 1:k) {
  indices <- (((i-1) * round((1/k)*nrow(cvdata))) + 1):((i*round((1/k) * nrow(cvdata))))
# Pareto
  fit.pareto    <- vglm(Claim ~ 1, paretoII, loc = 0, data = cvdata[-indices,])
  ksResultPareto <- ks.test(cvdata[indices,]$Claim, "pparetoII", loc = 0, 
                            shape = exp(coef(fit.pareto)[2]), 
                            scale = exp(coef(fit.pareto)[1]) )
  cvalvec[1,i]  <- ksResultPareto$statistic
# Gamma
  fit.gamma     <- glm(Claim ~ 1, data = cvdata[-indices,], family = Gamma(link = log)) 
  gamma_theta   <- exp(coef(fit.gamma)) * gamma.dispersion(fit.gamma)  
  alpha         <- 1 / gamma.dispersion(fit.gamma)
  ksResultGamma <- ks.test(cvdata[indices,]$Claim, "pgamma", shape = alpha, scale = gamma_theta)
  cvalvec[2,i]  <- ksResultGamma$statistic
  }
KScv            <- rowSums(cvalvec)/k

```

</div>

The results appear in Figure \@ref(fig:Fig813) where horizontal axis is Fold=1. This process was repeated for the other seven folds. The results summarized in Figure \@ref(fig:Fig813) show that the Pareto consistently provides a more reliable predictive distribution than the gamma.

(ref:Fig813) **Cross Validated Kolmogorov-Smirnov (KS) Statistics for the Property Fund Claims Data.** The solid black line is for the Pareto distribution, the green dashed line is for the gamma distribution. The KS statistic measures the largest deviation between the fitted distribution and the empirical distribution for each of 8 groups, or folds, of randomly selected data.

```{r Fig813,  fig.cap='(ref:Fig813)', echo=FALSE,  out.width='70%', fig.asp=.70}
# Plot the statistics
matplot(1:k,t(cvalvec),type="b", col=c(1,3), lty=1:2, 
        ylim=c(0,0.4), pch = 0, xlab="Fold", ylab="KS Statistic")
legend("left", c("Pareto", "Gamma"), col=c(1,3),lty=1:2, bty="n")
```

### Leave-One-Out Cross-Validation {#S:Sec842}

A special case where $k=n$ is known as `r Gloss('leave-one-out cross validation')`. This case is historically prominent and is closely related to `r Gloss('jackknife statistics')`, a precursor of the bootstrap technique. 

Even though we present it as a special case of cross-validation, it is helpful to given an explicit definition. Consider a generic statistic $\widehat{\theta}=t(\boldsymbol{x})$ that is an estimator for a parameter of interest $\theta$. The idea of the jackknife is to compute $n$ values $\widehat{\theta}_{-i}=t(\boldsymbol{x}_{-i})$, where $\boldsymbol{x}_{-i}$ is the subsample of $\boldsymbol{x}$ with the $i$-th value removed. The average of these values is denoted as

$$
\overline{\widehat{\theta}}_{(\cdot)}=\frac{1}{n}\sum_{i=1}^n \widehat{\theta}_{-i} .
$$

These values can be used to create estimates of the bias of the statistic $\widehat{\theta}$
\begin{equation}
Bias_{jack} = (n-1) \left(\overline{\widehat{\theta}}_{(\cdot)} - \widehat{\theta}\right)
(\#eq:Biasjack)
\end{equation}
as well as a standard deviation estimate
\begin{equation}
s_{jack} =\sqrt{\frac{n-1}{n}\sum_{i=1}^n \left(\widehat{\theta}_{-i} -\overline{\widehat{\theta}}_{(\cdot)}\right)^2} ~.
(\#eq:sdjack)
\end{equation}


**Example 8.4.2. Coefficient of Variation.** To illustrate, consider a small fictitious sample $\boldsymbol{x}=\{x_1,\ldots,x_n\}$ with realizations

`r CodeFontSmall()`

```{r}
sample_x <- c(2.46,2.80,3.28,3.86,2.85,3.67,3.37,3.40,
              5.22,2.55,2.79,4.50,3.37,2.88,1.44,2.56,2.00,2.07,2.19,1.77)
```

`r CodeFontLarge()`

Suppose that we are interested in the `r Gloss('coefficient of variation')`
$\theta = CV$ $= \sqrt{\mathrm{Var~}[X]}/\mathrm{E~}[X]$.


```{r Example842, echo = FALSE}
#  Sample Code for Example 8.4.2
CVar      <- function(x) sqrt(var(x))/mean(x)
JackCVar  <- function(i) sqrt(var(sample_x[-i]))/mean(sample_x[-i])
JackTheta <- Vectorize(JackCVar)(1:length(sample_x))
BiasJack  <- (length(sample_x)-1)*(mean(JackTheta) - CVar(sample_x))
sdJack    <- sd(JackTheta)
```

With this dataset, the estimator of the coefficient of variation turns out to be `r round(CVar(sample_x),digits = 5)`. But how reliable is it? To answer this question, we can compute the jackknife estimates of bias and its standard deviation. The following code shows that the jackknife estimator of the bias is $Bias_{jack} =$ `r round(BiasJack,digits = 5)` and the jackknife standard deviation is $s_{jack} =$ `r round(sdJack,digits = 5)`.


`r HideRCode('Example842.Hid','Show R Code for Example 8.4.2')`

`r CodeFontSmall()`

```{r ref.label = 'Example842', eval = FALSE}
```

`r CodeFontLarge()`

</div>

***


**Example 8.4.3. Bodily Injury Claims and Loss Elimination Ratios.** In [Example 8.3.1](#Ex:8.3.1), we showed how to compute bootstrap estimates of the bias and standard deviation for the loss elimination ratio using the Example 4.1.11 bodily injury claims data. We follow up now by providing comparable quantities using jackknife statistics.

[Table 8.8](#tab:8.8) summarizes the results of the jackknife estimation. It shows that jackknife estimates of the bias and standard deviation of the loss elimination ratio $\mathrm{E}~[\min(X,d)]/\mathrm{E}~[X]$ are largely consistent with the bootstrap methodology. Moreover, one can use the standard deviations to construct normal based confidence intervals, centered around a bias-corrected estimator. For example, at $d=14000$, we saw in Example 4.1.11 that the nonparametric estimate of *LER* is 0.97678. This has an estimated bias of 0.00010, resulting in the (jackknife) *bias-corrected* estimator 0.97688. The 95% confidence intervals are produced by creating an interval of twice the length of 1.96 jackknife standard deviations, centered about the bias-corrected estimator (1.96 is the approximate 97.5th quantile of the standard normal distribution). 


`r HideRCode('Jackknife.1','Show the R Code')`

```{r   echo=HtmlEval}
# Example from Derrig et al
BIData          <- read.csv("Data/DerrigResampling.csv", header =T)
BIData$Censored <- 1*(BIData$AmountPaid >= BIData$PolicyLimit)
BIDataUncensored <- subset(BIData, Censored == 0)
LER.boot <- function(ded, data, indices){
  resample.data <- data[indices,]
  sumClaims     <- sum(resample.data$AmountPaid)
  sumClaims_d   <- sum(pmin(resample.data$AmountPaid,ded))
  LER           <- sumClaims_d/sumClaims
  return(LER)  
}

x <- BIDataUncensored$AmountPaid
LER.jack <- function(ded,i){
  LER    <- sum(pmin(x[-i],ded))/sum(x[-i])
  return(LER)  
  }
LER <- function(ded) { sum(pmin(x,ded))/sum(x) }
##Derrig et al
set.seed(2019)
dVec2 <- c(4000, 5000, 10500, 11500, 14000, 18500)
OutJack <- matrix(0,length(dVec2),8)
  for (j in 1:length(dVec2)) {
OutJack[j,1] <- dVec2[j]
results <- boot(data=BIDataUncensored, statistic=LER.boot, R=1000, ded=dVec2[j])
OutJack[j,2] <- results$t0
biasboot <- mean(results$t)-results$t0 -> OutJack[j,3]
sdboot <- sd(results$t) -> OutJack[j,4]
temp <- boot.ci(results, type = "norm")

LER.jack.ded<- function(i) LER.jack(ded=dVec2[j],i)
JackTheta.ded <- Vectorize(LER.jack.ded)(1:length(x))
OutJack[j,5] <- BiasJack.ded <- (length(x)-1)*(mean(JackTheta.ded) - LER(ded=dVec2[j]))
OutJack[j,6] <- sd(JackTheta.ded)
OutJack[j,7:8] <- mean(JackTheta.ded)+qt(c(0.025,0.975),length(x)-1)*OutJack[j,6]
}

```

</div>


[Table 8.8]: ./ChapSimulation#tab:8.8

<a id=tab:8.8></a>

[Table 8.8]{#tab:8.8}. **Jackknife Estimates of LER at Selected Deductibles**


```{r  echo=FALSE}
OutJack.latex <- OutJack
colnames(OutJack) <- c("d","NP Estimate","Bootstrap Bias", "Bootstrap SD", 
                       "Jackknife Bias", "Jackknife SD","Lower Jackknife 95% CI",
                       "Upper Jackknife 95% CI")
if (HtmlEval) {knitr::kable(OutJack, "html",digits=5) }
if (PdfEval) {kbl(OutJack.latex,  booktabs = T, digits=5) %>%
  kable_styling(latex_options="scale_down") %>%
    add_header_above(c("","Estimate"=1,"Bias"=1, "SD"=1, 
                       "Bias"=1, "SD"=1,"95% CI"=1, "95% CI"=1)) %>%
      add_header_above(c("d"=1,"NP"=1,"Bootstrap"=1, "Bootstrap"=1, 
                       "Jackknife"=1, "Jackknife"=1,"Lower Jackknife"=1, "Upper Jackknife"=1))  %>%
     kable_styling(latex_options = "hold_position") 
}
```

`r Refer()`


***

**Discussion.** One of the many interesting things about the leave-one-out special case is the ability to replicate estimates exactly. That is, when the size of the fold is only one, then there is no additional uncertainty induced by the cross-validation. This means that analysts can exactly replicate work of one another, an important consideration.

Jackknife statistics were developed to understand precision of estimators, producing estimators of bias and standard deviation in equations \@ref(eq:Biasjack) and \@ref(eq:sdjack). This crosses into goals that we have associated with bootstrap techniques, not cross-validation methods. This demonstrates how statistical techniques can be used to achieve different goals.


### Cross-Validation and Bootstrap {#S:Sec843}

The bootstrap is useful in providing estimators of the precision, or variability, of statistics. It can also be useful for model validation. The bootstrap approach to model validation is similar to the leave-one-out and *k*-fold validation procedures:

- Create a bootstrap sample by re-sampling (with replacement) $n$ indices in $\{1,\cdots,n\}$. That will be our *training sample*. Estimate the model under consideration based on this sample.
- The *test*, or *validation sample*, consists of those observations not selected for training. Evaluate the fitted model (based on the training data) using the test data.

Repeat this process many (say $B$) times. Take an average over the results and choose the model based on the average evaluation statistic.


**Example 8.4.4. Wisconsin Property Fund.**  Return to [Example 8.3.1](#Ex:8.3.1) where we investigate the fit of the gamma and Pareto distributions on the property fund data. We again compare the predictive performance using the Kolmogorov-Smirnov (*KS*) statistic but this time using the bootstrap procedure to split the data between training and testing samples. The following provides illustrative code.

`r HideRCode('BootstrapValidation.1','Show R Code for Bootstrapping Validation Procedures')`

```{r  echo=HtmlEval}

n <- nrow(claim_data)
set.seed(12347)
indices <- 1:n
# Number of Bootstrap Samples
B <- 100
cvalvec <- matrix(0,2,B)
for (i in 1:B) {
  bootindex <- unique(sample(indices, size=n, replace= TRUE))
  traindata <- claim_data[bootindex,]
  testdata  <- claim_data[-bootindex,]
# Pareto
  fit.pareto <- vglm(Claim ~ 1, paretoII, loc = 0, data = traindata)
  ksResultPareto <- ks.test(testdata$Claim, "pparetoII", loc = 0, shape = exp(coef(fit.pareto)[2]), 
        scale = exp(coef(fit.pareto)[1]))
  cvalvec[1,i] <- ksResultPareto$statistic
# Gamma
  fit.gamma <- glm(Claim ~ 1, data = traindata, family = Gamma(link = log)) 
  gamma_theta <- exp(coef(fit.gamma)) * gamma.dispersion(fit.gamma)  
  alpha <- 1 / gamma.dispersion(fit.gamma)
  ksResultGamma <- ks.test(testdata$Claim, "pgamma", shape = alpha, scale = gamma_theta)
  cvalvec[2,i] <- ksResultGamma$statistic
}
KSBoot <- rowSums(cvalvec)/B

```

</div>

We did the sampling using $B=$ `r B` replications. The average *KS* statistic for the Pareto distribution was `r round (KSBoot[1], digits = 3)` compared to the average for the gamma distribution, `r round (KSBoot[2], digits = 3)`. This is consistent with earlier results and provides another piece of evidence that the Pareto is a better model for these data than the gamma.

`r Refer()`


## Further Resources and Contributors {#S:Sec85}

Section \@ref(S:Sec842) presented the jackknife statistic as an application of (leave one out) cross-validation methods. Another way to present this material is to consider the historical development. @efron1982jackknife attributes the jackknife idea to @quenouille1949approximate. Even in this simpler time before modern computing power became widely available, the jackknife provided a handy tool to estimate the bias and standard deviation for virtually any statistic. In addition, this provided motivation for the 1979 introduction of the bootstrap in @efron1979bootstrap (see also @Efron1992). The bootstrap provided a tool to understand the uncertainty of a statistic, including the standard deviation.

The presentation in this book, outlined in Chapter \@ref(ChapDataAnalytics), follows strategies adopted by analysts. We think of the jackknife and the bootstrap as tools that helps one understand qualities of a statistic of interest. In addition, cross-validation is a resampling strategy primarily devoted to model validation. As noted in @efron1982jackknife, the historical development of cross-validation is a bit murkier. It is a method borne from the very simple strategy of splitting a sample in half, then using a model trained on one half to predict performance in the other half. Comparing cross-validation methods to the jackknifing and bootstrapping techniques, all are based on resampling. In addition, questions of statistical inference naturally overlap with model validation issues, so there is a natural overlap among these methods.


-  For further reading, a classic, and still very readable, introduction to the jackknife and bootstrap is provided by @efron1982jackknife.
-  Here are some links to learn more about [reproducibility and randomness](https://freakonometrics.hypotheses.org/6470) and how to go
[from a random generator to a sample function](https://freakonometrics.hypotheses.org/6638).


#### Contributors {-}

- **Arthur Charpentier**, Universit&eacute; du Quebec &aacute; Montreal, and **Edward (Jed) Frees**, University of Wisconsin-Madison, are the principal authors of the initial version of this chapter. 
   - Chapter reviewers include Yvonne Chueh and Brian Hartman.
-  **Edward (Jed) Frees**, University of Wisconsin-Madison and Australian National University, is the author of the second edition of this chapter. Email: jfrees@bus.wisc.edu for chapter comments and suggested improvements.
   -  This chapter has benefited significantly from suggestions by Hirokazu (Iwahiro) Iwasawa.





