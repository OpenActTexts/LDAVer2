<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Bayesian Statistics and Modeling | Loss Data Analytics   Second Edition</title>
  <meta name="description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Bayesian Statistics and Modeling | Loss Data Analytics   Second Edition" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="github-repo" content="https://github.com/OpenActTexts/LDAVer2" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Bayesian Statistics and Modeling | Loss Data Analytics   Second Edition" />
  
  <meta name="twitter:description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  

<meta name="author" content="An open text authored by the Actuarial Community" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ChapSimulation.html"/>
<link rel="next" href="ChapPremiumFoundations.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YKJD3496MG"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-YKJD3496MG');
</script>


<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>


<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<script>
function markdownConverterEWF() {  
//Create showdown markdown converter
var converter = new showdown.Converter();
converter.setOption('ghCompatibleHeaderId', true);
survey
    .onTextMarkdown
    .add(function (survey, options) {
        //convert the markdown text to html
        var str = converter.makeHtml(options.text);
        //remove root paragraphs <p></p>
        str = str.substring(3);
        str = str.substring(0, str.length - 4);
        //set html
        options.html = str;
        MathJax.Hub.Queue(['Typeset',MathJax.Hub, 'options']);
    });  
};

// Quiz Header info
const jsonHeader = { 
    showProgressBar: "bottom",
    showTimerPanel: "none",
    maxTimeToFinishPage: 10000,
    maxTimeToFinish: 25000,
    firstPageIsStarted: true,
    startSurveyText: "Start Quiz" //,
//    title: "Does This Make Sense?"
}


// One and Two question quizzes
function jsonSummary1EWF(json) {  
let jsonEnd1 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
};  
return jsonEnd1;
};


function jsonSummary2EWF(json) {  
let jsonEnd2 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
};  
return jsonEnd2;
};

// Three, four, and five question quizzes
function jsonSummary3EWF(json) {  
let jsonEnd3 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][3]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][3]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][3]["questions"][0]["correctAnswer"]
};  
return jsonEnd3;
};

function jsonSummary4EWF(json) {  
jsonEnd4 = jsonSummary3EWF(json);
jsonEnd4.completedHtml = jsonEnd4.completedHtml +  
"<br>"+
json["pages"][4]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][4]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][4]["questions"][0]["correctAnswer"]
;  
return jsonEnd4;
};

function jsonSummary5EWF(json) {  
jsonEnd5 = jsonSummary4EWF(json);
jsonEnd5.completedHtml = jsonEnd5.completedHtml +  
"<br>"+
json["pages"][5]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][5]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][5]["questions"][0]["correctAnswer"]
;  
return jsonEnd5;
};

function jsonSummary6EWF(json) {  
jsonEnd6 = jsonSummary5EWF(json);
jsonEnd6.completedHtml = jsonEnd6.completedHtml +  
"<br>"+
json["pages"][6]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][6]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][6]["questions"][0]["correctAnswer"]
;  
return jsonEnd6;
};

function jsonSummary7EWF(json) {  
jsonEnd7 = jsonSummary6EWF(json);
jsonEnd7.completedHtml = jsonEnd7.completedHtml +  
"<br>"+
json["pages"][7]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][7]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][7]["questions"][0]["correctAnswer"]
;  
return jsonEnd7;
};

function jsonSummary8EWF(json) {  
jsonEnd8 = jsonSummary7EWF(json);
jsonEnd8.completedHtml = jsonEnd8.completedHtml +  
"<br>"+
json["pages"][8]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][8]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][8]["questions"][0]["correctAnswer"]
;  
return jsonEnd8;
};

function jsonSummary9EWF(json) {  
jsonEnd9 = jsonSummary8EWF(json);
jsonEnd9.completedHtml = jsonEnd9.completedHtml +  
"<br>"+
json["pages"][9]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][9]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][9]["questions"][0]["correctAnswer"]
;  
return jsonEnd9;
};


function jsonSummary10EWF(json) {  
jsonEnd10 = jsonSummary9EWF(json);
jsonEnd10.completedHtml = jsonEnd10.completedHtml +  
"<br>"+
json["pages"][10]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][10]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][10]["questions"][0]["correctAnswer"]
;  
return jsonEnd10;
};


function jsonSummary11EWF(json) {  
jsonEnd11 = jsonSummary10EWF(json);
jsonEnd11.completedHtml = jsonEnd11.completedHtml +  
"<br>"+
json["pages"][11]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][11]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][11]["questions"][0]["correctAnswer"]
;  
return jsonEnd11;
};

Survey.StylesManager.applyTheme("modern");

</script>  
<!-- This completes the code for the quizzes -->

<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>

<script>

/* update total correct if #webex-total_correct exists */
update_total_correct = function() {
  console.log("webex: update total_correct");

  if (t = document.getElementById("webex-total_correct")) {
    var correct = document.getElementsByClassName("webex-correct").length;
    var solvemes = document.getElementsByClassName("webex-solveme").length;
    var radiogroups = document.getElementsByClassName("webex-radiogroup").length;
    var selects = document.getElementsByClassName("webex-select").length;
    
    t.innerHTML = correct + " of " + (solvemes + radiogroups + selects) + " correct";
  }
}

/* webex-solution button toggling function */
b_func = function() {
  console.log("webex: toggle hide");
  
  var cl = this.parentElement.classList;
  if (cl.contains('open')) {
    cl.remove("open");
  } else {
    cl.add("open");
  }
}

/* function for checking solveme answers */
solveme_func = function(e) {
  console.log("webex: check solveme");

  var real_answers = JSON.parse(this.dataset.answer);
  var my_answer = this.value;
  var cl = this.classList;
  if (cl.contains("ignorecase")) {
    my_answer = my_answer.toLowerCase();
  }
  if (cl.contains("nospaces")) {
    my_answer = my_answer.replace(/ /g, "")
  }

  if (my_answer == "") {
    cl.remove("webex-correct");
    cl.remove("webex-incorrect");
  } else if (real_answers.includes(my_answer)) {
    cl.add("webex-correct");
    cl.remove("webex-incorrect");
  } else {
    cl.add("webex-incorrect");
    cl.remove("webex-correct");
  }

  // match numeric answers within a specified tolerance
  if(this.dataset.tol > 0){
    var tol = JSON.parse(this.dataset.tol);
    var matches = real_answers.map(x => Math.abs(x - my_answer) < tol)
    if (matches.reduce((a, b) => a + b, 0) > 0) {
      cl.add("webex-correct");
    } else {
      cl.remove("webex-correct");
    }
  }

  // added regex bit
  if (cl.contains("regex")){
    answer_regex = RegExp(real_answers.join("|"))
    if (answer_regex.test(my_answer)) {
      cl.add("webex-correct");
    }
  }

  update_total_correct();
}

/* function for checking select answers */
select_func = function(e) {
  console.log("webex: check select");
  
  var cl = this.classList
  
  /* add style */
  cl.remove("webex-incorrect");
  cl.remove("webex-correct");
  if (this.value == "answer") {
    cl.add("webex-correct");
  } else if (this.value != "blank") {
    cl.add("webex-incorrect");
  }
  
  update_total_correct();
}

/* function for checking radiogroups answers */
radiogroups_func = function(e) {
  console.log("webex: check radiogroups");

  var checked_button = document.querySelector('input[name=' + this.id + ']:checked');
  var cl = checked_button.parentElement.classList;
  var labels = checked_button.parentElement.parentElement.children;
  
  /* get rid of styles */
  for (i = 0; i < labels.length; i++) {
    labels[i].classList.remove("webex-incorrect");
    labels[i].classList.remove("webex-correct");
  }
  
  /* add style */
  if (checked_button.value == "answer") {
    cl.add("webex-correct");
  } else {
    cl.add("webex-incorrect");
  }
  
  update_total_correct();
}

window.onload = function() {
  console.log("onload");
  /* set up solution buttons */
  var buttons = document.getElementsByTagName("button");

  for (var i = 0; i < buttons.length; i++) {
    if (buttons[i].parentElement.classList.contains('webex-solution')) {
      buttons[i].onclick = b_func;
    }
  }

  /* set up webex-solveme inputs */
  var solveme = document.getElementsByClassName("webex-solveme");

  for (var i = 0; i < solveme.length; i++) {
    /* make sure input boxes don't auto-anything */
    solveme[i].setAttribute("autocomplete","off");
    solveme[i].setAttribute("autocorrect", "off");
    solveme[i].setAttribute("autocapitalize", "off");
    solveme[i].setAttribute("spellcheck", "false");
    solveme[i].value = "";

    /* adjust answer for ignorecase or nospaces */
    var cl = solveme[i].classList;
    var real_answer = solveme[i].dataset.answer;
    if (cl.contains("ignorecase")) {
      real_answer = real_answer.toLowerCase();
    }
    if (cl.contains("nospaces")) {
      real_answer = real_answer.replace(/ /g, "");
    }
    solveme[i].dataset.answer = real_answer;

    /* attach checking function */
    solveme[i].onkeyup = solveme_func;
    solveme[i].onchange = solveme_func;
  }
  
  /* set up radiogroups */
  var radiogroups = document.getElementsByClassName("webex-radiogroup");
  for (var i = 0; i < radiogroups.length; i++) {
    radiogroups[i].onchange = radiogroups_func;
  }
  
  /* set up selects */
  var selects = document.getElementsByClassName("webex-select");
  for (var i = 0; i < selects.length; i++) {
    selects[i].onchange = select_func;
  }

  update_total_correct();
}

</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="Format/style.css" type="text/css" />
<link rel="stylesheet" href="includeWebex/webex.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Loss Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i>Contributors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reviewers"><i class="fa fa-check"></i>Reviewers</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#other-collaborators"><i class="fa fa-check"></i>Other Collaborators</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#version-number"><i class="fa fa-check"></i>Version Number</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#for-our-readers"><i class="fa fa-check"></i>For our Readers</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ChapIntro.html"><a href="ChapIntro.html"><i class="fa fa-check"></i><b>1</b> Loss Data and Insurance Activities</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ChapIntro.html"><a href="ChapIntro.html#S:Intro"><i class="fa fa-check"></i><b>1.1</b> Data Driven Insurance Activities</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="ChapIntro.html"><a href="ChapIntro.html#nature-and-relevance-of-insurance"><i class="fa fa-check"></i><b>1.1.1</b> Nature and Relevance of Insurance</a></li>
<li class="chapter" data-level="1.1.2" data-path="ChapIntro.html"><a href="ChapIntro.html#S:DataDriven"><i class="fa fa-check"></i><b>1.1.2</b> Why Data Driven?</a></li>
<li class="chapter" data-level="1.1.3" data-path="ChapIntro.html"><a href="ChapIntro.html#S:InsProcesses"><i class="fa fa-check"></i><b>1.1.3</b> Insurance Processes</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="ChapIntro.html"><a href="ChapIntro.html#S:PredModApps"><i class="fa fa-check"></i><b>1.2</b> Insurance Company Operations</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ChapIntro.html"><a href="ChapIntro.html#initiating-insurance"><i class="fa fa-check"></i><b>1.2.1</b> Initiating Insurance</a></li>
<li class="chapter" data-level="1.2.2" data-path="ChapIntro.html"><a href="ChapIntro.html#renewing-insurance"><i class="fa fa-check"></i><b>1.2.2</b> Renewing Insurance</a></li>
<li class="chapter" data-level="1.2.3" data-path="ChapIntro.html"><a href="ChapIntro.html#claims-and-product-management"><i class="fa fa-check"></i><b>1.2.3</b> Claims and Product Management</a></li>
<li class="chapter" data-level="1.2.4" data-path="ChapIntro.html"><a href="ChapIntro.html#S:Reserving"><i class="fa fa-check"></i><b>1.2.4</b> Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ChapIntro.html"><a href="ChapIntro.html#S:LGPIF"><i class="fa fa-check"></i><b>1.3</b> Case Study: Wisconsin Property Fund</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="ChapIntro.html"><a href="ChapIntro.html#S:OutComes"><i class="fa fa-check"></i><b>1.3.1</b> Fund Claims Variables: Frequency and Severity</a></li>
<li class="chapter" data-level="1.3.2" data-path="ChapIntro.html"><a href="ChapIntro.html#S:FundVariables"><i class="fa fa-check"></i><b>1.3.2</b> Fund Rating Variables</a></li>
<li class="chapter" data-level="1.3.3" data-path="ChapIntro.html"><a href="ChapIntro.html#fund-operations"><i class="fa fa-check"></i><b>1.3.3</b> Fund Operations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ChapIntro.html"><a href="ChapIntro.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
<li class="chapter" data-level="1.5" data-path="ChapIntro.html"><a href="ChapIntro.html#Intro-further-reading-and-resources"><i class="fa fa-check"></i><b>1.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html"><i class="fa fa-check"></i><b>2</b> Introduction to Data Analytics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec21"><i class="fa fa-check"></i><b>2.1</b> Elements of Data Analytics</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#key-data-analytic-concepts"><i class="fa fa-check"></i><b>2.1.1</b> Key Data Analytic Concepts</a></li>
<li class="chapter" data-level="2.1.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec212"><i class="fa fa-check"></i><b>2.1.2</b> Data versus Algorithmic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec22"><i class="fa fa-check"></i><b>2.2</b> Data Analysis Process</a></li>
<li class="chapter" data-level="2.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec23"><i class="fa fa-check"></i><b>2.3</b> Single Variable Analytics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec231"><i class="fa fa-check"></i><b>2.3.1</b> Variable Types</a></li>
<li class="chapter" data-level="2.3.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec232"><i class="fa fa-check"></i><b>2.3.2</b> Exploratory versus Confirmatory</a></li>
<li class="chapter" data-level="2.3.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec233"><i class="fa fa-check"></i><b>2.3.3</b> Model Construction</a></li>
<li class="chapter" data-level="2.3.4" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec234"><i class="fa fa-check"></i><b>2.3.4</b> Model Selection</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec24"><i class="fa fa-check"></i><b>2.4</b> Analytics with Many Variables</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#supervised-and-unsupervised-learning"><i class="fa fa-check"></i><b>2.4.1</b> Supervised and Unsupervised Learning</a></li>
<li class="chapter" data-level="2.4.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#algorithmic-modeling"><i class="fa fa-check"></i><b>2.4.2</b> Algorithmic Modeling</a></li>
<li class="chapter" data-level="2.4.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-modeling"><i class="fa fa-check"></i><b>2.4.3</b> Data Modeling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec25"><i class="fa fa-check"></i><b>2.5</b> Data</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-types"><i class="fa fa-check"></i><b>2.5.1</b> Data Types</a></li>
<li class="chapter" data-level="2.5.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-structures-and-storage"><i class="fa fa-check"></i><b>2.5.2</b> Data Structures and Storage</a></li>
<li class="chapter" data-level="2.5.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-cleaning"><i class="fa fa-check"></i><b>2.5.3</b> Data Cleaning</a></li>
<li class="chapter" data-level="2.5.4" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec254"><i class="fa fa-check"></i><b>2.5.4</b> Big Data Analysis</a></li>
<li class="chapter" data-level="2.5.5" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#ethical-issues"><i class="fa fa-check"></i><b>2.5.5</b> Ethical Issues</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec26"><i class="fa fa-check"></i><b>2.6</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Sec261"><i class="fa fa-check"></i><b>2.6.1</b> Technical Supplement: Multivariate Exploratory Analysis</a></li>
<li class="chapter" data-level="2.6.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#tree-based-models"><i class="fa fa-check"></i><b>2.6.2</b> Tree-based Models</a></li>
<li class="chapter" data-level="2.6.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#technical-supplement-some-r-functions"><i class="fa fa-check"></i><b>2.6.3</b> Technical Supplement: Some R Functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html"><i class="fa fa-check"></i><b>3</b> Frequency Modeling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec31"><i class="fa fa-check"></i><b>3.1</b> Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec311"><i class="fa fa-check"></i><b>3.1.1</b> How Frequency Augments Severity Information</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec32"><i class="fa fa-check"></i><b>3.2</b> Basic Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec321"><i class="fa fa-check"></i><b>3.2.1</b> Foundations</a></li>
<li class="chapter" data-level="3.2.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec322"><i class="fa fa-check"></i><b>3.2.2</b> Moment and Probability Generating Functions</a></li>
<li class="chapter" data-level="3.2.3" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec323"><i class="fa fa-check"></i><b>3.2.3</b> Important Frequency Distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec33"><i class="fa fa-check"></i><b>3.3</b> The (<em>a</em>, <em>b</em>, 0) Class</a></li>
<li class="chapter" data-level="3.4" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec34"><i class="fa fa-check"></i><b>3.4</b> Estimating Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec341"><i class="fa fa-check"></i><b>3.4.1</b> Parameter Estimation</a></li>
<li class="chapter" data-level="3.4.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec342"><i class="fa fa-check"></i><b>3.4.2</b> Frequency Distributions MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec35"><i class="fa fa-check"></i><b>3.5</b> Other Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec351"><i class="fa fa-check"></i><b>3.5.1</b> Zero Truncation or Modification</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec36"><i class="fa fa-check"></i><b>3.6</b> Mixture Distributions</a></li>
<li class="chapter" data-level="3.7" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec37"><i class="fa fa-check"></i><b>3.7</b> Real Data Example</a></li>
<li class="chapter" data-level="3.8" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec38"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:Sec39"><i class="fa fa-check"></i><b>3.9</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:rcode"><i class="fa fa-check"></i><b>3.9.1</b> TS 3.A. R Code for Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ChapSeverity.html"><a href="ChapSeverity.html"><i class="fa fa-check"></i><b>4</b> Modeling Loss Severity</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec41"><i class="fa fa-check"></i><b>4.1</b> Basic Distributional Quantities</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec411"><i class="fa fa-check"></i><b>4.1.1</b> Moments and Moment Generating Functions</a></li>
<li class="chapter" data-level="4.1.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec412"><i class="fa fa-check"></i><b>4.1.2</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec42"><i class="fa fa-check"></i><b>4.2</b> Continuous Distributions for Modeling Loss Severity</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec421"><i class="fa fa-check"></i><b>4.2.1</b> Gamma Distribution</a></li>
<li class="chapter" data-level="4.2.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec422"><i class="fa fa-check"></i><b>4.2.2</b> Pareto Distribution</a></li>
<li class="chapter" data-level="4.2.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec423"><i class="fa fa-check"></i><b>4.2.3</b> Weibull Distribution</a></li>
<li class="chapter" data-level="4.2.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec424"><i class="fa fa-check"></i><b>4.2.4</b> The Generalized Beta Distribution of the Second Kind</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec43"><i class="fa fa-check"></i><b>4.3</b> Methods of Creating New Distributions</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec431"><i class="fa fa-check"></i><b>4.3.1</b> Functions of Random Variables and their Distributions</a></li>
<li class="chapter" data-level="4.3.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec432"><i class="fa fa-check"></i><b>4.3.2</b> Mixture Distributions for Severity</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec44"><i class="fa fa-check"></i><b>4.4</b> Estimating Loss Distributions</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec441"><i class="fa fa-check"></i><b>4.4.1</b> Nonparametric Estimation</a></li>
<li class="chapter" data-level="4.4.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec442"><i class="fa fa-check"></i><b>4.4.2</b> Parametric Estimation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec45"><i class="fa fa-check"></i><b>4.5</b> Exercises with a Practical Focus</a></li>
<li class="chapter" data-level="4.6" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Sec46"><i class="fa fa-check"></i><b>4.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html"><i class="fa fa-check"></i><b>5</b> Modeling Claim Severity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec51"><i class="fa fa-check"></i><b>5.1</b> Coverage Modifications</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec511"><i class="fa fa-check"></i><b>5.1.1</b> Policy Deductibles</a></li>
<li class="chapter" data-level="5.1.2" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec512"><i class="fa fa-check"></i><b>5.1.2</b> Policy Limits</a></li>
<li class="chapter" data-level="5.1.3" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec513"><i class="fa fa-check"></i><b>5.1.3</b> Coinsurance and Inflation</a></li>
<li class="chapter" data-level="5.1.4" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec514"><i class="fa fa-check"></i><b>5.1.4</b> Reinsurance</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec52"><i class="fa fa-check"></i><b>5.2</b> Parametric Estimation using Modified Data</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec521"><i class="fa fa-check"></i><b>5.2.1</b> Parametric Estimation using Grouped Data</a></li>
<li class="chapter" data-level="5.2.2" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec522"><i class="fa fa-check"></i><b>5.2.2</b> Censored Data</a></li>
<li class="chapter" data-level="5.2.3" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec523"><i class="fa fa-check"></i><b>5.2.3</b> Truncated Data</a></li>
<li class="chapter" data-level="5.2.4" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec524"><i class="fa fa-check"></i><b>5.2.4</b> Parametric Estimation using Censored and Truncated Data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec53"><i class="fa fa-check"></i><b>5.3</b> Nonparametric Estimation using Modified Data</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec531"><i class="fa fa-check"></i><b>5.3.1</b> Grouped Data</a></li>
<li class="chapter" data-level="5.3.2" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec532"><i class="fa fa-check"></i><b>5.3.2</b> Plug-in Principle</a></li>
<li class="chapter" data-level="5.3.3" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec533"><i class="fa fa-check"></i><b>5.3.3</b> Right-Censored Empirical Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Sec54"><i class="fa fa-check"></i><b>5.4</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html"><i class="fa fa-check"></i><b>6</b> Model Selection</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:Sec61"><i class="fa fa-check"></i><b>6.1</b> Tools for Model Selection and Diagnostics</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:Sec611"><i class="fa fa-check"></i><b>6.1.1</b> Graphical Comparison of Distributions</a></li>
<li class="chapter" data-level="6.1.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:Sec612"><i class="fa fa-check"></i><b>6.1.2</b> Statistical Comparison of Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:Sec62"><i class="fa fa-check"></i><b>6.2</b> Iterative Model Selection</a></li>
<li class="chapter" data-level="6.3" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:Sec63"><i class="fa fa-check"></i><b>6.3</b> Model Selection Based on a Training Dataset</a></li>
<li class="chapter" data-level="6.4" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:Sec64"><i class="fa fa-check"></i><b>6.4</b> Model Selection Based on a Test Dataset</a></li>
<li class="chapter" data-level="6.5" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:Sec65"><i class="fa fa-check"></i><b>6.5</b> Model Selection Based on Cross-Validation</a></li>
<li class="chapter" data-level="6.6" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:Sec66"><i class="fa fa-check"></i><b>6.6</b> Model Selection for Modified Data</a></li>
<li class="chapter" data-level="6.7" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#further-resources-and-contributors"><i class="fa fa-check"></i><b>6.7</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html"><i class="fa fa-check"></i><b>7</b> Aggregate Loss Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec71"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec72"><i class="fa fa-check"></i><b>7.2</b> Individual Risk Model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec721"><i class="fa fa-check"></i><b>7.2.1</b> Moments and Distribution</a></li>
<li class="chapter" data-level="7.2.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec722"><i class="fa fa-check"></i><b>7.2.2</b> Aggregate Loss Distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec73"><i class="fa fa-check"></i><b>7.3</b> Collective Risk Model</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec731"><i class="fa fa-check"></i><b>7.3.1</b> Moments and Distribution</a></li>
<li class="chapter" data-level="7.3.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec732"><i class="fa fa-check"></i><b>7.3.2</b> Stop-loss Insurance</a></li>
<li class="chapter" data-level="7.3.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec733"><i class="fa fa-check"></i><b>7.3.3</b> Closed-form Distributions</a></li>
<li class="chapter" data-level="7.3.4" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec734"><i class="fa fa-check"></i><b>7.3.4</b> Tweedie Distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec74"><i class="fa fa-check"></i><b>7.4</b> Computing the Aggregate Claims Distribution</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec741"><i class="fa fa-check"></i><b>7.4.1</b> Recursive Method</a></li>
<li class="chapter" data-level="7.4.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec742"><i class="fa fa-check"></i><b>7.4.2</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec75"><i class="fa fa-check"></i><b>7.5</b> Effects of Coverage Modifications</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec751"><i class="fa fa-check"></i><b>7.5.1</b> Impact of Exposure on Frequency</a></li>
<li class="chapter" data-level="7.5.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec752"><i class="fa fa-check"></i><b>7.5.2</b> Impact of Deductibles on Claim Frequency</a></li>
<li class="chapter" data-level="7.5.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec753"><i class="fa fa-check"></i><b>7.5.3</b> Impact of Policy Modifications on Aggregate Claims</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:Sec76"><i class="fa fa-check"></i><b>7.6</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#ts-7.a.1.-individual-risk-model-properties"><i class="fa fa-check"></i>TS 7.A.1. Individual Risk Model Properties</a></li>
<li class="chapter" data-level="" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#ts-7.a.2.-relationship-between-probability-generating-functions-of-x_i-and-x_it"><i class="fa fa-check"></i>TS 7.A.2. Relationship Between Probability Generating Functions of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_i^T\)</span></a></li>
<li class="chapter" data-level="" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#ts-7.a.3.-moment-generating-function-of-aggregate-loss-s_n-in-example-7.3.9"><i class="fa fa-check"></i>TS 7.A.3. Moment Generating Function of Aggregate Loss <span class="math inline">\(S_N\)</span> in Example 7.3.9</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ChapSimulation.html"><a href="ChapSimulation.html"><i class="fa fa-check"></i><b>8</b> Simulation and Resampling</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec81"><i class="fa fa-check"></i><b>8.1</b> Random Number Generation</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec811"><i class="fa fa-check"></i><b>8.1.1</b> Generating Independent Uniform Observations</a></li>
<li class="chapter" data-level="8.1.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec812"><i class="fa fa-check"></i><b>8.1.2</b> Inverse Transform Method</a></li>
<li class="chapter" data-level="8.1.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec813"><i class="fa fa-check"></i><b>8.1.3</b> Ready-made Random Number Generators</a></li>
<li class="chapter" data-level="8.1.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec814"><i class="fa fa-check"></i><b>8.1.4</b> Simulating from Complex Distributions</a></li>
<li class="chapter" data-level="8.1.5" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec815"><i class="fa fa-check"></i><b>8.1.5</b> Importance Sampling</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec82"><i class="fa fa-check"></i><b>8.2</b> Computing Distribution Parameters</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec821"><i class="fa fa-check"></i><b>8.2.1</b> Simulating Parameters</a></li>
<li class="chapter" data-level="8.2.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec822"><i class="fa fa-check"></i><b>8.2.2</b> Determining the Number of Simulations</a></li>
<li class="chapter" data-level="8.2.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec823"><i class="fa fa-check"></i><b>8.2.3</b> Simulation and Statistical Inference</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec83"><i class="fa fa-check"></i><b>8.3</b> Bootstrapping and Resampling</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec831"><i class="fa fa-check"></i><b>8.3.1</b> Bootstrap Foundations</a></li>
<li class="chapter" data-level="8.3.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec832"><i class="fa fa-check"></i><b>8.3.2</b> Bootstrap Precision: Bias, Standard Deviation, and Mean Square Error</a></li>
<li class="chapter" data-level="8.3.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec833"><i class="fa fa-check"></i><b>8.3.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="8.3.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec834"><i class="fa fa-check"></i><b>8.3.4</b> Parametric Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec84"><i class="fa fa-check"></i><b>8.4</b> Model Selection and Cross-Validation</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec841"><i class="fa fa-check"></i><b>8.4.1</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="8.4.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec842"><i class="fa fa-check"></i><b>8.4.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="8.4.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec843"><i class="fa fa-check"></i><b>8.4.3</b> Cross-Validation and Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sec85"><i class="fa fa-check"></i><b>8.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ChBayes.html"><a href="ChBayes.html"><i class="fa fa-check"></i><b>9</b> Bayesian Statistics and Modeling</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec91"><i class="fa fa-check"></i><b>9.1</b> A Gentle Introduction to Bayesian Statistics</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec911"><i class="fa fa-check"></i><b>9.1.1</b> Bayesian versus Frequentist Statistics</a></li>
<li class="chapter" data-level="9.1.2" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec912"><i class="fa fa-check"></i><b>9.1.2</b> A Brief History Lesson</a></li>
<li class="chapter" data-level="9.1.3" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec913"><i class="fa fa-check"></i><b>9.1.3</b> Bayes’ Rule</a></li>
<li class="chapter" data-level="9.1.4" data-path="ChBayes.html"><a href="ChBayes.html#an-introductory-example-of-bayes-rule"><i class="fa fa-check"></i><b>9.1.4</b> An Introductory Example of Bayes’ Rule</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec92"><i class="fa fa-check"></i><b>9.2</b> Building Blocks of Bayesian Statistics</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec921"><i class="fa fa-check"></i><b>9.2.1</b> Posterior Distribution</a></li>
<li class="chapter" data-level="9.2.2" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec922"><i class="fa fa-check"></i><b>9.2.2</b> Likelihood Function</a></li>
<li class="chapter" data-level="9.2.3" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec923"><i class="fa fa-check"></i><b>9.2.3</b> Prior Distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec93"><i class="fa fa-check"></i><b>9.3</b> Conjugate Families</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec931"><i class="fa fa-check"></i><b>9.3.1</b> The Beta–Binomial Conjugate Family</a></li>
<li class="chapter" data-level="9.3.2" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec932"><i class="fa fa-check"></i><b>9.3.2</b> The Gamma–Poisson Conjugate Family</a></li>
<li class="chapter" data-level="9.3.3" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec933"><i class="fa fa-check"></i><b>9.3.3</b> The Normal–Normal Conjugate Family</a></li>
<li class="chapter" data-level="9.3.4" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec934"><i class="fa fa-check"></i><b>9.3.4</b> Criticism of Conjugate Family Models</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec94"><i class="fa fa-check"></i><b>9.4</b> Posterior Simulation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec941"><i class="fa fa-check"></i><b>9.4.1</b> Introduction to Markov Chain Monte Carlo Methods</a></li>
<li class="chapter" data-level="9.4.2" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec942"><i class="fa fa-check"></i><b>9.4.2</b> The Gibbs Sampler</a></li>
<li class="chapter" data-level="9.4.3" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec943"><i class="fa fa-check"></i><b>9.4.3</b> The Metropolis–Hastings Algorithm</a></li>
<li class="chapter" data-level="9.4.4" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec944"><i class="fa fa-check"></i><b>9.4.4</b> Markov Chain Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec95"><i class="fa fa-check"></i><b>9.5</b> Bayesian Statistics in Practice</a></li>
<li class="chapter" data-level="9.6" data-path="ChBayes.html"><a href="ChBayes.html#S:Sec96"><i class="fa fa-check"></i><b>9.6</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChBayes.html"><a href="ChBayes.html#contributors-8"><i class="fa fa-check"></i>Contributors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html"><i class="fa fa-check"></i><b>10</b> Premium Foundations</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec101"><i class="fa fa-check"></i><b>10.1</b> Introduction to Ratemaking</a></li>
<li class="chapter" data-level="10.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec102"><i class="fa fa-check"></i><b>10.2</b> Data Sources</a></li>
<li class="chapter" data-level="10.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec103"><i class="fa fa-check"></i><b>10.3</b> Claims</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1031"><i class="fa fa-check"></i><b>10.3.1</b> Estimated Ultimate Claims</a></li>
<li class="chapter" data-level="10.3.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1032"><i class="fa fa-check"></i><b>10.3.2</b> Adjustments to Claims and Allocated Claims Adjustment Expenses</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec104"><i class="fa fa-check"></i><b>10.4</b> Exposures</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1041"><i class="fa fa-check"></i><b>10.4.1</b> Criteria for Choosing an Exposure</a></li>
<li class="chapter" data-level="10.4.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1042"><i class="fa fa-check"></i><b>10.4.2</b> Written and Earned Exposures</a></li>
<li class="chapter" data-level="10.4.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1043"><i class="fa fa-check"></i><b>10.4.3</b> Adjustments to Exposures</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec105"><i class="fa fa-check"></i><b>10.5</b> Pure Premiums</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1051"><i class="fa fa-check"></i><b>10.5.1</b> Experience Period</a></li>
<li class="chapter" data-level="10.5.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1052"><i class="fa fa-check"></i><b>10.5.2</b> Expected Pure Premium</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec106"><i class="fa fa-check"></i><b>10.6</b> Non-Claim Expenses</a></li>
<li class="chapter" data-level="10.7" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec107"><i class="fa fa-check"></i><b>10.7</b> Investment Income</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1071"><i class="fa fa-check"></i><b>10.7.1</b> Investment Income on Policyholder Cash Flows</a></li>
<li class="chapter" data-level="10.7.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1072"><i class="fa fa-check"></i><b>10.7.2</b> Investment Income on Surplus</a></li>
<li class="chapter" data-level="10.7.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1073"><i class="fa fa-check"></i><b>10.7.3</b> The Underwriting Profit Provisions</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec108"><i class="fa fa-check"></i><b>10.8</b> The Premium Equation</a></li>
<li class="chapter" data-level="10.9" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec109"><i class="fa fa-check"></i><b>10.9</b> Pricing Principles</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1091"><i class="fa fa-check"></i><b>10.9.1</b> Premium Principles</a></li>
<li class="chapter" data-level="10.9.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1092"><i class="fa fa-check"></i><b>10.9.2</b> Properties of Premium Principles</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1010"><i class="fa fa-check"></i><b>10.10</b> Reviewing Rate Adequacy</a>
<ul>
<li class="chapter" data-level="10.10.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec10101"><i class="fa fa-check"></i><b>10.10.1</b> The Loss Ratio Method</a></li>
<li class="chapter" data-level="10.10.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec10102"><i class="fa fa-check"></i><b>10.10.2</b> Target Loss Ratio</a></li>
<li class="chapter" data-level="10.10.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec10103"><i class="fa fa-check"></i><b>10.10.3</b> Experience Period Loss Ratios</a></li>
<li class="chapter" data-level="10.10.4" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec10104"><i class="fa fa-check"></i><b>10.10.4</b> Adjustments to Loss</a></li>
<li class="chapter" data-level="10.10.5" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec10105"><i class="fa fa-check"></i><b>10.10.5</b> Premium On-Level Adjustment</a></li>
<li class="chapter" data-level="10.10.6" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec10106"><i class="fa fa-check"></i><b>10.10.6</b> Premium Trend</a></li>
<li class="chapter" data-level="10.10.7" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec10107"><i class="fa fa-check"></i><b>10.10.7</b> Credibility</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:Sec1011"><i class="fa fa-check"></i><b>10.11</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#ts-10.a.-rate-regulation"><i class="fa fa-check"></i>TS 10.A. Rate Regulation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html"><i class="fa fa-check"></i><b>11</b> Risk Classification</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec111"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec112"><i class="fa fa-check"></i><b>11.2</b> Poisson Regression Model</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec1121"><i class="fa fa-check"></i><b>11.2.1</b> Need for Poisson Regression</a></li>
<li class="chapter" data-level="11.2.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec1122"><i class="fa fa-check"></i><b>11.2.2</b> Poisson Regression</a></li>
<li class="chapter" data-level="11.2.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec1123"><i class="fa fa-check"></i><b>11.2.3</b> Incorporating Exposure</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec113"><i class="fa fa-check"></i><b>11.3</b> Categorical Variables and Multiplicative Tariff</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec1131"><i class="fa fa-check"></i><b>11.3.1</b> Rating Factors and Tariff</a></li>
<li class="chapter" data-level="11.3.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec1132"><i class="fa fa-check"></i><b>11.3.2</b> Multiplicative Tariff Model</a></li>
<li class="chapter" data-level="11.3.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec1133"><i class="fa fa-check"></i><b>11.3.3</b> Poisson Regression for Multiplicative Tariff</a></li>
<li class="chapter" data-level="11.3.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec1134"><i class="fa fa-check"></i><b>11.3.4</b> Numerical Examples</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec114"><i class="fa fa-check"></i><b>11.4</b> Risk Classification vs Discrimination</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec1141"><i class="fa fa-check"></i><b>11.4.1</b> Economic Commodity versus Social Good</a></li>
<li class="chapter" data-level="11.4.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec1142"><i class="fa fa-check"></i><b>11.4.2</b> Information Asymmetry</a></li>
<li class="chapter" data-level="11.4.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec1143"><i class="fa fa-check"></i><b>11.4.3</b> Sensitive Variables and Regulation</a></li>
<li class="chapter" data-level="11.4.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec1144"><i class="fa fa-check"></i><b>11.4.4</b> Big Data Models and Proxy Discrimination</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec115"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
<li class="chapter" data-level="11.6" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:Sec116"><i class="fa fa-check"></i><b>11.6</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#ts-11.a.-estimating-poisson-regression-models"><i class="fa fa-check"></i>TS 11.A. Estimating Poisson Regression Models</a></li>
<li class="chapter" data-level="" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#ts-11.b.-selecting-rating-factors"><i class="fa fa-check"></i>TS 11.B. Selecting Rating Factors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ChapCredibility.html"><a href="ChapCredibility.html"><i class="fa fa-check"></i><b>12</b> Experience Rating Using Credibility Theory</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#introduction-to-applications-of-credibility-theory"><i class="fa fa-check"></i><b>12.1</b> Introduction to Applications of Credibility Theory</a></li>
<li class="chapter" data-level="12.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec122"><i class="fa fa-check"></i><b>12.2</b> Bühlmann Credibility</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec1221"><i class="fa fa-check"></i><b>12.2.1</b> Credibility-Weighted Estimate for the Expected Loss</a></li>
<li class="chapter" data-level="12.2.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec1222"><i class="fa fa-check"></i><b>12.2.2</b> Credibility <em>Z</em>, <em>EPV</em>, and <em>VHM</em></a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec123"><i class="fa fa-check"></i><b>12.3</b> Bayesian Inference and Bühlmann Credibility</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec1231"><i class="fa fa-check"></i><b>12.3.1</b> Beta-Binomial Model</a></li>
<li class="chapter" data-level="12.3.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec1232"><i class="fa fa-check"></i><b>12.3.2</b> Gamma-Poisson Model</a></li>
<li class="chapter" data-level="12.3.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec1233"><i class="fa fa-check"></i><b>12.3.3</b> Exact Credibility</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec124"><i class="fa fa-check"></i><b>12.4</b> Bühlmann-Straub Credibility</a></li>
<li class="chapter" data-level="12.5" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec125"><i class="fa fa-check"></i><b>12.5</b> Estimating Credibility Parameters</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#nonparametric-estimation-for-bühlmann-and-bühlmann-straub-models"><i class="fa fa-check"></i><b>12.5.1</b> Nonparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="12.5.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#semiparametric-estimation-for-bühlmann-and-bühlmann-straub-models"><i class="fa fa-check"></i><b>12.5.2</b> Semiparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec126"><i class="fa fa-check"></i><b>12.6</b> Limited Fluctuation Credibility</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec1261"><i class="fa fa-check"></i><b>12.6.1</b> Full Credibility for Claim Frequency</a></li>
<li class="chapter" data-level="12.6.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-for-aggregate-losses-and-pure-premium"><i class="fa fa-check"></i><b>12.6.2</b> Full Credibility for Aggregate Losses and Pure Premium</a></li>
<li class="chapter" data-level="12.6.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-for-severity"><i class="fa fa-check"></i><b>12.6.3</b> Full Credibility for Severity</a></li>
<li class="chapter" data-level="12.6.4" data-path="ChapCredibility.html"><a href="ChapCredibility.html#partial-credibility"><i class="fa fa-check"></i><b>12.6.4</b> Partial Credibility</a></li>
<li class="chapter" data-level="12.6.5" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-standard-for-limited-fluctuation-credibility"><i class="fa fa-check"></i><b>12.6.5</b> Full Credibility Standard for Limited Fluctuation Credibility</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec127"><i class="fa fa-check"></i><b>12.7</b> Balancing Credibility Estimators</a></li>
<li class="chapter" data-level="12.8" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Sec128"><i class="fa fa-check"></i><b>12.8</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html"><i class="fa fa-check"></i><b>13</b> Insurance Portfolio Management including Reinsurance</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#introduction-to-insurance-portfolios"><i class="fa fa-check"></i><b>13.1</b> Introduction to Insurance Portfolios</a></li>
<li class="chapter" data-level="13.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Sec132"><i class="fa fa-check"></i><b>13.2</b> Tails of Distributions</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#classification-based-on-moments"><i class="fa fa-check"></i><b>13.2.1</b> Classification Based on Moments</a></li>
<li class="chapter" data-level="13.2.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#comparison-based-on-limiting-tail-behavior"><i class="fa fa-check"></i><b>13.2.2</b> Comparison Based on Limiting Tail Behavior</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Sec133"><i class="fa fa-check"></i><b>13.3</b> Risk Measures</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#value-at-risk"><i class="fa fa-check"></i><b>13.3.1</b> Value-at-Risk</a></li>
<li class="chapter" data-level="13.3.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#expected-shortfall"><i class="fa fa-check"></i><b>13.3.2</b> Expected Shortfall</a></li>
<li class="chapter" data-level="13.3.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Sec1333"><i class="fa fa-check"></i><b>13.3.3</b> Coherent Risk Measures</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Sec134"><i class="fa fa-check"></i><b>13.4</b> Reinsurance</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Sec1341"><i class="fa fa-check"></i><b>13.4.1</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="13.4.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Sec1342"><i class="fa fa-check"></i><b>13.4.2</b> Non-Proportional Reinsurance</a></li>
<li class="chapter" data-level="13.4.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Sec1343"><i class="fa fa-check"></i><b>13.4.3</b> Additional Reinsurance Treaties</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#exercises-2"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
<li class="chapter" data-level="13.6" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Sec136"><i class="fa fa-check"></i><b>13.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html"><i class="fa fa-check"></i><b>14</b> Loss Reserving</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec141"><i class="fa fa-check"></i><b>14.1</b> Motivation</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec1412"><i class="fa fa-check"></i><b>14.1.1</b> Closed, IBNR, and RBNS Claims</a></li>
<li class="chapter" data-level="14.1.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec1411"><i class="fa fa-check"></i><b>14.1.2</b> Why Reserving?</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec142"><i class="fa fa-check"></i><b>14.2</b> Loss Reserve Data</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec1421"><i class="fa fa-check"></i><b>14.2.1</b> From Micro to Macro</a></li>
<li class="chapter" data-level="14.2.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec1422"><i class="fa fa-check"></i><b>14.2.2</b> Run-off Triangles</a></li>
<li class="chapter" data-level="14.2.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec1423"><i class="fa fa-check"></i><b>14.2.3</b> Loss Reserve Notation</a></li>
<li class="chapter" data-level="14.2.4" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec1424"><i class="fa fa-check"></i><b>14.2.4</b> R Code to Summarize Loss Reserve Data</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec143"><i class="fa fa-check"></i><b>14.3</b> The Chain-Ladder Method</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec1431"><i class="fa fa-check"></i><b>14.3.1</b> The Deterministic Chain-Ladder</a></li>
<li class="chapter" data-level="14.3.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec1432"><i class="fa fa-check"></i><b>14.3.2</b> Mack’s Distribution-Free Chain-Ladder Model</a></li>
<li class="chapter" data-level="14.3.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec1433"><i class="fa fa-check"></i><b>14.3.3</b> R code for Chain-Ladder Predictions</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec144"><i class="fa fa-check"></i><b>14.4</b> GLMs and Bootstrap for Loss Reserves</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#model-specification"><i class="fa fa-check"></i><b>14.4.1</b> Model Specification</a></li>
<li class="chapter" data-level="14.4.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#model-estimation-and-prediction"><i class="fa fa-check"></i><b>14.4.2</b> Model Estimation and Prediction</a></li>
<li class="chapter" data-level="14.4.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#bootstrap"><i class="fa fa-check"></i><b>14.4.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Sec145"><i class="fa fa-check"></i><b>14.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html"><i class="fa fa-check"></i><b>15</b> Experience Rating using Bonus-Malus</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:Sec151"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:Sec152"><i class="fa fa-check"></i><b>15.2</b> <em>BMS</em> in Several Countries</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#bms-in-malaysia"><i class="fa fa-check"></i><b>15.2.1</b> <em>BMS</em> in Malaysia</a></li>
<li class="chapter" data-level="15.2.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#bms-in-other-countries"><i class="fa fa-check"></i><b>15.2.2</b> <em>BMS</em> in Other Countries</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:Sec153"><i class="fa fa-check"></i><b>15.3</b> <em>BMS</em> and Markov Chain Model</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#transition-probability"><i class="fa fa-check"></i><b>15.3.1</b> Transition Probability</a></li>
<li class="chapter" data-level="15.3.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#some-applications"><i class="fa fa-check"></i><b>15.3.2</b> Some Applications</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:Sec154"><i class="fa fa-check"></i><b>15.4</b> <em>BMS</em> and Stationary Distribution</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#stationary-distribution"><i class="fa fa-check"></i><b>15.4.1</b> Stationary Distribution</a></li>
<li class="chapter" data-level="15.4.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-code-for-a-stationary-distribution"><i class="fa fa-check"></i><b>15.4.2</b> <code>R</code> Code for a Stationary Distribution</a></li>
<li class="chapter" data-level="15.4.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#premium-evolution"><i class="fa fa-check"></i><b>15.4.3</b> Premium Evolution</a></li>
<li class="chapter" data-level="15.4.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-code-for-premium-evolution"><i class="fa fa-check"></i><b>15.4.4</b> <code>R</code> Code for Premium Evolution</a></li>
<li class="chapter" data-level="15.4.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#convergence-rate"><i class="fa fa-check"></i><b>15.4.5</b> Convergence Rate</a></li>
<li class="chapter" data-level="15.4.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-code-for-convergence-rate"><i class="fa fa-check"></i><b>15.4.6</b> <code>R</code> Code for Convergence Rate</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:Sec155"><i class="fa fa-check"></i><b>15.5</b> <em>BMS</em> and Premium Rating</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#premium-rating"><i class="fa fa-check"></i><b>15.5.1</b> Premium Rating</a></li>
<li class="chapter" data-level="15.5.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#a-priori-risk-classification"><i class="fa fa-check"></i><b>15.5.2</b> A Priori Risk Classification</a></li>
<li class="chapter" data-level="15.5.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#modelling-of-residual-heterogeneity"><i class="fa fa-check"></i><b>15.5.3</b> Modelling of Residual Heterogeneity</a></li>
<li class="chapter" data-level="15.5.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#stationary-distribution-allowing-for-residual-heterogeneity"><i class="fa fa-check"></i><b>15.5.4</b> Stationary Distribution Allowing for Residual Heterogeneity</a></li>
<li class="chapter" data-level="15.5.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#determination-of-optimal-relativities"><i class="fa fa-check"></i><b>15.5.5</b> Determination of Optimal Relativities</a></li>
<li class="chapter" data-level="15.5.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#numerical-illustrations"><i class="fa fa-check"></i><b>15.5.6</b> Numerical Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:Sec156"><i class="fa fa-check"></i><b>15.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html"><i class="fa fa-check"></i><b>16</b> Quantifying Dependence</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Sec161"><i class="fa fa-check"></i><b>16.1</b> Classic Measures of Scalar Associations</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#association-measures-for-quantitative-variables"><i class="fa fa-check"></i><b>16.1.1</b> Association Measures for Quantitative Variables</a></li>
<li class="chapter" data-level="16.1.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Sec1612"><i class="fa fa-check"></i><b>16.1.2</b> Rank Based Measures</a></li>
<li class="chapter" data-level="16.1.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Sec1613"><i class="fa fa-check"></i><b>16.1.3</b> Tail Dependence Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Sec162"><i class="fa fa-check"></i><b>16.2</b> Introduction to Copulas</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#definition-of-a-copula"><i class="fa fa-check"></i><b>16.2.1</b> Definition of a Copula</a></li>
<li class="chapter" data-level="16.2.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#sklars-theorem"><i class="fa fa-check"></i><b>16.2.2</b> Sklar’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Sec163"><i class="fa fa-check"></i><b>16.3</b> Application Using Copulas</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#marginal-models"><i class="fa fa-check"></i><b>16.3.1</b> Marginal Models</a></li>
<li class="chapter" data-level="16.3.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Sec1632"><i class="fa fa-check"></i><b>16.3.2</b> Probability Integral Transformation</a></li>
<li class="chapter" data-level="16.3.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#joint-modeling-with-copula-function"><i class="fa fa-check"></i><b>16.3.3</b> Joint Modeling with Copula Function</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Sec164"><i class="fa fa-check"></i><b>16.4</b> Types of Copulas</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#normal-gaussian-copulas"><i class="fa fa-check"></i><b>16.4.1</b> Normal (Gaussian) Copulas</a></li>
<li class="chapter" data-level="16.4.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#t--and-elliptical-copulas"><i class="fa fa-check"></i><b>16.4.2</b> <em>t</em>- and Elliptical Copulas</a></li>
<li class="chapter" data-level="16.4.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#archimedean-copulas"><i class="fa fa-check"></i><b>16.4.3</b> Archimedean Copulas</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#properties-of-copulas"><i class="fa fa-check"></i><b>16.5</b> Properties of Copulas</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Sec1651"><i class="fa fa-check"></i><b>16.5.1</b> Bounds on Association</a></li>
<li class="chapter" data-level="16.5.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Sec1652"><i class="fa fa-check"></i><b>16.5.2</b> Measures of Association</a></li>
<li class="chapter" data-level="16.5.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#tail-dependency"><i class="fa fa-check"></i><b>16.5.3</b> Tail Dependency</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Sec166"><i class="fa fa-check"></i><b>16.6</b> Importance of Dependence Modeling</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#why-is-dependence-modeling-important"><i class="fa fa-check"></i><b>16.6.1</b> Why is Dependence Modeling Important?</a></li>
<li class="chapter" data-level="16.6.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#copula-regression"><i class="fa fa-check"></i><b>16.6.2</b> Copula Regression</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Sec167"><i class="fa fa-check"></i><b>16.7</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#ts-16.a.-other-classic-measures-of-scalar-associations"><i class="fa fa-check"></i>TS 16.A. Other Classic Measures of Scalar Associations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="CAppA.html"><a href="CAppA.html"><i class="fa fa-check"></i><b>17</b> Appendix A: Review of Statistical Inference</a>
<ul>
<li class="chapter" data-level="17.1" data-path="CAppA.html"><a href="CAppA.html#S:Sec171"><i class="fa fa-check"></i><b>17.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="CAppA.html"><a href="CAppA.html#S:Sec1711"><i class="fa fa-check"></i><b>17.1.1</b> Random Sampling</a></li>
<li class="chapter" data-level="17.1.2" data-path="CAppA.html"><a href="CAppA.html#S:Sec1712"><i class="fa fa-check"></i><b>17.1.2</b> Sampling Distribution</a></li>
<li class="chapter" data-level="17.1.3" data-path="CAppA.html"><a href="CAppA.html#S:Sec1713"><i class="fa fa-check"></i><b>17.1.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="CAppA.html"><a href="CAppA.html#S:Sec172"><i class="fa fa-check"></i><b>17.2</b> Point Estimation and Properties</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="CAppA.html"><a href="CAppA.html#S:Sec1721"><i class="fa fa-check"></i><b>17.2.1</b> Method of Moments Estimation</a></li>
<li class="chapter" data-level="17.2.2" data-path="CAppA.html"><a href="CAppA.html#S:Sec1722"><i class="fa fa-check"></i><b>17.2.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="CAppA.html"><a href="CAppA.html#S:Sec173"><i class="fa fa-check"></i><b>17.3</b> Interval Estimation</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="CAppA.html"><a href="CAppA.html#S:Sec1731"><i class="fa fa-check"></i><b>17.3.1</b> Exact Distribution for Normal Sample Mean</a></li>
<li class="chapter" data-level="17.3.2" data-path="CAppA.html"><a href="CAppA.html#large-sample-properties-of-mle"><i class="fa fa-check"></i><b>17.3.2</b> Large-sample Properties of <em>MLE</em></a></li>
<li class="chapter" data-level="17.3.3" data-path="CAppA.html"><a href="CAppA.html#confidence-interval"><i class="fa fa-check"></i><b>17.3.3</b> Confidence Interval</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="CAppA.html"><a href="CAppA.html#S:Sec174"><i class="fa fa-check"></i><b>17.4</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="17.4.1" data-path="CAppA.html"><a href="CAppA.html#S:Sec1741"><i class="fa fa-check"></i><b>17.4.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="17.4.2" data-path="CAppA.html"><a href="CAppA.html#S:Sec1742"><i class="fa fa-check"></i><b>17.4.2</b> Student-<em>t</em> test based on <em>mle</em></a></li>
<li class="chapter" data-level="17.4.3" data-path="CAppA.html"><a href="CAppA.html#S:Sec1743"><i class="fa fa-check"></i><b>17.4.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="17.4.4" data-path="CAppA.html"><a href="CAppA.html#S:Sec1744"><i class="fa fa-check"></i><b>17.4.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="CAppB.html"><a href="CAppB.html"><i class="fa fa-check"></i><b>18</b> Appendix B: Iterated Expectations</a>
<ul>
<li class="chapter" data-level="18.1" data-path="CAppB.html"><a href="CAppB.html#S:Sec181"><i class="fa fa-check"></i><b>18.1</b> Conditional Distribution and Conditional Expectation</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="CAppB.html"><a href="CAppB.html#conditional-distribution"><i class="fa fa-check"></i><b>18.1.1</b> Conditional Distribution</a></li>
<li class="chapter" data-level="18.1.2" data-path="CAppB.html"><a href="CAppB.html#conditional-expectation-and-conditional-variance"><i class="fa fa-check"></i><b>18.1.2</b> Conditional Expectation and Conditional Variance</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="CAppB.html"><a href="CAppB.html#S:Sec182"><i class="fa fa-check"></i><b>18.2</b> Iterated Expectations and Total Variance</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="CAppB.html"><a href="CAppB.html#S:Sec1821"><i class="fa fa-check"></i><b>18.2.1</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="18.2.2" data-path="CAppB.html"><a href="CAppB.html#law-of-total-variance"><i class="fa fa-check"></i><b>18.2.2</b> Law of Total Variance</a></li>
<li class="chapter" data-level="18.2.3" data-path="CAppB.html"><a href="CAppB.html#application"><i class="fa fa-check"></i><b>18.2.3</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="CAppB.html"><a href="CAppB.html#S:Sec183"><i class="fa fa-check"></i><b>18.3</b> Conjugate Distributions</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="CAppB.html"><a href="CAppB.html#linear-exponential-family"><i class="fa fa-check"></i><b>18.3.1</b> Linear Exponential Family</a></li>
<li class="chapter" data-level="18.3.2" data-path="CAppB.html"><a href="CAppB.html#S:Sec1832"><i class="fa fa-check"></i><b>18.3.2</b> Conjugate Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="CAppC.html"><a href="CAppC.html"><i class="fa fa-check"></i><b>19</b> Appendix C: Maximum Likelihood Theory</a>
<ul>
<li class="chapter" data-level="19.1" data-path="CAppC.html"><a href="CAppC.html#S:Sec191"><i class="fa fa-check"></i><b>19.1</b> Likelihood Function</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="CAppC.html"><a href="CAppC.html#likelihood-and-log-likelihood-functions"><i class="fa fa-check"></i><b>19.1.1</b> Likelihood and Log-likelihood Functions</a></li>
<li class="chapter" data-level="19.1.2" data-path="CAppC.html"><a href="CAppC.html#properties-of-likelihood-functions"><i class="fa fa-check"></i><b>19.1.2</b> Properties of Likelihood Functions</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="CAppC.html"><a href="CAppC.html#S:Sec192"><i class="fa fa-check"></i><b>19.2</b> Maximum Likelihood Estimators</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="CAppC.html"><a href="CAppC.html#definition-and-derivation-of-mle"><i class="fa fa-check"></i><b>19.2.1</b> Definition and Derivation of <em>MLE</em></a></li>
<li class="chapter" data-level="19.2.2" data-path="CAppC.html"><a href="CAppC.html#asymptotic-properties-of-mle"><i class="fa fa-check"></i><b>19.2.2</b> Asymptotic Properties of <em>MLE</em></a></li>
<li class="chapter" data-level="19.2.3" data-path="CAppC.html"><a href="CAppC.html#use-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>19.2.3</b> Use of Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="CAppC.html"><a href="CAppC.html#S:Sec193"><i class="fa fa-check"></i><b>19.3</b> Statistical Inference Based on Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="CAppC.html"><a href="CAppC.html#hypothesis-testing"><i class="fa fa-check"></i><b>19.3.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="19.3.2" data-path="CAppC.html"><a href="CAppC.html#S:Sec1932"><i class="fa fa-check"></i><b>19.3.2</b> <em>MLE</em> and Model Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html"><i class="fa fa-check"></i><b>20</b> Appendix D: Summary of Distributions</a>
<ul>
<li class="chapter" data-level="20.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#S:DiscreteDistributions"><i class="fa fa-check"></i><b>20.1</b> Discrete Distributions</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#the-ab0-class"><i class="fa fa-check"></i><b>20.1.1</b> The <em>(a,b,0)</em> Class</a></li>
<li class="chapter" data-level="20.1.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#the-ab1-class"><i class="fa fa-check"></i><b>20.1.2</b> The <em>(a,b,1)</em> Class</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#S:ContinuousDistributions"><i class="fa fa-check"></i><b>20.2</b> Continuous Distributions</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#one-parameter-distributions"><i class="fa fa-check"></i><b>20.2.1</b> One Parameter Distributions</a></li>
<li class="chapter" data-level="20.2.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#two-parameter-distributions"><i class="fa fa-check"></i><b>20.2.2</b> Two Parameter Distributions</a></li>
<li class="chapter" data-level="20.2.3" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#three-parameter-distributions"><i class="fa fa-check"></i><b>20.2.3</b> Three Parameter Distributions</a></li>
<li class="chapter" data-level="20.2.4" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#four-parameter-distribution"><i class="fa fa-check"></i><b>20.2.4</b> Four Parameter Distribution</a></li>
<li class="chapter" data-level="20.2.5" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#other-distributions"><i class="fa fa-check"></i><b>20.2.5</b> Other Distributions</a></li>
<li class="chapter" data-level="20.2.6" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#distributions-with-finite-support"><i class="fa fa-check"></i><b>20.2.6</b> Distributions with Finite Support</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#limited-expected-values"><i class="fa fa-check"></i><b>20.3</b> Limited Expected Values</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html"><i class="fa fa-check"></i><b>21</b> Appendix E: Conventions for Notation</a>
<ul>
<li class="chapter" data-level="21.1" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:General"><i class="fa fa-check"></i><b>21.1</b> General Conventions</a></li>
<li class="chapter" data-level="21.2" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:Abbreviations"><i class="fa fa-check"></i><b>21.2</b> Abbreviations</a></li>
<li class="chapter" data-level="21.3" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:StatSymbols"><i class="fa fa-check"></i><b>21.3</b> Common Statistical Symbols and Operators</a></li>
<li class="chapter" data-level="21.4" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:Symbols"><i class="fa fa-check"></i><b>21.4</b> Common Mathematical Symbols and Functions</a></li>
<li class="chapter" data-level="21.5" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#further-readings"><i class="fa fa-check"></i><b>21.5</b> Further Readings</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="DataResources.html"><a href="DataResources.html"><i class="fa fa-check"></i><b>22</b> Appendix. Data Resources</a>
<ul>
<li class="chapter" data-level="22.1" data-path="DataResources.html"><a href="DataResources.html#S:WiscPropFundA"><i class="fa fa-check"></i><b>22.1</b> Wisconsin Property Fund</a></li>
<li class="chapter" data-level="22.2" data-path="DataResources.html"><a href="DataResources.html#Sec:DataTravel"><i class="fa fa-check"></i><b>22.2</b> ANU Corporate Travel Data</a></li>
<li class="chapter" data-level="22.3" data-path="DataResources.html"><a href="DataResources.html#Sec:DataGPA"><i class="fa fa-check"></i><b>22.3</b> ANU Group Personal Accident Data</a></li>
<li class="chapter" data-level="22.4" data-path="DataResources.html"><a href="DataResources.html#Sec:DataAuto"><i class="fa fa-check"></i><b>22.4</b> ANU Motor Vehicle Data</a></li>
<li class="chapter" data-level="22.5" data-path="DataResources.html"><a href="DataResources.html#spanish-personal-insurance-data"><i class="fa fa-check"></i><b>22.5</b> Spanish Personal Insurance Data</a></li>
<li class="chapter" data-level="22.6" data-path="DataResources.html"><a href="DataResources.html#S:CASDatasets"><i class="fa fa-check"></i><b>22.6</b> ‘R’ Package CASdatasets</a></li>
<li class="chapter" data-level="22.7" data-path="DataResources.html"><a href="DataResources.html#other-data-sources"><i class="fa fa-check"></i><b>22.7</b> Other Data Sources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTexts/Loss-Data-Analytics" target="blank">Loss Data Analytics on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Loss Data Analytics <br> Second Edition</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ChBayes" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Bayesian Statistics and Modeling<a href="ChBayes.html#ChBayes" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Chapter Preview.</em> Up to this point in the book, we have focused almost exclusively on the <a href="#" class="tooltip" style="color:green"><em>frequentist</em><span style="font-size:8pt">Type of statistical inference based in frequentist probability, which treats probability in equivalent terms to frequency and draws conclusions from sample-data by means of emphasizing the frequency or proportion of findings in the data.</span></a> approach to estimate our various loss distribution parameters. In this chapter, we switch gears and discuss a different paradigm: Bayesianism. These approaches are different as <a href="#" class="tooltip" style="color:green"><em>Bayesian</em><span style="font-size:8pt">A type of statistical inference in which the model parameters and the data are random variables.</span></a> and frequentist statisticians disagree on the source of the uncertainty: Bayesian statistics assumes that the observed data sample is fixed and that model parameters are random, whereas frequentism considers the opposite (i.e., the sample data are random, and the model parameters are fixed but unknown).</p>
<p>In this chapter, we introduce Bayesian statistics and modeling with a particular focus on loss data analytics. We begin in Section <a href="ChBayes.html#S:Sec91">9.1</a> by explaining the basics of Bayesian statistics: we compare it to frequentism and provide some historical context for the paradigm. We also introduce the seminal Bayes’ rule that serves as a key component in Bayesian statistics. Then, building on this, we present the main ingredients of Bayesian statistics in Section <a href="ChBayes.html#S:Sec92">9.2</a>: the posterior distribution, the likelihood function, and the prior distribution. Section <a href="ChBayes.html#S:Sec93">9.3</a> provides some examples of simple cases where the prior distribution is chosen for algebraic convenience, giving rise to a closed-form expression for the posterior; these are called conjugate families in the literature. Section <a href="ChBayes.html#S:Sec94">9.4</a> is dedicated to cases where we cannot get closed-form expressions and for which numerical integration is needed. Specifically, we discuss two influential Markov chain Monte Carlo samplers: the Gibbs sampler and the Metropolis–Hastings algorithm. We also discuss how to interpret the chains obtained by these methods (i.e., Markov chain diagnostics). Finally, the last section of this chapter, Section <a href="ChBayes.html#S:Sec95">9.5</a>, explains the main computing resources available and gives an illustration in the context of loss data.</p>
<div style="page-break-after: always;"></div>
<div id="S:Sec91" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> A Gentle Introduction to Bayesian Statistics<a href="ChBayes.html#S:Sec91" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In Section <a href="ChBayes.html#S:Sec91">9.1</a>, you learn how to:</p>
<ul>
<li>Describe qualitatively Bayesianism as an alternative to the frequentist approach.</li>
<li>Give the historical context for Bayesian statistics.</li>
<li>Use Bayes’ rule to find conditional probabilities.</li>
<li>Understand the basics of Bayesian statistics.</li>
</ul>
<hr />
<div id="S:Sec911" class="section level3 hasAnchor" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Bayesian versus Frequentist Statistics<a href="ChBayes.html#S:Sec911" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Classic frequentist statistics rely on frequentist probability—an interpretation of probability in which an event’s probability is defined as the limit of its relative frequency (or propensity) in many, repeatable trials. It draws conclusion from a sample that is one of many hypothetical datasets that could have been collected; the uncertainty is therefore due to the sampling error associated with the sample, while model parameters and various quantities of interest are fixed (but unknown to the experimenter).</p>
<hr />
<p><strong>Example 9.1.1. Coin Toss.</strong> Considering the simple case of coin tossing, if we flip a fair coin many times, we expect to see heads about 50% of the time. If we flip the coin only a few times, however, we could see a different sample just by chance. Indeed, there is a non-zero probability of observing all heads (and this even if the sample is very large). Figure <a href="ChBayes.html#fig:Fig91">9.1</a> illustrates this the number of heads observed in 100 samples of five iid tosses; in this specific example, we observe six samples for which all tosses are heads.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig91"></span>
<img src="LDAEd2_files/figure-html/Fig91-1.png" alt="Frequency histogram of the number of heads in a sample of five data points" width="60%" />
<p class="caption">
Figure 9.1: <strong>Frequency histogram of the number of heads in a sample of five data points</strong>
</p>
</div>
<p>Yet, as the sample size increases, the relative frequency of heads should get closer to 50% if the coin is fair. Figure <a href="ChBayes.html#fig:Fig92">9.2</a> reports that, if the number of tosses increases, then relative frequency of heads gets closer to 0.5—the probability of seeing heads on a given coin toss. In other words, increasing the sample size makes the resulting parameter estimate less uncertain, and the experimenter should be reaching a probability of 0.5 in the limit, assuming they can reproduce the experiment an infinite number of times.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig92"></span>
<img src="LDAEd2_files/figure-html/Fig92-1.png" alt="Cumulative relative frequencies of heads for an increasing sample size" width="60%" />
<p class="caption">
Figure 9.2: <strong>Cumulative relative frequencies of heads for an increasing sample size</strong>
</p>
</div>
<hr />
<p>Bayesians see things differently: they interpret probabilities as degrees of certainty about some quantity of interest. To find such probabilities, they draw on prior knowledge about those quantities, expressing one’s beliefs before some data are taken into account. Then, as data are collected, knowledge about the world is updated, allowing us to incorporate such new information in a consistent manner; the resulting distribution is referred to as the posterior, which summarizes the information in both the prior and the data.</p>
<p>In the context of Bayesian statistics and modeling, this interpretation of probability implies that model parameters are assumed to be random variables—unlike the frequentist approach that considers them fixed. Starting from the prior distribution, the data—summarized via the likelihood function—are used to update the prior distribution and create a posterior distribution of the parameters (see Section <a href="ChBayes.html#S:Sec92">9.2</a> for more details on the <a href="#" class="tooltip" style="color:green"><em>posterior distribution</em><span style="font-size:8pt">The posterior distribution is the updated probability distribution of a parameter after incorporating prior information and observed data through Bayesian inference.</span></a>, the <a href="#" class="tooltip" style="color:green"><em>likelihood function</em><span style="font-size:8pt">A function of the likeliness of the parameters in a model, given the observed data.</span></a>, and the <a href="#" class="tooltip" style="color:green"><em>prior distribution</em><span style="font-size:8pt">A probability distribution assigned prior to observing additional data</span></a>). The influence of the prior distribution on the posterior distribution becomes weaker as the size of the observed data sample increases: the prior information is less and less relevant as new information comes in.</p>
<hr />
<p><strong>Example 9.1.1. Coin Toss, continued.</strong> We now reconsider the coin tossing experiment above through a Bayesian lens. Let us first assume that we have a (potentially unfair) coin, and we wish to understand the probability of obtaining heads, denoted by <span class="math inline">\(q\)</span> in this example. Consistent with the Bayesian paradigm, this parameter is random; let us assume that the random variable associated with the probability of observing heads is denoted by <span class="math inline">\(Q\)</span>. For simplicity, we assume that we do not have prior information on the specific coin under investigation.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> Assuming again that our sample contains only five iid tosses, we know that the probability of observing <span class="math inline">\(x\)</span> heads is given by the binomial distribution with <span class="math inline">\(m=5\)</span> such that
<span class="math display">\[
p_{X|Q=q}(x) = \mathrm{Pr}(X = x \, | \, Q = q) = \binom{5}{x} q^x (1-q)^{5-x}, \quad x \in \{0,1,...,5\},
\]</span>
where <span class="math inline">\(0 \leq q \leq 1\)</span>, which emphasizes the fact that this probability depends on parameter <span class="math inline">\(q\)</span> by explicitly conditioning on it (unlike the notation used so far in this book, note that we append subscripts to the various pdf and pmf in this chapter to denote the random variables under study; this additional notation allows us to consider the pdf and pmf of different random variables in the same problem).</p>
<p>Let us generate a sample of these five tosses:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="ChBayes.html#cb123-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb123-2"><a href="ChBayes.html#cb123-2" tabindex="-1"></a>nbheads <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>)</span>
<span id="cb123-3"><a href="ChBayes.html#cb123-3" tabindex="-1"></a>num_flips <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb123-4"><a href="ChBayes.html#cb123-4" tabindex="-1"></a>coin <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;heads&quot;</span>, <span class="st">&quot;tails&quot;</span>)</span>
<span id="cb123-5"><a href="ChBayes.html#cb123-5" tabindex="-1"></a>flips <span class="ot">&lt;-</span> <span class="fu">sample</span>(coin, <span class="at">size =</span> <span class="dv">5</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb123-6"><a href="ChBayes.html#cb123-6" tabindex="-1"></a>nbheads <span class="ot">&lt;-</span> <span class="fu">sum</span>(flips <span class="sc">==</span> <span class="st">&quot;heads&quot;</span>)</span>
<span id="cb123-7"><a href="ChBayes.html#cb123-7" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Number of heads:&quot;</span>, nbheads)</span></code></pre></div>
<pre><code>Number of heads: 3</code></pre>
<p>Based on this simulation, we obtain a data sample that contains three heads and two tails. Therefore, using Bayesian statistics, we can show that
<span class="math display">\[
f_{Q|X=3}(q) \, \propto \, q^3 (1-q)^{2},
\]</span>
where <span class="math inline">\(\propto\)</span> means proportional to (note that obtaining this equation requires some tools that will be introduced in Section <a href="ChBayes.html#S:Sec92">9.2</a>).<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> Figure <a href="ChBayes.html#fig:Fig93">9.3</a> illustrates this pdf and reports the uncertainty about parameter <span class="math inline">\(q\)</span> based on this sample of five data points. In this example, one can see that the uncertainty is quite large; this is a by-product of using only five data points. Indeed, based on these five observations, one could argue that the probability should be close to <span class="math inline">\(\frac{3}{5}=0.6\)</span>. This Bayesian analysis shows that 0.6 is likely, but that it is also very uncertain—a conclusion that is not direct in the frequentist approach.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig93"></span>
<img src="LDAEd2_files/figure-html/Fig93-1.png" alt="Posterior probability density function of parameter \(q\) for a sample of five data points" width="60%" />
<p class="caption">
Figure 9.3: <strong>Posterior probability density function of parameter <span class="math inline">\(q\)</span> for a sample of five data points</strong>
</p>
</div>
<p>Figure <a href="ChBayes.html#fig:Fig94">9.4</a> reports the analog of Figure <a href="ChBayes.html#fig:Fig92">9.2</a> through a Bayesian lens: we see the evolution of the posterior density of parameter <span class="math inline">\(q\)</span> as a function of the sample size for the same sample used in Figure <a href="ChBayes.html#fig:Fig92">9.2</a>. As we obtain more evidence, the posterior density becomes more concentrated around 0.5—a consequence of using a fair coin in the simulations above. Yet, even if the sample size if 1,000, we still see some parameter uncertainty.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig94"></span>
<img src="Figures/Fig94.gif" alt="Posterior probability density function of parameter \(q\) as a function of the sample size" width="50%" />
<p class="caption">
Figure 9.4: <strong>Posterior probability density function of parameter <span class="math inline">\(q\)</span> as a function of the sample size</strong>
</p>
</div>
<hr />
<p><strong>But why be Bayesian?</strong> There are indeed several advantages to the Bayesian approach. First, this approach allows us to describe the entire distribution of parameters conditional on the data and to provide probability statements regarding the parameters that could be interpreted as such. Second, it provides a unified approach for estimating parameters. Some non-Bayesian methods, such as <a href="#" class="tooltip" style="color:green"><em>least squares</em><span style="font-size:8pt">A technique for estimating parameters in linear regression. it is a standard approach in regression analysis to the approximate solution of overdetermined systems. in this technique, one determines the parameters that minimize the sum of squared differences between each observation and the corresponding linear combination of explanatory variables.</span></a>, require a separate approach to estimate variance components. In contrast, in Bayesian methods, all parameters can be treated in a similar fashion. Third, it allows experimenters to blend prior information from other sources with the data in a coherent manner.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<p><strong>Are there any disadvantages to being Bayesian?</strong> Well, of course: while the Bayesian approach has many advantages, it is not without its disadvantages. First, it tends to be very computationally demanding (i.e., Bayesian methods often require complex computations, especially when dealing with high-dimensional problems or large datasets). For instance, complex models may not have closed-form solutions and require specialized computational techniques, which can be time-consuming. Second, there is some subjectivity in selecting priors—our initial beliefs and knowledge about the parameters—and this can lead to different results in the end. Third, Bayesian analysis often produces results that can be challenging to communicate effectively to non-experts.</p>
<p>Despite these disadvantages, the Bayesian approach remains powerful and flexible for many actuarial problems.</p>
<p><strong>Do I need to be a Bayesian to embrace Bayesian statistics?</strong> No, this can be decided on a case-by-case basis. Consider a Bayesian study when you have prior knowledge or beliefs about the parameters, need to explicitly quantify uncertainty in your estimates, have limited data, require a flexible framework for complex models, or when decision-making under uncertainty is a key aspect of your analysis.</p>
<p>Even if one does not want to be a Bayesian truly, they can still recognize the usefulness of some of the methods. Indeed, some modern statistical tools in artificial intelligence and machine learning rely heavily on Bayesian techniques (e.g., Bayesian neural networks, Gaussian processes, and Bayesian classifiers, to name a few).</p>
</div>
<div id="S:Sec912" class="section level3 hasAnchor" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> A Brief History Lesson<a href="ChBayes.html#S:Sec912" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Interestingly, some have argued that the birth of Bayesian statistics is intimately related to insurance; see, for instance, <span class="citation">Cowles (<a href="bibliography.html#ref-cowles2013applied">2013</a>)</span>. Specifically, the Great Fire of London in 1666—destroying more than 10,000 homes and about 100 churches—led to the rise of insurance as we know it today. Shortly after, the first full-fledged fire insurance company came into existence in England during the 1680s. By the turn of the century, the idea of insurance was well ingrained and its use was booming in England; see, for instance, <span class="citation">Haueter (<a href="bibliography.html#ref-haueter2017history">2017</a>)</span>. Yet, the lack of statistical models and methods—much needed to understand risk—drove some insurers to bankruptcy.</p>
<p>Thomas Bayes, an English statistician, philosopher and Presbyterian minister, applied his mind to some of these important statistical questions raised by insurers. This culminated into Bayes’ theory of probability in his seminal essay entitled <em>Essay towards solving a problem in the doctrine of chances</em>, published posthumously in 1763. This essay laid out the foundation of what we now know as Bayesian statistics.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ThomasBayes"></span>
<img src="Figures/ThomasBayes.gif" alt="Portrait of an unknown Presbyterian clergyman identified as Thomas Bayes in O’Donnell (1936)" width="33%" />
<p class="caption">
Figure 9.5: <strong>Portrait of an unknown Presbyterian clergyman identified as Thomas Bayes in <span class="citation">O’Donnell (<a href="bibliography.html#ref-o1936history">1936</a>)</span></strong>
</p>
</div>
<p>Thomas Bayes’ work also helped Pierre-Simon Laplace, a famous French scholar and polymath, to develop and popularize the Bayesian interpretation of probability in the late 1700s and early 1800s. He also moved beyond Bayes’ essay and generalized his framework. Laplace’s efforts were followed by many, and Bayesian thinking continued to progress throughout the years with the help of statisticians like Bruno de Finetti, Harold Jeffreys, Dennis Lindley, and Leonard Jimmie Savage.</p>
<p>Nowadays, Bayesian statistics and modeling is widely used in science, thanks to the increase in computational power over the past 30 years. Actuarial science and loss modeling, more specifically, have also been breeding grounds for Bayesian methodology. So, Bayesian statistics circles back to insurance, in a sense, where it all started.</p>
</div>
<div id="S:Sec913" class="section level3 hasAnchor" number="9.1.3">
<h3><span class="header-section-number">9.1.3</span> Bayes’ Rule<a href="ChBayes.html#S:Sec913" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This subsection introduces how the Bayes’ rule is applied to calculating conditional probabilities for events.</p>
<p><strong>Conditional Probability.</strong> The concept of conditional probability considers the relationship between probabilities of two (or more) events happening. In its most simple form, being interested in conditional probability boils down to answering this question: <em>given that event <span class="math inline">\(B\)</span> happened, how does this affect the probability that <span class="math inline">\(A\)</span> happens?</em> To answer this question, we can define formally the concept of conditional probability:
<span class="math display">\[
\Pr\left(A \, \middle| \, B\right) = \frac{\Pr(A \cap B)}{\Pr(B)}.
\]</span>
To be properly defined, we must assume that <span class="math inline">\(\Pr(B)\)</span> is larger than zero; that is, event <span class="math inline">\(B\)</span> is not impossible. Simply put, a conditional probability turns <span class="math inline">\(B\)</span> into the new probability space, and then cares only about the part of <span class="math inline">\(A\)</span> that is inside <span class="math inline">\(B\)</span> (i.e., <span class="math inline">\(A \cap B\)</span>).</p>
<hr />
<p><strong>Example 9.1.2. Actuarial Exam Question.</strong>
An insurance company estimates that 40% of policyholders who have an extended health policy but no long-term disability policy will renew next year, and 70% of policyholders who have a long-term disability policy but no extended health policy will renew next year. The company also estimates that 50% of their clients who have both policies will renew at least one next year. The company records report that 65% of clients have an extended health policy, 40% have a long-term disability policy, and 10% have both. Using the data above, calculate the percentage of policyholders that will renew at least one policy next year.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></p>
<h5 style="text-align: center;">
<a id="displayExample.9.1.2" href="javascript:toggleEX('toggleExample.9.1.2','displayExample.9.1.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.1.2" style="display: none">
<p><em>Example Solution.</em> Let <span class="math inline">\(E\)</span> be the event that a policyholder has an extended health policy, <span class="math inline">\(D\)</span> be the event that a policyholder has a long-term disability policy, and <span class="math inline">\(R\)</span> be the event that a policyholder renews a policy. We are given:</p>
<ul>
<li><span class="math inline">\(\Pr(E)=0.65\)</span>,</li>
<li><span class="math inline">\(\Pr(D)=0.40\)</span>,</li>
<li><span class="math inline">\(\Pr(E \cap D)=0.10\)</span>,</li>
<li><span class="math inline">\(\Pr(R \,| \,E\cap D^{\text{c}})=0.40\)</span>,</li>
<li><span class="math inline">\(\Pr(R \,| \,E^{\text{c}}\cap D)=0.70\)</span>,</li>
<li><span class="math inline">\(\Pr(R\,| \,E\cap D)=0.50\)</span>.</li>
</ul>
<p>We are looking for <span class="math inline">\(\Pr(R)\)</span>.</p>
<p>Note that
<span class="math display">\[
\Pr(E\cap D^{\text{c}}) = \Pr(E) - \Pr(E \cap D) = 0.65 - 0.10 = 0.55,
\]</span>
and
<span class="math display">\[
\Pr(E^{\text{c}}\cap D) = \Pr(D) - \Pr(E \cap D) = 0.40 - 0.10 = 0.30.
\]</span>
Moreover, note that <span class="math inline">\(E\cap D^{\text{c}}\)</span>, <span class="math inline">\(E^{\text{c}}\cap D\)</span>, and <span class="math inline">\(E\cap D\)</span> are mutually disjoint, and that=
<span class="math display">\[\begin{align*}
\Pr(R) = &amp; \, \Pr( R \cap (E \cap D^{\text{c}}) ) + \Pr( R \cap (E^{\text{c}} \cap D) ) + \Pr( R \cap (E \cap D) ) \\
= &amp; \, \Pr( R \, |\, (E \cap D^{\text{c}}) ) \Pr(E \cap D^{\text{c}}) + \Pr( R  \, |\, (E^{\text{c}} \cap D) ) \Pr(E^{\text{c}} \cap D) \\
&amp;\, + \Pr( R  \, |\, (E \cap D) ) \Pr(E \cap D) \\
= &amp;\, 0.40 \times 0.55 + 0.70 \times 0.30 + 0.50 \times 0.10 \\
= &amp;\, 0.48.
\end{align*}\]</span></p>
</div>
<hr />
<p><strong>Independence.</strong> If two events are unrelated to one another, we say that they are <a href="#" class="tooltip" style="color:green"><em>independent</em><span style="font-size:8pt">Two variables are independent if conditional information given about one variable provides no information regarding the other variable</span></a>. Specifically, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if
<span class="math display">\[
\Pr(A \cap B) = \Pr(A) \, \Pr(B).
\]</span>
For positive probability events, independence between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is also equivalent to
<span class="math display">\[
\Pr(A\,|\, B) = \Pr(A) \quad \text{and} \quad \Pr(B\,|\, A) = \Pr(B),
\]</span>
which means that the occurrence of event <span class="math inline">\(B\)</span> does not have an impact on the occurrence of <span class="math inline">\(A\)</span>, and vice versa.</p>
<p><strong>Bayes’ Rule.</strong> Intuitively speaking, <a href="#" class="tooltip" style="color:green"><em>Bayes’ rule</em><span style="font-size:8pt"></span></a> provides a mechanism to put our Bayesian thinking into practice. It allows us to update our information by combining the data—from the likelihood—and the prior together to obtain a posterior probability.</p>
<hr />
<p><strong>Proposition 9.1.1. Bayes’ Rule for Events.</strong>
For events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the posterior probability of event <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> follows
<span class="math display">\[
\Pr(A\,|\,B) = \frac{\Pr(B\, |\, A) \Pr(A)}{\Pr(B)},
\]</span>
where the law of total probability allows us to find</p>
<p><span class="math display">\[
\Pr(B) = \Pr(A) \Pr(B \, | \, A) + \Pr(A^{\text{c}}) \Pr(B \, |\, A^{\text{c}}).
\]</span>
Note, again, that this works as long as event <span class="math inline">\(B\)</span> is possible (i.e., <span class="math inline">\(\Pr(B) &gt; 0\)</span>).<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.1" href="javascript:toggleTheory('toggleTheory.Theory.1','displayCode.Theory.1');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.1" style="display: none">
<hr />
<p><span class="math inline">\(\textbf{Proof}.\)</span> Bayes’ rule may be derived from the definition of conditional probability shown above:
<span class="math display">\[
\Pr(A\, |\, B) = \frac{\Pr(A \cap B)}{\Pr(B)}
\]</span>
if <span class="math inline">\(\Pr(B) &gt; 0\)</span>. Similarly,
<span class="math display">\[
\Pr(B\, |\, A) = \frac{\Pr(A \cap B)}{\Pr(A)}
\]</span>
if <span class="math inline">\(\Pr(A) &gt; 0\)</span>. Solving for <span class="math inline">\(\Pr(A \cap B)\)</span> in the last equation and substituting into the first one yields Bayes’ rule:
<span class="math display">\[
\Pr(A\,|\,B) = \frac{\Pr(B\, |\, A) \Pr(A)}{\Pr(B)}.
\]</span></p>
<hr />
</div>
<hr />
<p>Simply put, the posterior probability of event <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is obtained by combining the likelihood of <span class="math inline">\(B\)</span> given a fixed <span class="math inline">\(A\)</span>—proxied by <span class="math inline">\(\Pr(B\, |\, A)\)</span>—with the prior probability of observing <span class="math inline">\(A\)</span>, and then dividing it by the marginal probability of event <span class="math inline">\(B\)</span> to make sure that the probabilities sum up to one.</p>
<hr />
<p><strong>Example 9.1.3. Actuarial Exam Question.</strong> An automobile insurance company insures drivers of all ages. An actuary compiled the probability of having an accident for some age bands as well as an estimate of the portion of the company’s insured drivers in each age band:
<span class="math display">\[
\small{
\begin{matrix}
    \begin{array}{c|c|c} \hline
    \text{Age of Driver} &amp; \text{Probability of Accident} &amp; \text{Portion of Company&#39;s } \\
    &amp; &amp; \text{Insured Drivers} \\\hline
    \text{16-20} &amp; 0.06 &amp; 0.08 \\\hline
    \text{21-30} &amp; 0.03 &amp; 0.15 \\\hline
    \text{31-65} &amp; 0.02 &amp; 0.49 \\\hline
    \text{66-99} &amp; 0.04 &amp; 0.28 \\\hline
    \end{array}
\end{matrix}
}
\]</span></p>
<p>A randomly selected driver that the company insures has an accident. Calculate the probability that the driver was age 16-20.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a></p>
<h5 style="text-align: center;">
<a id="displayExample.9.1.3" href="javascript:toggleEX('toggleExample.9.1.3','displayExample.9.1.3');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.1.3" style="display: none">
<p><em>Example Solution.</em> Let <span class="math inline">\(B\)</span> be the event of an insured driver having an accident, and let</p>
<ul>
<li><span class="math inline">\(A_1\)</span> be the event related to the driver’s age being in the range 16-20, </li>
<li><span class="math inline">\(A_2\)</span> be the event related to the driver’s age being in the range 21-30,</li>
<li><span class="math inline">\(A_3\)</span> be the event related to the driver’s age being in the range 31-65,</li>
<li><span class="math inline">\(A_4\)</span> be the event related to the driver’s age being in the range 66-99.</li>
</ul>
<p>Then,
<span class="math display">\[\begin{align*}
\Pr(A_1\, |\, B) = &amp;\, \frac{\Pr(B\, |\, A_1) \Pr(A_1)}{\Pr(B\, |\, A_1) \Pr(A_1)+\Pr(B\, |\, A_2) \Pr(A_2)+\Pr(B\, |\, A_3) \Pr(A_3)+\Pr(B\, |\, A_4) \Pr(A_4)} \\
=&amp;\, \frac{0.06\times 0.08}{0.06\times 0.08 + 0.03 \times 0.15 + 0.02 \times 0.49 + 0.04\times 0.28} \\
=&amp;\, 0.1584.
\end{align*}\]</span></p>
</div>
</div>
<div id="an-introductory-example-of-bayes-rule" class="section level3 hasAnchor" number="9.1.4">
<h3><span class="header-section-number">9.1.4</span> An Introductory Example of Bayes’ Rule<a href="ChBayes.html#an-introductory-example-of-bayes-rule" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The example above illustrates how to use Bayes’ rule in an academic context; the focus of this book is, nonetheless, data analytics. We therefore also wish to illustrate Bayes’ rule by using <em>real</em> data. In this introductory example, we use the Singapore auto data <code>sgautonb</code> of the R package <code>CASdatasets</code> that was already used in Chapter <a href="ChapFrequency-Modeling.html#ChapFrequency-Modeling">3</a>.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="ChBayes.html#cb125-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;CASdatasets&quot;</span>)</span>
<span id="cb125-2"><a href="ChBayes.html#cb125-2" tabindex="-1"></a><span class="fu">data</span>(sgautonb)</span></code></pre></div>
<p>This dataset contains information about the number of car accidents and some risk factors (i.e., the type of the vehicle insured, the age of the vehicle, the sex of the policyholder, and the age of the policyholder grouped into seven categories).<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></p>
<hr />
<p><strong>Example 9.1.4. Singapore Insurance Data.</strong> A new insurance company—targeting an older segment of the population—estimates that 20% of their policyholders will be 65 years old and older. The actuaries working at the insurance company believes that the Singapore insurance dataset is credible to understand the accident occurrence of the new company. Based on this information, find the probability that a randomly selected driver who has (at least) one accident, is 65 years or older.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.1.4" href="javascript:toggleEX('toggleExample.9.1.4','displayExample.9.1.4');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.1.4" style="display: none">
<p><em>Example Solution.</em> Let <span class="math inline">\(O\)</span> denote the event related to the policyholder being 65 years old and older (i.e., Age Category 6 in the dataset), and <span class="math inline">\(A\)</span> the event of a policyholder having at least an accident. Using Bayes’ rule, we have that</p>
<p><span class="math display">\[
\Pr(O\, |\, A) = \frac{\Pr(A\, |\, O)\Pr(O)}{\Pr(A)},
\]</span></p>
<p>where the prior probability <span class="math inline">\(\Pr(O)\)</span> is given by the problem statement: <span class="math inline">\(\Pr(O) = 0.20\)</span>. This implies that <span class="math inline">\(\Pr(O^{\text{c}}) = 1-0.20 = 0.80\)</span>. From the Singapore insurance data, we know that <span class="math inline">\(\Pr(A\, |\, O) = 0.1082803\)</span> and <span class="math inline">\(\Pr(A\, |\, O^{\text{c}}) = 0.06415506\)</span>, which allow us to use the law of total probability to obtain:
<span class="math display">\[
\Pr(A) = \Pr(A\, |\, O)\Pr(O) + \Pr(A\, |\, O^{\text{c}})\Pr(O^{\text{c}}).
\]</span></p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="ChBayes.html#cb126-1" tabindex="-1"></a><span class="co"># Example 9.1.4 Illustrative Code</span></span>
<span id="cb126-2"><a href="ChBayes.html#cb126-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(sgautonb<span class="sc">$</span>AgeCat)</span>
<span id="cb126-3"><a href="ChBayes.html#cb126-3" tabindex="-1"></a>nO <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>AgeCat <span class="sc">==</span> <span class="dv">6</span>)</span>
<span id="cb126-4"><a href="ChBayes.html#cb126-4" tabindex="-1"></a>nOc <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>AgeCat <span class="sc">!=</span> <span class="dv">6</span>)</span>
<span id="cb126-5"><a href="ChBayes.html#cb126-5" tabindex="-1"></a>nAandO <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>AgeCat <span class="sc">==</span> <span class="dv">6</span> <span class="sc">&amp;</span> sgautonb<span class="sc">$</span>Clm_Count <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb126-6"><a href="ChBayes.html#cb126-6" tabindex="-1"></a>nAandOc <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>AgeCat <span class="sc">!=</span> <span class="dv">6</span> <span class="sc">&amp;</span> sgautonb<span class="sc">$</span>Clm_Count <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb126-7"><a href="ChBayes.html#cb126-7" tabindex="-1"></a></span>
<span id="cb126-8"><a href="ChBayes.html#cb126-8" tabindex="-1"></a>PAO <span class="ot">&lt;-</span> nAandO<span class="sc">/</span>nO</span>
<span id="cb126-9"><a href="ChBayes.html#cb126-9" tabindex="-1"></a>PAOc <span class="ot">&lt;-</span> nAandOc<span class="sc">/</span>nOc</span>
<span id="cb126-10"><a href="ChBayes.html#cb126-10" tabindex="-1"></a></span>
<span id="cb126-11"><a href="ChBayes.html#cb126-11" tabindex="-1"></a>POA <span class="ot">&lt;-</span> PAO <span class="sc">*</span> <span class="fl">0.2</span><span class="sc">/</span>(PAO <span class="sc">*</span> <span class="fl">0.2</span> <span class="sc">+</span> PAOc <span class="sc">*</span> <span class="fl">0.8</span>)</span>
<span id="cb126-12"><a href="ChBayes.html#cb126-12" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The probability that policyholder having accident </span><span class="sc">\n</span><span class="st"> is 65 years old and older is&quot;</span>,</span>
<span id="cb126-13"><a href="ChBayes.html#cb126-13" tabindex="-1"></a>    POA)</span></code></pre></div>
<pre><code>The probability that policyholder having accident 
 is 65 years old and older is 0.296739115</code></pre>
<hr />
<p>The probability that a randomly selected driver who has (at least) one accident, is 65 years or older is therefore about 29.7% for the new insurance company. Simply put, we started with an <em>a priori</em> probability of 20%, meaning that unconditionally, we should have about 20% of policyholders aged 65 years old and older, and that 20% of the policyholders should have at least one accident. Then, based on the observed data, this probability is updated to 29.7%: the data seem to imply that, of all people having accidents, there are more older policyholders than what we would have guessed just based on our prior assumption.</p>
<hr />
</div>
<hr />
<p>In the next section, we expand on the idea of Bayes’ rule and apply it to slightly more general cases involving random variables instead of events.</p>
</div>
</div>
<div id="S:Sec92" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Building Blocks of Bayesian Statistics<a href="ChBayes.html#S:Sec92" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In Section <a href="ChBayes.html#S:Sec92">9.2</a>, you learn how to:</p>
<ul>
<li>Describe the main components of Bayesian statistics; that is, the posterior distribution, the likelihood function, and the prior distribution.</li>
<li>Summarize the different classes of priors used in practice.</li>
</ul>
<hr />
<p>Proposition 9.1.1 above deals with the elementary case of Bayes’ rule for events. Although this version of Bayes’ rule is useful to understand the foundation of Bayesian statistics, we will need slightly more general versions of it to achieve our aim. Specifically, Proposition 9.1.1 needs to be generalized to the case of random variables.</p>
<p>Let us first consider the case of discrete random variables. Assume <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are both discrete random variables that allow for the following joint pmf of
<span class="math display">\[
p_{X,Y}(x,y) = \Pr(X=x \,\text{ and }\, Y=y)
\]</span>
as well as the following marginal distributions for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:
<span class="math display">\[
p_X(x)= \Pr(X=x) = \sum_k p_{X,Y}(x,k) \quad \text{and} \quad p_Y(y)= \Pr(Y=y) = \sum_k p_{X,Y}(k,y),
\]</span>
respectively. Using the result of Proposition 9.1.1 and setting event <span class="math inline">\(A\)</span> as <span class="math inline">\(\{Y=y\}\)</span> and <span class="math inline">\(B\)</span> as <span class="math inline">\(\{X=x\}\)</span> yields
<span class="math display">\[
p_{Y|X=x}(y)= \frac{p_{X|Y=y}(x)\, p_Y(y)}{p_X(x)},
\]</span>
where <span class="math inline">\(p_{Y|X=x}(y) = \Pr\left( Y = y \, \middle|\, X = x\right)\)</span> is the conditional pmf of <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(X\)</span> being equal to <span class="math inline">\(x\)</span>. Using the law of total probability,
<span class="math display">\[
p_X(x)= \sum_k p_{X,Y}(x,k) = \sum_k p_{X|Y=k}(x) \, p_Y(k),
\]</span>
we can rewrite the denominator above to get the following version of Bayes’ rule:
<span class="math display">\[
p_{Y|X=x}(y) = \frac{p_{X|Y=y}(x)\, p_Y(y)}{\sum_k p_{X|Y=k}(x) \, p_Y(k)}.
\]</span>
We can also obtain a similar Bayes’ rule for continuous random variables by replacing probability mass functions by probability density functions, and sums by integrals.</p>
<hr />
<p><strong>Proposition 9.2.1. Bayes’ Rule for Continuous Random Variables.</strong>
For two continuous random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the conditional probability density function of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> follows
<span class="math display">\[
f_{Y|X=x}(y) = \frac{f_{X|Y=y}(x)\, f_Y(y)}{f_X(x)},
\]</span>
where the marginal distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are given as follows:
<span class="math display">\[
f_X(x)=\int_{-\infty}^{\infty} \! f_{X,Y}(x,u) \, du \quad \text{and} \quad f_Y(y)=\int_{-\infty}^{\infty} \! f_{X,Y}(u,y) \, du,
\]</span>
respectively. Similar to the discrete random variable case, we can swap the denominator of the equation above for
<span class="math display">\[
f_{X}(x) = \int_{-\infty}^{\infty} \! f_{X,Y}(x,u) \, du = \int_{-\infty}^{\infty} \! f_{X|Y=u}(x) \, f_Y (u) \, du
\]</span>
by using the law of total probability.</p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.2" href="javascript:toggleTheory('toggleTheory.Theory.2','displayCode.Theory.2');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.2" style="display: none">
<hr />
<p><span class="math inline">\(\textbf{Proof}\)</span>. Bayes’ rule for continuous random variables may be derived from the definition of conditional probability density functions:
<span class="math display">\[
f_{Y|X=x}(y) = \frac{f_{X,Y}(x,y)}{f_{X}(x)},
\]</span>
if <span class="math inline">\(f_{X}(x)&gt;0\)</span>. Similarly,
<span class="math display">\[
f_{X|Y=y}(x) = \frac{f_{X,Y}(x,y)}{f_{Y}(y)}.
\]</span>
if <span class="math inline">\(f_{Y}(y) &gt; 0\)</span>. Solving for <span class="math inline">\(f_{X,Y}(x,y)\)</span> in the last equation and substituting into the first one yields Bayes’ rule for continuous random variables:
<span class="math display">\[
f_{Y|X=x}(y) = \frac{f_{X|Y=y}(x)\, f_Y(y)}{f_X(x)}.
\]</span></p>
<hr />
</div>
<hr />
<p>Note that one can mix the discrete and continuous definitions of Bayes’ rule to accommodate for cases where the parameters have continuous random variables and the observations are expressed via discrete random variables, or vice versa.</p>
<div id="S:Sec921" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Posterior Distribution<a href="ChBayes.html#S:Sec921" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Model parameters are assumed to be random variables under the Bayesian paradigm, meaning that Bayes’ rule for (discrete or continuous) random variables can be applied to update the prior knowledge about parameters by using new data. This is indeed similar to the process used in Section <a href="ChBayes.html#S:Sec911">9.1.1</a>.</p>
<p>Let us consider only one unknown model parameter <span class="math inline">\(\theta\)</span> associated with random variable <span class="math inline">\(\Theta\)</span> for now.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> Further, consider <span class="math inline">\(n\)</span> observations
<span class="math display">\[
\mathbf{x} = (x_1, x_2, ..., x_n),
\]</span>
which are realizations of the collection of random variables
<span class="math display">\[
\mathbf{X} = (X_1, X_2, ..., X_n).
\]</span>
If <span class="math inline">\(Y\)</span> in Proposition 9.2.1 is replaced by <span class="math inline">\(\Theta\)</span> and <span class="math inline">\(X\)</span> by <span class="math inline">\(\mathbf{X}\)</span>, we obtain
<span class="math display">\[
f_{\Theta|\mathbf{X}=\mathbf{x}}(\theta) = \frac{f_{\mathbf{X}|\Theta=\theta}(\mathbf{x})\, f_{\Theta}(\theta)}{f_{\mathbf{X}}(\mathbf{x})},
\]</span>
which represents the posterior distribution of the model parameter after updating the distribution based on the new observations <span class="math inline">\(\mathbf{x}\)</span>, and where</p>
<ul>
<li><span class="math inline">\(f_{\mathbf{X}|\Theta=\theta}(\mathbf{x})\)</span> is the likelihood function, also known as the conditional joint pdf of the observations assuming a given value of parameter <span class="math inline">\(\theta\)</span>,</li>
<li><span class="math inline">\(f_{\Theta}(\theta)\)</span> is the unconditional pdf of the parameter that represents the prior information, and</li>
<li><span class="math inline">\(f_{\mathbf{X}}(\mathbf{x})\)</span> is the marginal likelihood, which is a constant term with respect to <span class="math inline">\(\theta\)</span>, making the posterior density integrate to one.</li>
</ul>
<p>In other words, Bayes’ rule provides a way to update the prior distribution of the parameter into a posterior distribution—by considering the observations <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>Note that the marginal likelihood is constant once we have the observations. It does not depend on <span class="math inline">\(\theta\)</span> and does not impact the overall shape of the pdf: it only provides the adequate scaling to ensure that the density integrates to one. For this reason, it is common to write down the posterior distribution using a proportional relationship instead:
<span class="math display">\[
f_{\Theta|\mathbf{X}=\mathbf{x}}(\theta) \propto \underbrace{f_{\mathbf{X}|\Theta=\theta}(\mathbf{x})}_{\text{Likelihood}} \, \, \underbrace{\vphantom{f_{\mathbf{X}|\Theta=\theta}(\mathbf{x})}f_{\Theta}(\theta)}_{\text{Prior}}.
\]</span></p>
<hr />
<p><strong>Example 9.2.1. A Problem Inspired from <span class="citation">Meyers (<a href="bibliography.html#ref-meyers1994quantifying">1994</a>)</span>.</strong>
A car insurance pays the following (independent) claim amounts on an automobile insurance policy:
<span class="math display">\[
1050, \quad\quad 1250, \quad\quad 1550, \quad\quad 2600, \quad\quad 5350, \quad\quad 10200.
\]</span>
The amount of a single payment is distributed as a single-parameter Pareto distribution with <span class="math inline">\(\theta = 1000\)</span> and <span class="math inline">\(\alpha\)</span> unknown, such that
<span class="math display">\[
f_{X_i|A=\alpha}(x_i) = \frac{\alpha \, 1000^{\alpha}}{x_i^{\alpha+1}}, \quad x_i \in \mathbb{R}_+.
\]</span>
We assume that the prior distribution of <span class="math inline">\(\alpha\)</span> is given by a gamma distribution with shape parameter 2 and scale parameter 1, and its pdf is given by
<span class="math display">\[
f_{A}(\alpha) = \alpha\,e^{-\alpha}, \quad \alpha \in \mathbb{R}_+.
\]</span>
Find the posterior distribution of parameter <span class="math inline">\(\alpha\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.2.1" href="javascript:toggleEX('toggleExample.9.2.1','displayExample.9.2.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.2.1" style="display: none">
<p><em>Example Solution.</em> The likelihood function is constructed by multiplying the pdf of the single payment amounts because they are independent; that is,
<span class="math display">\[
f_{\mathbf{X}|A=\alpha}(\mathbf{x}) = \prod_{i=1}^6 f_{X_i|A=\alpha}(x) = \frac{\alpha^6 \, 1000^{6\alpha}}{\prod_{i=1}^6 x_i^{\alpha+1}} = \alpha^{6} \, e^{-5.66518\alpha-41.44653} .
\]</span>
The posterior distribution is given by
<span class="math display">\[
f_{A|\mathbf{X}=\mathbf{x}}(\alpha) = \frac{\alpha^{7} \, e^{-6.66518\alpha-41.44653}}{\int_0^{\infty} \! \alpha^{7} \, e^{-6.66518\alpha-41.44653} \, d\alpha} = \frac{\alpha^{7} \, e^{-6.66518\alpha}}{\int_0^{\infty} \! \alpha^{7} \, e^{-6.66518\alpha} \, d\alpha} .
\]</span>
Interestingly, we do not need to solve the integral in the denominator to find this distribution. As we know that the results should be a proper pdf and that the numerator looks like a gamma distribution, we can deduce that
<span class="math display">\[
f_{A|\mathbf{X}=\mathbf{x}}(\alpha) = \frac{6.66518^8}{\Gamma(8)} \, \alpha^7 \, e^{-6.66518 \, \alpha},
\]</span>
which is a gamma distribution with shape parameter 8 and scale parameter <span class="math inline">\(\frac{1}{6.66518}\)</span>. Figure 9.6 reports the posterior distribution of <span class="math inline">\(\alpha\)</span>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig96"></span>
<img src="LDAEd2_files/figure-html/Fig96-1.png" alt="Posterior densities of parameter \(\alpha\)" width="60%" />
<p class="caption">
Figure 9.6: <strong>Posterior densities of parameter <span class="math inline">\(\alpha\)</span></strong>
</p>
</div>
</div>
<hr />
<p>The discussion above considered continuous random variables, but the same logic can be applied to discrete random variables by replacing probability density functions by probability mass functions.</p>
<hr />
<p><strong>Example 9.2.2. Coin Toss Revisited.</strong>
Assume that you observe three heads out of five (independent) tosses. Each toss has a probability of <span class="math inline">\(q\)</span> of observing heads and <span class="math inline">\(1-q\)</span> of observing tails. Find the posterior distribution of <span class="math inline">\(q\)</span> assuming a uniform prior distribution over the interval <span class="math inline">\([0,1]\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.2.2" href="javascript:toggleEX('toggleExample.9.2.2','displayExample.9.2.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.2.2" style="display: none">
<p><em>Example Solution.</em> The prior distribution of <span class="math inline">\(q\)</span> is given by
<span class="math display">\[
f_Q(q) = 1, \quad q \in [0,1].
\]</span>
Assuming the likelihood function conditional on <span class="math inline">\(Q=q\)</span> is given by a binomial distribution with <span class="math inline">\(m=5\)</span> and <span class="math inline">\(x=3\)</span>,
<span class="math display">\[
p_{X|Q=q}(x) = \binom{5}{3} q^3 (1-q)^2,
\]</span>
we have that the posterior distribution of <span class="math inline">\(q\)</span> is given by
<span class="math display">\[
f_{Q|X=3}(q) \,  \propto \, p_{X|Q=q}(x) \, f_Q(q) = q^3 \,(1-q)^2,
\]</span>
which is a beta distribution with <span class="math inline">\(a = 4\)</span>, <span class="math inline">\(b = 3\)</span>, and <span class="math inline">\(\theta = 1\)</span>; that is, we can easily deduce that
<span class="math display">\[
f_{Q|X=3}(q) = \frac{\Gamma(7)}{\Gamma(4)\Gamma(3)} \, q^3 \, (1-q)^2.
\]</span></p>
</div>
<hr />
<p>In the following subsections, we will discuss at greater length the two main building blocks used to build the posterior distribution: the likelihood function and the prior distribution.</p>
</div>
<div id="S:Sec922" class="section level3 hasAnchor" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> Likelihood Function<a href="ChBayes.html#S:Sec922" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The likelihood function is a fundamental concept in statistics. It is used to estimate the parameters of a statistical model based on observed data. As mentioned in previous chapters, the likelihood function can be used to find the maximum likelihood estimator. In Bayesian statistics, the likelihood function is used to update the prior based on the evidence (or data).</p>
<p>As explained above and in Chapter <a href="CAppA.html#CAppA">17</a>, the likelihood function is defined as the conditional joint pdf or pmf of the observed data, given the model parameters. In other words, it is the probability of observing the data given a specific parameter values.</p>
<p>Mathematically, the likelihood function is written as <span class="math inline">\(f_{\mathbf{X}|\Theta=\theta}(x)\)</span> (for continuous random variables) or <span class="math inline">\(p_{\mathbf{X}|\Theta=\theta}(x)\)</span> (for discrete random variables). Note that, throughout the book, the notation <span class="math inline">\(L(\theta|\mathbf{x})\)</span> has also been used for the likelihood function, and we will use both interchangeably in this chapter.</p>
<p><strong>Special Case: Independent and Identically Distributed Observations.</strong> Oftentimes, in many problems and real-world applications, the observations are assumed to be iid. If they are, then we can easily write the likelihood function as:</p>
<p><span class="math display">\[
f_{\mathbf{X}|\Theta=\theta}(\mathbf{x}) = \prod_{i=1}^n f_{X_i|\Theta=\theta}(x_i) \quad \text{ or } \quad p_{\mathbf{X}|\Theta=\theta}(\mathbf{x}) = \prod_{i=1}^n p_{X_i|\Theta=\theta}(x_i).
\]</span></p>
</div>
<div id="S:Sec923" class="section level3 hasAnchor" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> Prior Distribution<a href="ChBayes.html#S:Sec923" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the Bayesian paradigm, the prior distribution represents our knowledge or beliefs about the unknown parameters before we observe any data. It is a probability distribution that expresses the uncertainty about the values of the parameters. The prior distribution is typically specified by choosing a family of probability distributions and selecting specific values for its parameters.</p>
<p>The choice of prior distribution is subjective and often based on external information or previous studies. In some cases, noninformative priors can be used, which represent minimal prior knowledge or assumptions about the parameters. In other cases, informative and weakly informative priors can be used, which incorporate prior knowledge or assumptions based on external sources. The selection of the prior distribution should be carefully considered, and sensitivity analysis can be performed to assess the robustness of the results to different prior assumptions.</p>
<p><strong>Why Does It Matter?</strong> The choice of prior distribution can have a significant impact on the results of a Bayesian analysis. Different prior distributions can lead to different posterior distributions, which are the updated probability distributions for the parameters after we observe the data. Therefore, it is important to choose a prior distribution that reflects our prior knowledge or beliefs about the parameters.</p>
<div id="informative-and-weakly-informative-priors" class="section level4 unnumbered hasAnchor">
<h4>Informative and Weakly Informative Priors<a href="ChBayes.html#informative-and-weakly-informative-priors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><a href="#" class="tooltip" style="color:green"><em>Informative</em><span style="font-size:8pt">An informative prior, in statistics, is a prior probability distribution that is chosen deliberately to incorporate specific information or beliefs about a parameter before observing new data.</span></a> and <a href="#" class="tooltip" style="color:green"><em>weakly informative</em><span style="font-size:8pt">A weakly informative prior is a prior probability distribution that introduces some general constraints or vague beliefs about a parameter, without heavily influencing the final inference.</span></a> priors are terms used to describe the amount of prior knowledge or beliefs that is incorporated into a statistical model. Informative priors contain substantial prior knowledge about the parameters of a model, while weakly informative priors contain moderate prior knowledge.</p>
<p>Informative priors are useful when there is strong, potentially subjective prior information available about the model parameters, which can help to constrain the posterior distribution and improve inference. For example, in an insurance claims analysis study, an informative prior may be used to incorporate previous knowledge, such as the results of a previous claims study.</p>
<p>On the other hand, weakly informative priors are used when there is some—yet little—prior knowledge available or when the goal is to allow the data to drive the analysis. Weakly informative priors are designed to mildly impact the posterior distribution and are often chosen based on principles such as symmetry or scale invariance.</p>
<p>Overall, the choice of prior depends on the specific problem at hand and the available prior knowledge or beliefs. Informative priors can be useful when prior information is available and can improve the precision of the posterior distribution. In contrast, weakly informative priors can be useful when the goal is to allow the data to drive the analysis and avoid imposing strong prior assumptions.</p>
<hr />
<p><strong>Example 9.2.3. Actuarial Exam Question.</strong> You are given:</p>
<ul>
<li>Annual claim frequencies follow a Poisson distribution with mean <span class="math inline">\(\lambda\)</span>.</li>
<li>The prior distribution of <span class="math inline">\(\lambda\)</span> has the following pdf:
<span class="math display">\[
f_{\Lambda}(\lambda) = (0.3) \frac{1}{6} e^{-\frac{\lambda}{6}} + (0.7) \frac{1}{12} e^{-\frac{\lambda}{12}}, \quad \text{where } \lambda &gt; 0.
\]</span></li>
</ul>
<p>Ten claims are observed for an insured in Year 1. Calculate the expected value of the posterior distribution of <span class="math inline">\(\lambda\)</span>.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a></p>
<h5 style="text-align: center;">
<a id="displayExample.9.2.3" href="javascript:toggleEX('toggleExample.9.2.3','displayExample.9.2.3');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.2.3" style="display: none">
<p><em>Example Solution.</em> The posterior distribution can be found from:
<span class="math display">\[\begin{align*}
f_{\Lambda|X=10}(\lambda) \, = &amp; \frac{p_{X|\Lambda=\lambda}(10) f_{\Lambda}(\lambda)}{p_{X}(10)} \\
= &amp; \, \frac{\frac{e^{-\lambda}\lambda^{10}}{10!} \left( (0.3) \frac{1}{6} e^{-\frac{\lambda}{6}} + (0.7) \frac{1}{12} e^{-\frac{\lambda}{12}} \right)}{\int_0^{\infty} \! \frac{e^{-\lambda}\lambda^{10}}{10!} \left( (0.3) \frac{1}{6} e^{-\frac{\lambda}{6}} + (0.7) \frac{1}{12} e^{-\frac{\lambda}{12}} \right) \, d\lambda} \\
= &amp;\, \frac{\lambda^{10} \left( \frac{0.3}{6} e^{-\frac{7\lambda}{6}} + \frac{0.7l}{12} e^{-\frac{13\lambda}{12}} \right)}{121050} .
\end{align*}\]</span>
The posterior mean is therefore given by
<span class="math display">\[\begin{align*}
\mathrm{E}\left[\Lambda \, \middle|\, X = 10\right] = &amp;\, \frac{1}{121050} \int_0^{\infty} \! \lambda^{11} \left( \frac{0.3}{6} e^{-\frac{7\lambda}{6}} + \frac{0.7}{12} e^{-\frac{13\lambda}{12}} \right) \, d\lambda \\
= &amp;\, \frac{1}{118170} \left( \frac{0.3}{6}(11!)(6/7)^{12} + \frac{0.7}{12}(11!)(12/13)^{12} \right) \\
= &amp;\, 9.95442.
\end{align*}\]</span></p>
</div>
<hr />
</div>
<div id="noninformative-priors" class="section level4 unnumbered hasAnchor">
<h4>Noninformative Priors<a href="ChBayes.html#noninformative-priors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>It is possible to take the idea of weakly informative priors to the extreme by using <a href="#" class="tooltip" style="color:green"><em>noninformative</em><span style="font-size:8pt">A noninformative prior is a prior probability distribution that intentionally avoids incorporating specific information or strong beliefs about a parameter.</span></a> priors. A noninformative prior is a prior distribution that is intentionally chosen to allow the data to have a more decisive influence on the posterior distribution rather than being overly influenced by prior beliefs or assumptions.</p>
<p>Noninformative priors can take different forms, such as flat priors, for instance. A flat prior assigns equal probability to all possible parameter values without additional information or assumptions.</p>
<hr />
<p><strong>Example 9.2.4. Informative Versus Noninformative Priors.</strong>
You wish to investigate the impact of having informative and noninformative priors on a claim frequency analysis. Assume that the claim frequency for each policy follows a Bernoulli random variable with a probability of <span class="math inline">\(q\)</span> such that
<span class="math display">\[
q_{X_i|Q=q}(x_i) = q^{x_i} (1-q)^{1-x_i}, \quad x_i \in \{0,1\},
\]</span>
where <span class="math inline">\(q \in [0,1]\)</span>, and consider two different prior distributions:</p>
<ul>
<li>Informative: Based on past experience, you know that the claim probability is typically less than 5%, thus justifying the use of a uniform distribution over <span class="math inline">\([0,0.05]\)</span>.</li>
<li>Noninformative: You do not wish your posterior distribution to be impacted by your prior assumption and simply select a uniform distribution over the domain of <span class="math inline">\(q\)</span>, which is <span class="math inline">\([0,1]\)</span>.</li>
</ul>
<p>Using the first 100 lines of the Singapore insurance dataset (see Example 9.1.4 for more details on this dataset), find the two posterior distributions as well as the posterior expected value of the probability <span class="math inline">\(q\)</span> under both prior assumptions.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.2.4" href="javascript:toggleEX('toggleExample.9.2.4','displayExample.9.2.4');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.2.4" style="display: none">
<p><em>Example Solution.</em> Let us start with the informative prior, where
<span class="math display">\[
f_Q(q)= \frac{1}{0.05 - 0} = 20, \quad \text{ if }\, q \in [0, 0.05],
\]</span>
and zero otherwise. In this case, assuming <span class="math inline">\(x = \sum_{i=1}^{100} x_i\)</span>, the posterior density is given by
<span class="math display">\[\begin{align*}
f_{Q|\mathbf{X}=\mathbf{x}} (q) \propto &amp;\, f_{\mathbf{X}|Q=q}(\mathbf{x}) f_Q(q) \\
\propto&amp; \, \prod_{i=1}^{100} q^{x_i} (1-q)^{1-x_i} \\
= &amp;\, q^{x} (1-q)^{100-x}, \quad \text{ if }\, 0 \leq q \leq 0.05,
\end{align*}\]</span>
and zero otherwise. We can numerically obtain the shape of this posterior distribution by dividing <span class="math inline">\(q^{x} (1-q)^{100-x}\)</span> by
<span class="math display">\[
\int_0^{0.05} \! q^{x} (1-q)^{100-x} \, dq.
\]</span>
Note that this prior makes it impossible for the estimated frequency to be greater than 0.05.</p>
<p>The second prior is still uniform, but over <span class="math inline">\([0,1]\)</span> this time, which is given mathematically by
<span class="math display">\[
f_Q(q)= \frac{1}{1 - 0} = 1, \quad \text{ if }\, q \in [0,1],
\]</span>
and zero otherwise, leading to the following posterior distribution:
<span class="math display">\[\begin{align*}
f_{Q|\mathbf{X}=\mathbf{x}} (q) \propto &amp;\, f_{\mathbf{X}|Q=q}(\mathbf{x}) f_Q(q) \\
\propto &amp; \, \prod_{i=1}^{100} q^{x_i} (1-q)^{1-x_i} \\
= &amp;\, q^{x} (1-q)^{100-x}, \quad \text{ if }\, 0 \leq q \leq 1,
\end{align*}\]</span>
and zero otherwise.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="ChBayes.html#cb128-1" tabindex="-1"></a>qs <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="fl">0.12</span>, <span class="at">by =</span> <span class="fl">0.0001</span>)</span>
<span id="cb128-2"><a href="ChBayes.html#cb128-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>Clm_Count[<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>])</span>
<span id="cb128-3"><a href="ChBayes.html#cb128-3" tabindex="-1"></a></span>
<span id="cb128-4"><a href="ChBayes.html#cb128-4" tabindex="-1"></a>integrandposterior1 <span class="ot">&lt;-</span> <span class="cf">function</span>(q) {</span>
<span id="cb128-5"><a href="ChBayes.html#cb128-5" tabindex="-1"></a>    q<span class="sc">^</span>x <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> q)<span class="sc">^</span>(<span class="dv">100</span> <span class="sc">-</span> x) <span class="sc">*</span> <span class="fu">ifelse</span>(q <span class="sc">&gt;=</span> <span class="dv">0</span> <span class="sc">&amp;</span> q <span class="sc">&lt;=</span> <span class="fl">0.05</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb128-6"><a href="ChBayes.html#cb128-6" tabindex="-1"></a>}</span>
<span id="cb128-7"><a href="ChBayes.html#cb128-7" tabindex="-1"></a>marglikelihood1 <span class="ot">&lt;-</span> <span class="fu">integrate</span>(integrandposterior1, <span class="dv">0</span>, <span class="dv">1</span>, <span class="at">abs.tol =</span> .Machine<span class="sc">$</span>double.eps<span class="sc">^</span><span class="dv">2</span>)<span class="sc">$</span>value</span>
<span id="cb128-8"><a href="ChBayes.html#cb128-8" tabindex="-1"></a>posterior1 <span class="ot">&lt;-</span> <span class="fu">integrandposterior1</span>(qs)<span class="sc">/</span>marglikelihood1</span>
<span id="cb128-9"><a href="ChBayes.html#cb128-9" tabindex="-1"></a></span>
<span id="cb128-10"><a href="ChBayes.html#cb128-10" tabindex="-1"></a>integrandposterior2 <span class="ot">&lt;-</span> <span class="cf">function</span>(q) {</span>
<span id="cb128-11"><a href="ChBayes.html#cb128-11" tabindex="-1"></a>    q<span class="sc">^</span>x <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> q)<span class="sc">^</span>(<span class="dv">100</span> <span class="sc">-</span> x) <span class="sc">*</span> <span class="fu">ifelse</span>(q <span class="sc">&gt;=</span> <span class="dv">0</span> <span class="sc">&amp;</span> q <span class="sc">&lt;=</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb128-12"><a href="ChBayes.html#cb128-12" tabindex="-1"></a>}</span>
<span id="cb128-13"><a href="ChBayes.html#cb128-13" tabindex="-1"></a>marglikelihood2 <span class="ot">&lt;-</span> <span class="fu">integrate</span>(integrandposterior2, <span class="dv">0</span>, <span class="dv">1</span>, <span class="at">abs.tol =</span> .Machine<span class="sc">$</span>double.eps<span class="sc">^</span><span class="dv">2</span>)<span class="sc">$</span>value</span>
<span id="cb128-14"><a href="ChBayes.html#cb128-14" tabindex="-1"></a>posterior2 <span class="ot">&lt;-</span> <span class="fu">integrandposterior2</span>(qs)<span class="sc">/</span>marglikelihood2</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig97"></span>
<img src="LDAEd2_files/figure-html/Fig97-1.png" alt="Posterior densities based on informative (gray) and noninformative priors (black)" width="60%" />
<p class="caption">
Figure 9.7: <strong>Posterior densities based on informative (gray) and noninformative priors (black)</strong>
</p>
</div>
<p>We also wish to obtain the expected value of <span class="math inline">\(q\)</span> for both posterior distribution. This can be obtained by numerically integrating the following equation:
<span class="math display">\[
\mathrm{E}[Q|\mathbf{X}=\mathbf{x}] = \int_0^1 \! q \, f_{Q|\mathbf{X}=\mathbf{x}} (q) \, dq.
\]</span>
</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="ChBayes.html#cb129-1" tabindex="-1"></a>integrandexpvalue1 <span class="ot">&lt;-</span> <span class="cf">function</span>(q) {</span>
<span id="cb129-2"><a href="ChBayes.html#cb129-2" tabindex="-1"></a>    <span class="fu">integrandposterior1</span>(q)<span class="sc">/</span>marglikelihood1 <span class="sc">*</span> q</span>
<span id="cb129-3"><a href="ChBayes.html#cb129-3" tabindex="-1"></a>}</span>
<span id="cb129-4"><a href="ChBayes.html#cb129-4" tabindex="-1"></a>expectedvalue1 <span class="ot">&lt;-</span> <span class="fu">integrate</span>(integrandexpvalue1, <span class="dv">0</span>, <span class="dv">1</span>, <span class="at">abs.tol =</span> .Machine<span class="sc">$</span>double.eps<span class="sc">^</span><span class="dv">2</span>)<span class="sc">$</span>value</span>
<span id="cb129-5"><a href="ChBayes.html#cb129-5" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The posterior expected value of the parameter </span><span class="sc">\n</span><span class="st"> </span></span>
<span id="cb129-6"><a href="ChBayes.html#cb129-6" tabindex="-1"></a><span class="st">    when using the informative prior is&quot;</span>,</span>
<span id="cb129-7"><a href="ChBayes.html#cb129-7" tabindex="-1"></a>    expectedvalue1)</span></code></pre></div>
<pre><code>The posterior expected value of the parameter 
 
    when using the informative prior is 0.0304525117</code></pre>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="ChBayes.html#cb131-1" tabindex="-1"></a>integrandexpvalue2 <span class="ot">&lt;-</span> <span class="cf">function</span>(q) {</span>
<span id="cb131-2"><a href="ChBayes.html#cb131-2" tabindex="-1"></a>    <span class="fu">integrandposterior2</span>(q)<span class="sc">/</span>marglikelihood2 <span class="sc">*</span> q</span>
<span id="cb131-3"><a href="ChBayes.html#cb131-3" tabindex="-1"></a>}</span>
<span id="cb131-4"><a href="ChBayes.html#cb131-4" tabindex="-1"></a>expectedvalue2 <span class="ot">&lt;-</span> <span class="fu">integrate</span>(integrandexpvalue2, <span class="dv">0</span>, <span class="dv">1</span>, <span class="at">abs.tol =</span> .Machine<span class="sc">$</span>double.eps<span class="sc">^</span><span class="dv">2</span>)<span class="sc">$</span>value</span>
<span id="cb131-5"><a href="ChBayes.html#cb131-5" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The posterior expected value of the parameter </span><span class="sc">\n</span><span class="st"> </span></span>
<span id="cb131-6"><a href="ChBayes.html#cb131-6" tabindex="-1"></a><span class="st">    when using the noninformative prior is&quot;</span>,</span>
<span id="cb131-7"><a href="ChBayes.html#cb131-7" tabindex="-1"></a>    expectedvalue2)</span></code></pre></div>
<pre><code>The posterior expected value of the parameter 
 
    when using the noninformative prior is 0.0392156863</code></pre>
<p>As one can see, these values are different, meaning that the prior distribution can have a material impact on the posterior distribution. One should therefore be careful when selecting a prior distribution.</p>
</div>
<hr />
</div>
<div id="improper-priors" class="section level4 unnumbered hasAnchor">
<h4>Improper Priors<a href="ChBayes.html#improper-priors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An <a href="#" class="tooltip" style="color:green"><em>improper</em><span style="font-size:8pt">An improper prior is a prior probability distribution that does not integrate to a finite value over the entire parameter space.</span></a> prior is a prior distribution that is not a proper probability distribution, meaning that it does not integrate (or sum) to one over the entire parameter space. Improper priors can be used in Bayesian analyses, but they require careful handling because they can lead to improper posterior distributions.</p>
<p>Improper priors are typically used when there is little or no prior information about the parameter of interest—some noninformative priors are indeed improper—and they can be thought of as representing a very diffuse or noncommittal prior belief. For instance, the uniform distribution on an infinite interval is a common choice of improper prior.</p>
<hr />
<p><strong>Example 9.2.5. Improper Prior, Proper Posterior.</strong>
Let us assume a random sample <span class="math inline">\(\mathbf{x}\)</span> of size <span class="math inline">\(n\)</span>, which is a realization of the collection of random variables <span class="math inline">\(\mathbf{X} = (X_1,X_2, ..., X_n)\)</span>. Further, assume that each random variable <span class="math inline">\(X_i\)</span> is independent and normally distributed with mean of <span class="math inline">\(\mu\)</span> and variance of 1:
<span class="math display">\[
f_{X_i|M=\mu}(x_i) = \frac{1}{\sqrt{2\pi}} \exp\left( - \frac{1}{2} \left( x_i -\mu\right)^2 \right), \quad x_i \in \mathbb{R},
\]</span>
where <span class="math inline">\(\mu\)</span> is a (random) parameter. Obtain the posterior distribution of <span class="math inline">\(\mu\)</span> assuming that its prior distribution is improper and given by <span class="math inline">\(f_M(\mu) \propto 1\)</span>, where <span class="math inline">\(\mu \in \mathbb{R}\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.2.5" href="javascript:toggleEX('toggleExample.9.2.5','displayExample.9.2.5');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.2.5" style="display: none">
<p><em>Example Solution.</em> According to Bayes’ rule, we have that
<span class="math display">\[
f_{M|\mathbf{X}=\mathbf{x}}(\mu) = \frac{f_{\mathbf{X}|M=\mu}(\mathbf{x})\, f_{M}(\mu)}{f_{\mathbf{X}}(\mathbf{x})} \propto \prod_{i=1}^n f_{X_i|M=\mu}(x_i)
\]</span>
because <span class="math inline">\(f_M(\mu) \propto 1\)</span> and <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x})\)</span> does not depend on <span class="math inline">\(\mu\)</span>. Using the equation above, we can obtain the posterior distribution by simplifying the following equation:
<span class="math display">\[\begin{align*}
f_{M|\mathbf{X}=\mathbf{x}}(\mu) \propto &amp;\, \left(\frac{1}{\sqrt{2\pi}}\right)^n \exp\left( - \frac{1}{2} \sum_{i=1}^n  \left( x_i -\mu\right)^2 \right) \\
\propto &amp;\, \exp\left( - \frac{1}{2} \left( \sum_{i=1}^n x_i^2 -2\mu \sum_{i=1}^n x_i + n \mu^2 \right) \right) \\
\propto &amp;\, \exp\left( - \frac{n}{2} \left( \frac{\sum_{i=1}^n x_i^2}{n} -\frac{2\mu \sum_{i=1}^n x_i}{n} + \mu^2 \right) \right) \\
\propto &amp;\, \exp\left( - \frac{n}{2} \left( -\frac{2\mu \sum_{i=1}^n x_i}{n} + \mu^2 \right) \right) \\
\propto &amp;\, \exp\left( - \frac{n}{2} \left(  \mu - \frac{\sum_{i=1}^n x_i}{n}\right)^2  \right) \\
\propto &amp;\, \frac{1}{\sqrt{2\pi \frac{1}{n}}} \exp\left( - \frac{1}{2} \frac{\left(  \mu - \frac{\sum_{i=1}^n x_i}{n}\right)^2}{\frac{1}{n}} \right),
\end{align*}\]</span>
which is a normal distribution with mean <span class="math inline">\(\frac{\sum_{i=1}^n x_i}{n}\)</span> and variance <span class="math inline">\(\frac{1}{n}\)</span>. Interestingly, this posterior distribution is proper even though the prior distribution was improper.</p>
</div>
<hr />
<p>Special care is needed when dealing with improper priors. Indeed, if one can derive the posterior distribution in closed form and show that it is proper—like in Example 9.2.5—it should not be a concern. On the other hand, in cases where the posterior distribution cannot be obtained in closed form, there is no assurance that the posterior will be proper and extra attention is required.</p>
</div>
<div id="choice-of-the-prior-distribution" class="section level4 unnumbered hasAnchor">
<h4>Choice of the Prior Distribution<a href="ChBayes.html#choice-of-the-prior-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The selection of a prior in Bayesian statistics is a crucial step that reflects the experimenter’s prior beliefs, knowledge, or assumptions about the parameters of interest. There are different approaches to selecting priors.</p>
<ol style="list-style-type: decimal">
<li>Informative priors are generally based on the experimenter’s subjective beliefs, knowledge, or experience. For instance, one might have a subjective belief that a parameter is likely to fall within a certain range, and this belief is formalized as an informative prior distribution.</li>
<li>Noninformative priors are chosen to be minimally informative, expressing little or no prior information about the parameters. For example, uniform priors are commonly used as noninformative priors, expressing a lack of prior preference for any particular parameter value.</li>
<li>Empirical Bayes priors rely on the data itself, combining empirical information with Bayesian methodology. This can be done by estimating a prior distribution hyperparameter by using the observed data to inform the prior distribution.</li>
<li>Priors that rely on expert elicitation involves seeking input from domain experts to inform the prior. For instance, the experimenter might have additional knowledge about the problem at hand and use a prior distribution that represents their beliefs about the parameters.</li>
</ol>
</div>
<div id="prior-sensitivity-analysis" class="section level4 unnumbered hasAnchor">
<h4>Prior Sensitivity Analysis<a href="ChBayes.html#prior-sensitivity-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Prior sensitivity analysis is an important step in Bayesian modeling processes. It refers to the examination and evaluation of the impact of different prior assumptions on the results of a statistical analysis. In other words, such analyses aim to verify the robustness of the conclusions drawn from Bayesian inference to the choice of the prior distribution. By exploring a range of plausible prior distributions, experimenters can gain insights into how much the choice of prior influences the final results and whether those conclusions remain consistent under different prior assumptions.</p>
<p>For instance, prior distributions may significantly influence the posterior estimates, leading to different conclusions. Some of these might be subjective (i.e., informative priors) or based on expert knowledge, and assessing the impact of such assumptions promotes transparency and objectivity in the analysis.</p>
</div>
</div>
</div>
<div id="S:Sec93" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Conjugate Families<a href="ChBayes.html#S:Sec93" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In Section <a href="ChBayes.html#S:Sec93">9.3</a>, you learn how to:</p>
<ul>
<li>Describe three specific classes of conjugate families.</li>
<li>Use conjugate distributions to determine posterior distributions of parameters.</li>
<li>Understand the pros and cons of conjugate family models.</li>
</ul>
<hr />
<p>In Bayesian statistics, if a posterior distribution comes from the same distribution as the prior distribution, the prior and posterior are called <a href="#" class="tooltip" style="color:green"><em>conjugate distributions</em><span style="font-size:8pt">Distributions such that the posterior and the prior come from the same family of distributions.</span></a>. Note that both posterior and prior have similar shapes but will have different parameters, generally speaking.</p>
<p><strong>But Why?</strong> Two main reasons explain why conjugate families have been so popular historically:</p>
<ol style="list-style-type: decimal">
<li>They are easy to use from a computational standpoint: posterior distributions in most conjugate families can be obtained in closed form, making this class of models easy to use even if we do not have access to computing power.</li>
<li>They tend to be easy to interpret: posterior distributions are compromises between data and prior distributions. Having both prior and posterior distributions in the same family—but with different parameters—allows us to understand and quantify how the data changed our initial assumptions.</li>
</ol>
<hr />
<div id="S:Sec931" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> The Beta–Binomial Conjugate Family<a href="ChBayes.html#S:Sec931" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first conjugate family that we investigate in this book is the beta–binomial family. Let <span class="math inline">\(\mathbf{X} = (X_1, X_2, ... X_m)\)</span> represent a sample of iid Bernoulli random variables such that
<span class="math display">\[
X_i = \begin{cases}
1 &amp; \text{if success}  \\
0 &amp; \text{if failure}
\end{cases}
,
\]</span>
with probabilities <span class="math inline">\(q\)</span> and <span class="math inline">\(1-q\)</span>, respectively. Let us further define <span class="math inline">\(x = \sum_{i=1}^m x_i\)</span> the sum of the realized successes.</p>
<p>We know from elementary probability that <span class="math inline">\(X = \sum_{i=1}^m X_i\)</span> follows a binomial distribution (i.e., the number of successes <span class="math inline">\(x\)</span> in <span class="math inline">\(m\)</span> Bernoulli trials) with unknown probability of success <span class="math inline">\(q\)</span> in <span class="math inline">\([0,1]\)</span>, similar to the coin tossing case of Example 9.1.1, such that the likelihood function is given by
<span class="math display">\[
p_{\mathbf{X}|Q=q}(\mathbf{x}) = \binom{m}{x} q^x (1-q)^{m-x}, \quad x \in \{0,1,...,m\},
\]</span>
where <span class="math inline">\(x = \sum_{i=1}^m x_i\)</span>. The latter represents our evidence. Then, we combine it with its usual conjugate prior—the beta distribution with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The pdf of the beta distribution is given as follows:
<span class="math display">\[
f_{Q}(q) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} q^{a-1} (1-q)^{b-1}, \quad q \in [0,1],
\]</span>
where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are shape parameters of the beta distribution.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></p>
<p>We can now combine the prior distribution—beta—with the likelihood function—binomial—to obtain the posterior distribution.</p>
<hr />
<p><strong>Proposition 9.3.1. Beta–Binomial Conjugate Family.</strong> Consider a sample of <span class="math inline">\(m\)</span> iid Bernoulli experiments <span class="math inline">\((X_1,X_2,...,X_m)\)</span> each with success probability <span class="math inline">\(q\)</span>. Further assume that the random variable associated with the success probability, <span class="math inline">\(Q\)</span>, has a prior that is beta with shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The posterior distribution of <span class="math inline">\(Q\)</span> is therefore given by
<span class="math display">\[
f_{Q|\mathbf{X}=\mathbf{x}}(q) = \frac{\Gamma(a+b+m)}{\Gamma(a+x)\Gamma(b+m-x)} q^{a+x-1} (1-q)^{b+m-x-1},
\]</span>
where <span class="math inline">\(x = \sum_{i=1}^m x_i\)</span>, which is a beta distribution with shape parameters <span class="math inline">\(a+x\)</span> and <span class="math inline">\(b+m-x\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.3" href="javascript:toggleTheory('toggleTheory.Theory.3','displayCode.Theory.3');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.3" style="display: none">
<hr />
<p><span class="math inline">\(\textbf{Proof}\)</span>. From Section 9.2.1, we know that
<span class="math display">\[\begin{align*}
f_{Q|\mathbf{X}=\mathbf{x}}(q) = \frac{p_{\mathbf{X}|Q=q}(\mathbf{x})\, f_{Q}(q)}{p_{\mathbf{X}}(\mathbf{x})} \propto&amp;\, \binom{m}{x} q^x (1-q)^{m-x} \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} q^{a-1}(1-q)^{b-1} \notag \\[1ex]
\propto&amp;\, q^{a+x-1} (1-q)^{b+m-x-1}. \notag
\end{align*}\]</span>
We therefore only need to find the normalizing constant that ensures that the right-hand of the equation above is a density. Interestingly, the right-hand side looks like a beta distribution; specifically,
<span class="math display">\[\begin{align*}
&amp;\,\int_0^1 \! q^{a+x-1} (1-q)^{b+m-x-1} \, dq \\[1ex]
=&amp;\, \frac{\Gamma(a+x)\Gamma(b+m-x)}{\Gamma(a+b+m)} \int_0^1 \! \frac{\Gamma(a+b+m)}{\Gamma(a+x)\Gamma(b+m-x)} q^{a+x-1} (1-q)^{b+m-x-1} \, dq \\[1ex]
=&amp;\, \frac{\Gamma(a+x)\Gamma(b+m-x)}{\Gamma(a+b+m)},
\end{align*}\]</span>
and
<span class="math display">\[
f_{Q|\mathbf{X}=\mathbf{x}}(q) = \frac{\Gamma(a+b+m)}{\Gamma(a+x)\Gamma(b+m-x)} q^{a+x-1} (1-q)^{b+m-x-1} .
\]</span></p>
<hr />
</div>
<hr />
<p><strong>Parameters Versus Hyperparameters.</strong> In this context, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are called <a href="#" class="tooltip" style="color:green"><em>hyperparameters</em><span style="font-size:8pt">Hyperparameters are parameters that define the distribution of a prior distribution</span></a>—parameters of the prior. These are different from parameters of the underlying model (i.e., <span class="math inline">\(q\)</span> in the beta–binomial family). Hyperparameters are typically assumed and determined by the experimenter, whereas the underlying model parameters are random in the Bayesian context.</p>
<hr />
<p><strong>Example 9.3.1. Actuarial Exam Question.</strong> You are given:</p>
<ul>
<li><p>The annual number of claims in Year <span class="math inline">\(i\)</span> for a policyholder has a binomial distribution with pmf
<span class="math display">\[
p_{X_i|Q=q}(x_i) = \binom{2}{x} q^{x_i} (1-q)^{2-x_i}, \quad x_i \in \{ 0, 1, 2\}.
\]</span></p></li>
<li><p>The prior distribution is
<span class="math display">\[
f_Q(q) = 4 q^3, \quad q \in [0,1].
\]</span>
The policyholder had one claim in each of Years 1 and 2. Calculate the Bayesian estimate of the expected number of claims in Year 3.<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a></p></li>
</ul>
<h5 style="text-align: center;">
<a id="displayExample.9.3.1" href="javascript:toggleEX('toggleExample.9.3.1','displayExample.9.3.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.3.1" style="display: none">
<p><em>Example Solution.</em> The likelihood function based on this policyholder’s number of claims in Years 1 and 2 is given by:
<span class="math display">\[
p_{\mathbf{X}|Q=q}(\mathbf{x}) = p_{X_1|Q=q}(1) \, p_{X_2|Q=q}(1) = \binom{2}{1} q^{1} (1-q)^{1} \, \binom{2}{1} q^{1} (1-q)^{1} \propto q^{2} (1-q)^2,
\]</span>
which is proportional to a binomial pmf with <span class="math inline">\(m=4\)</span>, two successes, and a success probability of <span class="math inline">\(q\)</span>. Because the prior distribution is beta distributed with <span class="math inline">\(a=4\)</span> and <span class="math inline">\(b=1\)</span>, we know that the posterior distribution of parameter <span class="math inline">\(q\)</span> is given by
<span class="math display">\[\begin{align*}
f_{Q|\mathbf{X}=\mathbf{x}}(q) =&amp;\, \frac{\Gamma(4+1+4)}{\Gamma(4+2)\Gamma(1+4-2)} q^{4+2-1} (1-q)^{1+4-2-1}  \notag \\
=&amp;\, \frac{\Gamma(9)}{\Gamma(6)\Gamma(3)} q^{5} (1-q)^{2}  \notag \\
=&amp;\, 168 q^5 (1-q)^2, \notag
\end{align*}\]</span>
which is also a beta distribution with shape parameters 6 and 3, respectively.</p>
<p>The expected number of claim in Year 3 is
<span class="math display">\[
\mathrm{E}\left[ \mathrm{E} \left[ X_3 \, \middle|\, Q=q \, \right] \, \middle|\, X_1, X_2 \right] = \mathrm{E}\left[ 2 q \, \middle|\, X_1, X_2\right] = 2  \, \mathrm{E}\left[ q \, \middle|\, X_1, X_2\right],
\]</span>
and <span class="math inline">\(\mathrm{E}\left[ q \, \middle|\, X_1, X_2\right]\)</span> is the expected value of the beta distribution, which is given by
<span class="math display">\[
\mathrm{E}\left[ q \, \middle|\, X_1, X_2\right] = \frac{6}{6+3} = \frac{2}{3}.
\]</span>
Ultimately, this leads to an expected number of claim in Year 3 of <span class="math inline">\(2 \left(\tfrac{2}{3} \right) = \tfrac{4}{3}\)</span>.</p>
</div>
<hr />
<p><strong>Example 9.3.2. Impact of Beta Prior on Posterior.</strong> You wish to investigate the impact of having different beta hyperparameters on the posterior distribution. Assume that the claim frequency for each policy follows a Bernoulli random variable with a probability of <span class="math inline">\(q\)</span> such that
<span class="math display">\[
p_{X_i|Q=q}(x_i) = q^{x_i} (1-q)^{1-x_i}, \quad x_i \in \{0,1\},
\]</span>
where <span class="math inline">\(q \in [0,1]\)</span>, and consider two different sets of hyperparameters:</p>
<ul>
<li>Set 1: <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b=10\)</span>.</li>
<li>Set 2: <span class="math inline">\(a = 2\)</span> and <span class="math inline">\(b=2\)</span>.</li>
</ul>
<p>Figure <a href="ChBayes.html#fig:Fig98">9.8</a> shows the pdf of these two prior distributions. The first prior assumes a small prior mean frequency of <span class="math inline">\(\frac{1}{11}\)</span>, whereas the second prior distribution has a mean of <span class="math inline">\(\frac{1}{2}\)</span>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig98"></span>
<img src="LDAEd2_files/figure-html/Fig98-1.png" alt="Beta prior densities: \(a=1\) and \(b=10\) (gray), and \(a=2\) and \(b=2\) (black)" width="80%" />
<p class="caption">
Figure 9.8: <strong>Beta prior densities: <span class="math inline">\(a=1\)</span> and <span class="math inline">\(b=10\)</span> (gray), and <span class="math inline">\(a=2\)</span> and <span class="math inline">\(b=2\)</span> (black)</strong>
</p>
</div>
<p>Using again the first 100 lines of the Singapore insurance dataset (see Example 9.1.4 for more details on this dataset), find the two posterior distributions.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.3.2" href="javascript:toggleEX('toggleExample.9.3.2','displayExample.9.3.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.3.2" style="display: none">
<p><em>Example Solution.</em> The likelihood function associated with the observations is given by
<span class="math display">\[
p_{\mathbf{X}|Q=q}(\mathbf{x}) = \binom{100}{x} \, q^{x} (1-q)^{100-x}, \quad \text{where }\, x = \sum_{i=1}^{100} x_i,
\]</span>
as mentioned already in Example 9.2.4. Combining this likelihood with a beta prior gives a beta posterior:
<span class="math display">\[
f_{Q|\mathbf{X}=\mathbf{x}}(q) = \frac{\Gamma(a+b+100)}{\Gamma\left(a+x\right)\, \Gamma\left(b+100-x\right)} \, q^{a+x-1} (1-q)^{b+100-x-1},
\]</span></p>
<p>that can be evaluated for various values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Figure 9.9 reports the two posterior distributions associated with the priors mentioned above.</p>

<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="ChBayes.html#cb133-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>Clm_Count[<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>])</span>
<span id="cb133-2"><a href="ChBayes.html#cb133-2" tabindex="-1"></a></span>
<span id="cb133-3"><a href="ChBayes.html#cb133-3" tabindex="-1"></a>posterior1 <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(qs, <span class="at">shape1 =</span> <span class="dv">1</span> <span class="sc">+</span> x, <span class="at">shape2 =</span> <span class="dv">10</span> <span class="sc">+</span> <span class="dv">100</span> <span class="sc">-</span> x)</span>
<span id="cb133-4"><a href="ChBayes.html#cb133-4" tabindex="-1"></a>posterior2 <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(qs, <span class="at">shape1 =</span> <span class="dv">2</span> <span class="sc">+</span> x, <span class="at">shape2 =</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">100</span> <span class="sc">-</span> x)</span>
<span id="cb133-5"><a href="ChBayes.html#cb133-5" tabindex="-1"></a></span>
<span id="cb133-6"><a href="ChBayes.html#cb133-6" tabindex="-1"></a>dataposterior <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> qs, <span class="at">y1 =</span> posterior1, <span class="at">y2 =</span> posterior2)</span>
<span id="cb133-7"><a href="ChBayes.html#cb133-7" tabindex="-1"></a></span>
<span id="cb133-8"><a href="ChBayes.html#cb133-8" tabindex="-1"></a><span class="fu">ggplot</span>(dataposterior, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y1)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;darkgray&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb133-9"><a href="ChBayes.html#cb133-9" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> y2), <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span> <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="dv">35</span>) <span class="sc">+</span></span>
<span id="cb133-10"><a href="ChBayes.html#cb133-10" tabindex="-1"></a>    <span class="fu">xlab</span>(<span class="fu">expression</span>(<span class="fu">italic</span>(<span class="st">&quot;q&quot;</span>))) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Posterior density&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig99"></span>
<img src="LDAEd2_files/figure-html/Fig99-1.png" alt="Posterior densities based on two different priors: \(a=1\) and \(b=10\) (gray), and \(a=2\) and \(b=2\) (black)" width="80%" />
<p class="caption">
Figure 9.9: <strong>Posterior densities based on two different priors: <span class="math inline">\(a=1\)</span> and <span class="math inline">\(b=10\)</span> (gray), and <span class="math inline">\(a=2\)</span> and <span class="math inline">\(b=2\)</span> (black)</strong>
</p>
</div>
</div>
<hr />
<p>The prior distribution (and its hyperparameters) clearly have an impact on the posterior distribution. As a general rule of thumb for the beta prior, a higher <span class="math inline">\(a\)</span> puts more weight on higher values of <span class="math inline">\(q\)</span> and a higher <span class="math inline">\(b\)</span> puts more weight on lower values of <span class="math inline">\(q\)</span>.</p>
</div>
<div id="S:Sec932" class="section level3 hasAnchor" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> The Gamma–Poisson Conjugate Family<a href="ChBayes.html#S:Sec932" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now present a second conjugate family: the gamma–Poisson family. Let <span class="math inline">\(\mathbf{X} = (X_1, X_2, ..., X_n)\)</span> be a sample of iid Poisson random variables such that
<span class="math display">\[
p_{X_i|\Lambda=\lambda}(x_i) = \frac{\lambda^{x_i}\, e^{-\lambda}}{x_i!}, \quad x_i \in \mathbb{R}_+ .
\]</span>
The likelihood function associated with this sample would therefore be given by
<span class="math display">\[
f_{\mathbf{X}|\Lambda=\lambda}(\mathbf{x}) = \prod_{i=1}^n p_{X_i|\Lambda=\lambda}(x_i) = \prod_{i=1}^n \frac{\lambda^{x_i}\, e^{-\lambda}}{x_i!} = \frac{\lambda^x \, e^{-n\lambda}}{\prod_{i=1}^n x_i !} \propto \lambda^x \, e^{-n\lambda},
\]</span>
where <span class="math inline">\(x = \sum_{i=1}^n x_i\)</span>. The shape of this likelihood function, as a function of <span class="math inline">\(\lambda\)</span>, is reminiscent of a gamma distribution, hinting to the fact that this distribution would be a good contender for a conjugate prior. Indeed, if we let the prior distribution be gamma with shape hyperparameter <span class="math inline">\(\alpha\)</span> and scale hyperparameter <span class="math inline">\(\theta\)</span>,
<span class="math display">\[
f_{\Lambda}(\lambda) = \frac{1}{\Gamma(\alpha)\theta^{\alpha}} \lambda^{\alpha-1} \, e^{-\frac{\lambda}{\theta}}, \quad \lambda \in \mathbb{R}_+,
\]</span>
we can show that the posterior distribution of <span class="math inline">\(\lambda\)</span> is also gamma.</p>
<hr />
<p><strong>Proposition 9.3.2. Gamma–Poisson Conjugate Family.</strong> Consider a sample of <span class="math inline">\(n\)</span> iid Poisson experiments <span class="math inline">\((X_1,X_2,...,X_n)\)</span>, each with rate parameter <span class="math inline">\(\lambda\)</span>. Further assume that the random variable associated with the rate, <span class="math inline">\(\Lambda\)</span>, has a prior that is gamma distributed with shape hyperparameter <span class="math inline">\(\alpha\)</span> and scale hyperparameter <span class="math inline">\(\theta\)</span>. The posterior distribution of <span class="math inline">\(\Lambda\)</span> is therefore given by
<span class="math display">\[
f_{\Lambda|\mathbf{X}=\mathbf{x}}(\lambda) = \frac{1}{\Gamma(\alpha+x)\left(\frac{\theta}{n\theta+1} \right)^{\alpha+x}} \lambda^{\alpha+x-1} \, e^{-\frac{\lambda\,(n\theta+1)}{\theta}},
\]</span>
where <span class="math inline">\(x = \sum_{i=1}^n x_i\)</span>, which is a gamma distribution with shape parameter <span class="math inline">\(\alpha+x\)</span> and scale parameter <span class="math inline">\(\tfrac{\theta}{n\theta+1}\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.4" href="javascript:toggleTheory('toggleTheory.Theory.4','displayCode.Theory.4');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.4" style="display: none">
<hr />
<p><span class="math inline">\(\textbf{Proof}\)</span>. From Section 9.2.1, we know that
<span class="math display">\[\begin{align*}
f_{\Lambda|\mathbf{X}=\mathbf{x}}(\lambda) = \frac{p_{\mathbf{X}|\Lambda=\lambda}(\mathbf{x})\, f_{\Lambda}(\lambda)}{p_{\mathbf{X}}(\mathbf{x})} \propto &amp;\, \lambda^x \, e^{-n\lambda} \, \lambda^{\alpha-1} \, e^{-\frac{\lambda}{\theta}} \notag \\
\propto&amp;\, \lambda^{\alpha+x-1} \, e^{-\frac{\lambda (n\theta+1)}{\theta}}, \notag
\end{align*}\]</span>
where <span class="math inline">\(x = \sum_{i=1}^n x_i\)</span>. We therefore only need to find the normalizing constant that ensures that the right-hand of the equation above is a density. Interestingly, the right-hand side looks like a gamma distribution; specifically, <span class="math display">\[\begin{align*}
&amp;\,\int_0^{\infty} \! \lambda^{\alpha+x-1} \, e^{-\frac{\lambda (n\theta+1)}{\theta}} \, d\lambda \\
=&amp;\, \Gamma(\alpha+x)\left(\frac{\theta}{n\theta+1} \right)^{\alpha+x}  \int_0^{\infty} \!\frac{1}{\Gamma(\alpha+x)\left(\frac{\theta}{n\theta+1} \right)^{\alpha+x}}  \, \lambda^{\alpha+x-1} \, e^{-\frac{\lambda (n\theta+1)}{\theta}} \, d\lambda \\
=&amp;\, \Gamma(\alpha+x)\left(\frac{\theta}{n\theta+1} \right)^{\alpha+x},
\end{align*}\]</span>
and
<span class="math display">\[
f_{\Lambda|\mathbf{X}=\mathbf{x}}(\lambda) = \frac{1}{\Gamma(\alpha+x)\left(\frac{\theta}{n\theta+1} \right)^{\alpha+x}} \lambda^{\alpha+x-1} \, e^{-\frac{\lambda\,(n\theta+1)}{\theta}}.
\]</span></p>
<hr />
</div>
<hr />
<p><strong>Example 9.3.3. Actuarial Exam Question.</strong> You are given:</p>
<ul>
<li>The number of claims incurred in a month by any insured has a Poisson distribution with mean <span class="math inline">\(\lambda\)</span>.</li>
<li>The claim frequencies of different insured are iid.</li>
<li>The prior distribution is gamma with pdf
<span class="math display">\[
f_{\Lambda}(\lambda) = \frac{(100\lambda)^6}{120\lambda}e^{-100\lambda}, \quad \lambda \in \mathbb{R}_+.
\]</span></li>
<li>The number of claims every month is distributed as follows:
<span class="math display">\[
\small{
\begin{matrix}
  \begin{array}{c|c|c} \hline
  \text{Month} &amp; \text{Number of Insured} &amp; \text{Number of Claims} \\
  \hline
  \text{1} &amp; 100 &amp; 6 \\\hline
  \text{2} &amp; 150 &amp; 8 \\\hline
  \text{3} &amp; 200 &amp; 11 \\\hline
  \text{4} &amp; 300 &amp; ? \\\hline
  \end{array}
\end{matrix}
}
\]</span></li>
</ul>
<p>Calculate the expected number of claims in Month 4.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.3.3" href="javascript:toggleEX('toggleExample.9.3.3','displayExample.9.3.3');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.3.3" style="display: none">
<p><em>Example Solution.</em> The likelihood function based on this policyholder’s number of claims in Months 1, 2, and 3 is given by:
<span class="math display">\[
p_{\mathbf{X}|\Lambda = \lambda}(\mathbf{x}) = p_{X_1|\Lambda = \lambda}(6) \, p_{X_2|\Lambda = \lambda}(8) \, p_{X_3|\Lambda = \lambda}(11) \propto \lambda^{6+8+11}\, e^{-\lambda(100+150+200)}.
\]</span>
Because the prior distribution is gamma distributed with <span class="math inline">\(\alpha=6\)</span> and <span class="math inline">\(\theta=\tfrac{1}{100}\)</span>, we know that the posterior distribution of parameter <span class="math inline">\(\lambda\)</span> is also gamma distributed with shape parameter
<span class="math display">\[
\alpha + x = 6+6+8+11 = 31
\]</span>
and scale parameter
<span class="math display">\[
\frac{\theta}{n\theta+1} = \frac{\frac{1}{100}}{(100+150+200)\frac{1}{100}+1} = \frac{1}{550}.
\]</span>
The expected number of claim in Month 4 conditional on the information of Months 1, 2, and 3 is
<span class="math display">\[
\mathrm{E}\left[ \mathrm{E} \left[ X_4 \, \middle|\, \Lambda=\lambda \, \right]  \, \middle|\, X_1, X_2, X_3 \right] = \mathrm{E}\left[ 300 \lambda \, \middle|\, X_1, X_2, X_3 \right] = 300  \, \mathrm{E}\left[ \lambda \, \middle|\, X_1, X_2, X_3\right],
\]</span></p>
<p>and <span class="math inline">\(\mathrm{E}\left[ \lambda \, \middle|\, X_1, X_2, X_3\right]\)</span> is the expected value of the posterior distribution, which is given by
<span class="math display">\[
\mathrm{E}\left[ \lambda \, \middle|\, X_1, X_2, X_3 \right] = \frac{31}{550}.
\]</span>
Ultimately, this leads to an expected number of claim in Month 4 of <span class="math inline">\(300 \left(\tfrac{31}{550} \right) = \tfrac{930}{55} \approx 16.91\)</span>.</p>
</div>
<hr />
</div>
<div id="S:Sec933" class="section level3 hasAnchor" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> The Normal–Normal Conjugate Family<a href="ChBayes.html#S:Sec933" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The last conjugate family is the normal–normal family. Let <span class="math inline">\(\mathbf{X}=(X_1,X_2,...,X_n)\)</span> be a sample of iid normal random variables such that
<span class="math display">\[
f_{X_i|M=\mu}(x_i) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left( - \frac{1}{2} \frac{(x_i-\mu)^2}{\sigma^2} \right), \quad x_i \in \mathbb{R}.
\]</span>
Further, to keep our focus on <span class="math inline">\(\mu\)</span>, we will assume throughout our analysis that the variance parameter <span class="math inline">\(\sigma^2\)</span> is known.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a> The likelihood function associated with this sample would therefore be given by
<span class="math display">\[\begin{align*}
f_{\mathbf{X}|M=\mu}(\mathbf{x}) =&amp;\, \prod_{i=1}^n f_{X_i|M=\mu}(x_i)  \\
=&amp;\, \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n \exp\left( - \frac{1}{2} \sum_{i=1}^n \frac{(x_i-\mu)^2}{\sigma^2} \right) \\
\propto&amp;\, \exp\left( - \frac{1}{2} \sum_{i=1}^n \frac{(x_i-\mu)^2}{\sigma^2} \right).
\end{align*}\]</span></p>
<p>A very natural prior distribution that matches the likelihood structure is unsurprisingly the normal distribution. Let us assume that the prior distribution for <span class="math inline">\(\mu\)</span> is given by
<span class="math display">\[
f_{M}(\mu) = \frac{1}{\sqrt{2\pi \tau^2}} \exp\left( -\frac{1}{2} \frac{(\mu-\theta)^2}{\tau^2} \right),
\]</span>
where <span class="math inline">\(\theta\)</span> is the mean parameter and <span class="math inline">\(\tau^2\)</span> is the variance parameter. We can then easily show that the posterior distribution of <span class="math inline">\(\mu\)</span> is also given by a normal distribution.</p>
<hr />
<p><strong>Proposition 9.3.3. Normal–Normal Conjugate Family.</strong>
Consider a sample of <span class="math inline">\(n\)</span> iid normals <span class="math inline">\((X_1,X_2,...,X_n)\)</span>, each with mean parameter <span class="math inline">\(\mu\)</span> and variance parameter <span class="math inline">\(\sigma^2\)</span> that is known. Further assume that the random variable associated with the mean, <span class="math inline">\(M\)</span>, has a prior that is normally distributed with mean hyperparameter <span class="math inline">\(\theta\)</span> and variance hyperparameter <span class="math inline">\(\tau^2\)</span>. The posterior distribution of <span class="math inline">\(M\)</span> is therefore given by
<span class="math display">\[
f_{M|\mathbf{X}=\mathbf{x}}(\mu) = \frac{1}{\sqrt{2\pi\left( \frac{\tau^2\sigma^2}{n\tau^2 +\sigma^2} \right)} } \exp\left( - \frac{1}{2} \frac{ \left( \mu - \left( \frac{x}{n} \frac{\tau^2}{n\tau^2 +\sigma^2} + \theta \frac{\sigma^2}{n\tau^2 +\sigma^2}  \right) \right)^2 }{ \frac{\tau^2\sigma^2}{n\tau^2 +\sigma^2} } \right),
\]</span>
where <span class="math inline">\(x = \sum_{i=1}^n x_i\)</span>, which is a normal distribution with mean parameter
<span class="math display">\[
\frac{x}{n} \frac{n\tau^2}{n\tau^2 +\sigma^2} + \theta \frac{\sigma^2}{n\tau^2 +\sigma^2}
\]</span>
and variance parameter
<span class="math display">\[
\frac{\tau^2\sigma^2}{n\tau^2 +\sigma^2}.
\]</span></p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.5" href="javascript:toggleTheory('toggleTheory.Theory.5','displayCode.Theory.5');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.5" style="display: none">
<hr />
<p><span class="math inline">\(\textbf{Proof}\)</span>. From Section 9.2.1, we know that
<span class="math display">\[\begin{align*}
f_{M|\mathbf{X}=\mathbf{x}}(\mu) = &amp;\, \frac{f_{\mathbf{X}|M=\mu}(\mathbf{x})\, f_{M}(\mu)}{f_{\mathbf{X}}(\mathbf{x})} \\
\propto &amp;\,
\exp\left( - \frac{1}{2} \sum_{i=1}^n \frac{(x_i-\mu)^2}{\sigma^2} \right) \exp\left( -\frac{1}{2} \frac{(\mu-\theta)^2}{\tau^2} \right) \notag \\
\propto&amp;\, \exp\left( - \frac{1}{2} \frac{\sum_{i=1}^n x_i^2 - 2 \mu x + n \mu^2 }{\sigma^2} - \frac{1}{2} \frac{\mu^2-2\mu\theta + \theta^2}{\tau^2} \right) \notag \\
\propto&amp;\, \exp\left( - \frac{1}{2} \frac{ n \mu^2 - 2 \mu x}{\sigma^2} - \frac{1}{2} \frac{\mu^2-2\mu\theta}{\tau^2} \right) \notag \\
\propto&amp;\, \exp\left( - \frac{1}{2} \frac{  \mu^2\left(n \tau^2 + \sigma^2\right) - 2 \mu \tau^2 x -2\mu \sigma^2 \theta}{\tau^2\sigma^2 } \right) \notag \\
\propto&amp;\, \exp\left( - \frac{1}{2} \frac{ \mu^2 - 2 \mu \left(  x \frac{\tau^2}{n \tau^2 + \sigma^2} + \theta \frac{\sigma^2}{n \tau^2 + \sigma^2} \right) }{\frac{\tau^2\sigma^2}{n \tau^2 + \sigma^2} } \right) \notag \\
\propto&amp;\, \frac{1}{\sqrt{2\pi\left( \frac{\tau^2\sigma^2}{n\tau^2 +\sigma^2} \right)} } \exp\left( - \frac{1}{2} \frac{ \left( \mu - \left( \frac{x}{n} \frac{n\tau^2}{n\tau^2 +\sigma^2} + \theta \frac{\sigma^2}{n\tau^2 +\sigma^2} \right) \right)^2 }{ \frac{\tau^2\sigma^2}{n\tau^2 +\sigma^2} } \right) , \notag
\end{align*}\]</span>
where <span class="math inline">\(x = \sum_{i=1}^n x_i\)</span>.</p>
<hr />
</div>
<hr />
<p>The prior distribution hyperparameters and posterior distribution parameters can be interpreted in the normal–normal conjugate family:</p>
<ul>
<li>For the prior, <span class="math inline">\(\theta\)</span> represents the <em>a priori</em> value of the mean parameter, and <span class="math inline">\(\tau^2\)</span> is related to the <a href="#" class="tooltip" style="color:green"><em>precision</em><span style="font-size:8pt">Precision is the inverse of variance and is often used to quantify the amount of uncertainty or variability in a prior or posterior distribution.</span></a> of that prior mean (i.e., the larger the value, the less precise the prior mean is, and vice versa).</li>
<li>For the posterior, the new mean parameter is a weighted average between the prior mean parameter <span class="math inline">\(\theta\)</span> and the sample mean <span class="math inline">\(\tfrac{x}{n}\)</span>. The new variance parameter is informed by the prior variability <span class="math inline">\(\tau^2\)</span> and the variability of the data <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
<hr />
<p><strong>Example 9.3.4. Impact of Normal Prior on Posterior.</strong>
Assume the following observed automobile claims for a small portfolio of policies:
<span class="math display">\[
1050, \quad\quad 1250, \quad\quad 1550, \quad\quad 2600, \quad\quad 5350, \quad\quad 10200.
\]</span>
Further assume that the logarithm of the claim amount follows a normal distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span>. Find the posterior distribution of the mean parameter <span class="math inline">\(\mu\)</span> for a normal prior distribution where <span class="math inline">\(\theta = 7\)</span>. Consider different values of <span class="math inline">\(\tau^2\)</span>; that is, <span class="math inline">\(\tau^2 = 0.1\)</span>, <span class="math inline">\(\tau^2 = 1\)</span>, and <span class="math inline">\(\tau^2 = 10\)</span>. Figure <a href="ChBayes.html#fig:Fig910">9.10</a> shows the pdf of these three prior distributions.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig910"></span>
<img src="LDAEd2_files/figure-html/Fig910-1.png" alt="Normal prior densities: \(\tau^2 = 0.1\) (light gray), \(\tau^2=1\) (gray), and \(\tau^2 = 10\) (black)" width="80%" />
<p class="caption">
Figure 9.10: <strong>Normal prior densities: <span class="math inline">\(\tau^2 = 0.1\)</span> (light gray), <span class="math inline">\(\tau^2=1\)</span> (gray), and <span class="math inline">\(\tau^2 = 10\)</span> (black)</strong>
</p>
</div>
<h5 style="text-align: center;">
<a id="displayExample.9.3.4" href="javascript:toggleEX('toggleExample.9.3.4','displayExample.9.3.4');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.3.4" style="display: none">
<p><em>Example Solution.</em> Using the results of Proposition 9.3.3, we can obtain the following posterior distributions:</p>

<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="ChBayes.html#cb134-1" tabindex="-1"></a>xi <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1050</span>, <span class="dv">1250</span>, <span class="dv">1550</span>, <span class="dv">2600</span>, <span class="dv">5350</span>, <span class="dv">10200</span>)</span>
<span id="cb134-2"><a href="ChBayes.html#cb134-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">log</span>(xi))</span>
<span id="cb134-3"><a href="ChBayes.html#cb134-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(xi)</span>
<span id="cb134-4"><a href="ChBayes.html#cb134-4" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb134-5"><a href="ChBayes.html#cb134-5" tabindex="-1"></a></span>
<span id="cb134-6"><a href="ChBayes.html#cb134-6" tabindex="-1"></a>mean1 <span class="ot">&lt;-</span> theta <span class="sc">*</span> (sigma2<span class="sc">/</span>(n <span class="sc">*</span> tau21 <span class="sc">+</span> sigma2)) <span class="sc">+</span> x<span class="sc">/</span>n <span class="sc">*</span> ((n <span class="sc">*</span> tau21)<span class="sc">/</span>(n <span class="sc">*</span> tau21 <span class="sc">+</span></span>
<span id="cb134-7"><a href="ChBayes.html#cb134-7" tabindex="-1"></a>    sigma2))</span>
<span id="cb134-8"><a href="ChBayes.html#cb134-8" tabindex="-1"></a>mean2 <span class="ot">&lt;-</span> theta <span class="sc">*</span> (sigma2<span class="sc">/</span>(n <span class="sc">*</span> tau22 <span class="sc">+</span> sigma2)) <span class="sc">+</span> x<span class="sc">/</span>n <span class="sc">*</span> ((n <span class="sc">*</span> tau22)<span class="sc">/</span>(n <span class="sc">*</span> tau22 <span class="sc">+</span></span>
<span id="cb134-9"><a href="ChBayes.html#cb134-9" tabindex="-1"></a>    sigma2))</span>
<span id="cb134-10"><a href="ChBayes.html#cb134-10" tabindex="-1"></a>mean3 <span class="ot">&lt;-</span> theta <span class="sc">*</span> (sigma2<span class="sc">/</span>(n <span class="sc">*</span> tau23 <span class="sc">+</span> sigma2)) <span class="sc">+</span> x<span class="sc">/</span>n <span class="sc">*</span> ((n <span class="sc">*</span> tau23)<span class="sc">/</span>(n <span class="sc">*</span> tau23 <span class="sc">+</span></span>
<span id="cb134-11"><a href="ChBayes.html#cb134-11" tabindex="-1"></a>    sigma2))</span>
<span id="cb134-12"><a href="ChBayes.html#cb134-12" tabindex="-1"></a></span>
<span id="cb134-13"><a href="ChBayes.html#cb134-13" tabindex="-1"></a>var1 <span class="ot">&lt;-</span> (tau21 <span class="sc">*</span> sigma2)<span class="sc">/</span>(n <span class="sc">*</span> tau21 <span class="sc">+</span> sigma2)</span>
<span id="cb134-14"><a href="ChBayes.html#cb134-14" tabindex="-1"></a>var2 <span class="ot">&lt;-</span> (tau22 <span class="sc">*</span> sigma2)<span class="sc">/</span>(n <span class="sc">*</span> tau22 <span class="sc">+</span> sigma2)</span>
<span id="cb134-15"><a href="ChBayes.html#cb134-15" tabindex="-1"></a>var3 <span class="ot">&lt;-</span> (tau23 <span class="sc">*</span> sigma2)<span class="sc">/</span>(n <span class="sc">*</span> tau23 <span class="sc">+</span> sigma2)</span>
<span id="cb134-16"><a href="ChBayes.html#cb134-16" tabindex="-1"></a></span>
<span id="cb134-17"><a href="ChBayes.html#cb134-17" tabindex="-1"></a>posterior1 <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(xs, <span class="at">mean =</span> mean1, <span class="at">sd =</span> <span class="fu">sqrt</span>(var1))</span>
<span id="cb134-18"><a href="ChBayes.html#cb134-18" tabindex="-1"></a>posterior2 <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(xs, <span class="at">mean =</span> mean2, <span class="at">sd =</span> <span class="fu">sqrt</span>(var2))</span>
<span id="cb134-19"><a href="ChBayes.html#cb134-19" tabindex="-1"></a>posterior3 <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(xs, <span class="at">mean =</span> mean3, <span class="at">sd =</span> <span class="fu">sqrt</span>(var3))</span>
<span id="cb134-20"><a href="ChBayes.html#cb134-20" tabindex="-1"></a></span>
<span id="cb134-21"><a href="ChBayes.html#cb134-21" tabindex="-1"></a>dataposterior <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> xs, <span class="at">y1 =</span> posterior1, <span class="at">y2 =</span> posterior2, <span class="at">y3 =</span> posterior3)</span>
<span id="cb134-22"><a href="ChBayes.html#cb134-22" tabindex="-1"></a></span>
<span id="cb134-23"><a href="ChBayes.html#cb134-23" tabindex="-1"></a><span class="fu">ggplot</span>(dataposterior, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y1)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;lightgray&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb134-24"><a href="ChBayes.html#cb134-24" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> y2), <span class="at">color =</span> <span class="st">&quot;darkgray&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> y3),</span>
<span id="cb134-25"><a href="ChBayes.html#cb134-25" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span> <span class="fu">xlim</span>(<span class="dv">1</span>, <span class="dv">13</span>) <span class="sc">+</span> <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="fl">1.75</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="fu">expression</span>(<span class="fu">italic</span>(mu))) <span class="sc">+</span></span>
<span id="cb134-26"><a href="ChBayes.html#cb134-26" tabindex="-1"></a>    <span class="fu">ylab</span>(<span class="st">&quot;Posterior density&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig911"></span>
<img src="LDAEd2_files/figure-html/Fig911-1.png" alt="Posterior densities based on three different priors: \(\tau^2 = 0.1\) (light gray), \(\tau^2=1\) (gray), and \(\tau^2 = 10\) (black)" width="80%" />
<p class="caption">
Figure 9.11: <strong>Posterior densities based on three different priors: <span class="math inline">\(\tau^2 = 0.1\)</span> (light gray), <span class="math inline">\(\tau^2=1\)</span> (gray), and <span class="math inline">\(\tau^2 = 10\)</span> (black)</strong>
</p>
</div>
</div>
<hr />
<p>Interestingly, as shown in Example 9.3.4, the prior distribution can have some impact on the final posterior distribution. When the prior assumption about the mean is very precise, having a few data points do not create a huge gap between the prior and the posterior (see the light gray curves in Figures <a href="ChBayes.html#fig:Fig910">9.10</a> and <a href="ChBayes.html#fig:Fig911">9.11</a>). When the prior is very imprecise, on the other hand, then the data are allow to speak, and the posterior can be quite different from the prior distribution.</p>
</div>
<div id="S:Sec934" class="section level3 hasAnchor" number="9.3.4">
<h3><span class="header-section-number">9.3.4</span> Criticism of Conjugate Family Models<a href="ChBayes.html#S:Sec934" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While conjugate family models have some advantages, such as ease of interpretation and computational simplicity, they also have some limitations:</p>
<ol style="list-style-type: decimal">
<li>Conjugate families are oftentimes chosen for their mathematical convenience rather than their ability to accurately model the data under study. This can lead to models that are too simplistic and lack the flexibility needed to model real-world phenomena.</li>
<li>Conjugate family models rely on the choice of prior distribution, and different choices of possibly non-conjugate priors can lead to very different posterior distributions.</li>
<li>Conjugate family models are only applicable to a narrow range of problems, which limit their usefulness in practical applications.</li>
</ol>
<p>It is important to note that while conjugate family models have their limitations, they can still be useful in certain situations, especially when the assumptions of the model are well understood and the data are relatively simple.</p>
</div>
</div>
<div id="S:Sec94" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Posterior Simulation<a href="ChBayes.html#S:Sec94" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In Section <a href="ChBayes.html#S:Sec94">9.4</a>, you learn how to:</p>
<ul>
<li>Use the standard computational tools for Bayesian statistics.</li>
<li>Diagnose Markov chain convergence.</li>
</ul>
<hr />
<div id="S:Sec941" class="section level3 hasAnchor" number="9.4.1">
<h3><span class="header-section-number">9.4.1</span> Introduction to Markov Chain Monte Carlo Methods<a href="ChBayes.html#S:Sec941" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sometimes, using conjugate family models is ill-suited for the problem at hand, and more complicated priors need to be selected. Under other circumstances, complex models involve many parameters making the posterior distribution intractable. In these cases, the posterior distribution of the parameters will not have a closed-form solution, generally speaking, and will need to be estimated via numerical methods.</p>
<p>A common way to generate draws of the parameter posterior distribution is to create Markov chains for which their stationary distributions—the probability distribution that remains unchanged when the Markov chain has reached a state where the transition probabilities no longer evolve over time—correspond to the posterior of interest. These Markov chain-based methods are known as Markov chain Monte Carlo (MCMC) methods in the literature. This section provides a brief overview of these methods and of their uses. We do not intend to give much of the theory behind these methods, which would require a deep understanding of Markov chains and their theory.<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a> Instead, we focus on their applications in insurance and loss modeling. Specifically, in the next two subsections, we introduce the two most common MCMC methods; that is, the <a href="#" class="tooltip" style="color:green"><em>Gibbs sampler</em><span style="font-size:8pt">The Gibbs sampler is an iterative algorithm in statistics used for simulating samples from complex probability distributions. It’s particularly useful in Bayesian analysis for drawing samples from multivariate distributions by updating one variable at a time while keeping others fixed.</span></a> of <span class="citation">Gelfand and Smith (<a href="bibliography.html#ref-gelfand1990sampling">1990</a>)</span> and the <a href="#" class="tooltip" style="color:green"><em>Metropolis–Hastings algorithm</em><span style="font-size:8pt">The Metropolis–Hastings algorithm is a method to generate samples from complex distributions by proposing new samples and deciding whether to accept them, making it valuable for Bayesian analysis and complex modeling.</span></a> of <span class="citation">Hastings (<a href="bibliography.html#ref-hastings1970monte">1970</a>)</span> and <span class="citation">Metropolis et al. (<a href="bibliography.html#ref-metropolis1953equation">1953</a>)</span>.</p>
</div>
<div id="S:Sec942" class="section level3 hasAnchor" number="9.4.2">
<h3><span class="header-section-number">9.4.2</span> The Gibbs Sampler<a href="ChBayes.html#S:Sec942" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As mentioned above, sometimes, we cannot use conjugate families. In other cases where the parameter space is large, it can be very hard to find the marginal likelihood <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x})\)</span> (also known as the normalizing constant); that is, assuming that the model parameters are given by <span class="math inline">\(\pmb{\theta} = [\, \theta_1 \quad ... \theta_2 \quad ... \quad \theta_k \,]\)</span> and contains <span class="math inline">\(k\)</span> parameters, the marginal likelihood given by</p>
<p><span class="math display">\[
f_{\mathbf{X}}(\mathbf{x}) = \int \int ... \int f_{\mathbf{X}|\pmb{\Theta} = \pmb{\theta}} (\mathbf{x}) f_{\pmb{\Theta}}(\pmb{\theta}) \, d\theta_1 \,d\theta_2 \,... \,d\theta_k
\]</span></p>
<p>is hard to compute even when using typical quadrature-based rules, especially if <span class="math inline">\(k\)</span> is large.</p>
<p>Fortunately, under very mild regularity conditions, samples of the joint estimates of parameters can be obtained by sequentially sampling each parameter individually and by keeping all the other parameters constant. To do so, the distribution of any given parameter conditional on all the other parameters (and the data) needs to be known. These distributions are known as full conditional distributions; that is,</p>
<p><span class="math display">\[
f_{\Theta_i \,|\, \mathbf{X} = \mathbf{x}, \, \pmb{\Theta}_{\backslash i} = \pmb{\theta}_{\backslash i}} (\theta_i),
\]</span></p>
<p>for parameter <span class="math inline">\(\theta_i\)</span>, where <span class="math inline">\(\pmb{\theta}_{\backslash i}\)</span> represents all parameters except for the <span class="math inline">\(i^{\text{th}}\)</span> one, and <span class="math inline">\(\pmb{\Theta}_{\backslash i}\)</span> is the random variable associated with this set of parameters.</p>
<p>The full conditional distribution is an important building block in Gibbs sampling. Indeed, if one can obtain each parameter’s distribution conditional on having the value of all the other parameters in closed form, then it is possible to generate samples for each parameter. Specifically, starting from an arbitrary set of starting values <span class="math inline">\(\pmb{\theta}^{(0)}_{\vphantom{2}} = [ \, {\theta}_1^{(0)} \quad {\theta}_2^{(0)} \quad ... \quad {\theta}_k^{(0)} \, ]\)</span>, samples for each parameter can be generated by performing the following steps for <span class="math inline">\(m = 1, 2, ..., M\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Draw <span class="math inline">\(\theta^{(m)}_1\)</span> from <span class="math inline">\(f_{\Theta_1^{\vphantom{(m)}} \, |\, \mathbf{X} = \mathbf{x}, \, \Theta_2^{\vphantom{(m)}} = \, \theta_2^{(m-1)}, \,..., \, \Theta_k^{\vphantom{(m)}} = \, \theta_k^{(m-1)}} (\theta_1)\)</span>.</li>
<li>Draw <span class="math inline">\(\theta^{(m)}_2\)</span> from <span class="math inline">\(f_{\Theta_2^{\vphantom{(m)}} \, |\, \mathbf{X} = \mathbf{x}, \, \Theta_1^{\vphantom{(m)}} = \, \theta_1^{(m)}, \, \Theta_3^{\vphantom{(m)}} = \,\theta_3^{(m-1)}, \,..., \, \Theta_k^{\vphantom{(m)}} = \,\theta_k^{(m-1)}} (\theta_2)\)</span>.</li>
<li>Draw <span class="math inline">\(\theta^{(m)}_3\)</span> from <span class="math inline">\(f_{\Theta_3^{\vphantom{(m)}} \, |\, \mathbf{X} = \mathbf{x}, \, \Theta_1^{\vphantom{(m)}} =\, \theta_1^{(m)}, \, \Theta_2^{\vphantom{(m)}} =\, \theta_2^{(m)}, \, \Theta_4^{\vphantom{(m)}} = \,\theta_4^{(m-1)}, \,..., \, \Theta_k^{\vphantom{(m)}} =\, \theta_k^{(m-1)}} (\theta_3)\)</span>.</li>
</ol>
<p><span class="math inline">\(~~\vdots\)</span></p>
<p><span class="math inline">\(k\)</span>. Draw <span class="math inline">\(\theta^{(m)}_k\)</span> from <span class="math inline">\(f_{\Theta_k^{\vphantom{(m)}} \, |\, \mathbf{X} = \mathbf{x}, \, \Theta_1^{\vphantom{(m)}} =\, \theta_1^{(m)}, \,..., \, \Theta_{k-1}^{\vphantom{(m)}} = \,\theta_{k-1}^{(m)}} (\theta_k)\)</span>.</p>
<p>The sample, especially at first, will depend on the initial values, <span class="math inline">\(\pmb{\theta}^{(0)}_{\vphantom{2}}\)</span>, and it might take some time until the sampler can get to the stationary distribution. For this reason, in practice, experimenters discard the first <span class="math inline">\(M^*\)</span> iterations to make sure their analysis is not impacted by the choice of initial parameter; this initial period of discarded sample is known as the burn-in period.</p>
<p>The rest of the sample—the remaining <span class="math inline">\(M-M^*\)</span> iterations—is kept to estimate the posterior distribution and any quantities of interest.</p>
<div id="application-to-bayesian-linear-regression" class="section level4 unnumbered hasAnchor">
<h4>Application to Bayesian Linear Regression<a href="ChBayes.html#application-to-bayesian-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In statistics and in its most simple form, a linear regression is an approach for modeling the relationship between a scalar response and an explanatory variable. The former quantity is denoted by <span class="math inline">\(x_i\)</span> for <span class="math inline">\(i \in \{1, ..., n\}\)</span>, and the latter quantity is denoted by <span class="math inline">\(z_{i}\)</span> for <span class="math inline">\(i \in \{1,...,n\}\)</span> in this chapter. Mathematically, we can write this relationship as</p>
<p><span class="math display">\[
x_i = \alpha + \beta z_{i} + \varepsilon_i,
\]</span></p>
<p>where <span class="math inline">\(\varepsilon_i\)</span> is a disturbance term that captures the potential for errors in the linear relationship. This error term is typically assumed to be normally distributed with mean zero and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>In general, the coefficients <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are unknown and need to be estimated. The experimenter can rely on Bayesian statistics to find out the posterior distribution of the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> along with that of <span class="math inline">\(\sigma^2\)</span>. For the rest of the subsection, we investigate a specific application of Gibbs sampling to the context of linear regression.</p>
<p>We begin by computing the likelihood function conditional on the parameter values:
<span class="math display">\[\begin{align*}
f_{\mathbf{X}\,|\,A=\alpha,\, B=\beta,\,\Sigma^2=\sigma^2}(\mathbf{x})=&amp;\, \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( - \frac{\left( x_i - \alpha -\beta z_{i} \right)^{2}}{2 \sigma^2} \right) \\
=&amp;\, \left(  2 \pi \sigma^2 \right)^{-\tfrac{n}{2}} \exp\left(- \frac{\sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{2 \sigma^2} \right),
\end{align*}\]</span>
which is the first building block to construct our posterior distribution.</p>
<p>Then, we need a prior distribution, which could be informative, weakly informative, or noninformative. In this application, we select a prior that allows us to obtain each parameter’s full conditional distribution in closed form. Specifically, we use a normal distribution for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, and an inverse gamma distribution for <span class="math inline">\(\sigma^2\)</span> with shape parameter <span class="math inline">\(\tfrac{n_{\sigma}}{2}\)</span> and scale parameter <span class="math inline">\(\tfrac{\theta_{\sigma}}{2}\)</span>, where
<span class="math display">\[\begin{align*}
f_{A}(\alpha) =&amp;\, \frac{1}{\sqrt{2\pi \tau_{\alpha}^2\vphantom{2\pi \tau_{\beta}^2} }} \exp\left( - \frac{1}{2} \frac{(\alpha-\theta_{\alpha})^2}{\tau_{\alpha}^2 } \right), \\
f_{B}(\beta) =&amp;\,  \frac{1}{\sqrt{2\pi \tau_{\beta}^2 }} \exp\left( - \frac{1}{2} \frac{(\beta-\theta_{\beta})^2}{\tau_{\beta}^2 } \right), \\
f_{\Sigma^2}(\sigma^2) =&amp;\,  \frac{(\theta_{\sigma}/2)^{n_{\sigma}/2}}{\Gamma(n_{\sigma}/2)} \left( \frac{1}{\sigma^2} \right)^{n_{\sigma}/2+1} \exp\left( - \frac{\theta_{\sigma}/2}{\sigma^2} \right).
\end{align*}\]</span></p>
<hr />
<p><strong>Proposition 9.4.1. Full Conditional Distributions of Bayesian Linear Regression Parameters.</strong> Consider a sample of <span class="math inline">\(n\)</span> observations <span class="math inline">\(\mathbf{x} = (x_1,...,x_n)\)</span> for which
<span class="math display">\[
x_i = \alpha + \beta z_{i} + \varepsilon_i,
\]</span>
where <span class="math inline">\(\varepsilon_i\)</span> is normally distributed with mean zero and variance <span class="math inline">\(\sigma^2\)</span>. The full conditional distributions of parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> are given by the following expressions:
<span class="math display">\[\begin{align*}
A\sim&amp;\, \text{Normal}\left( \frac{1}{n} \left(\sum_{i=1}^n x_i - \beta z_i \right) \frac{n \tau_{\alpha}^2}{n\tau_{\alpha}^2 + \sigma^2} + \theta_{\alpha} \frac{\sigma^2}{n\tau_{\alpha}^2+\sigma^2}, \frac{\tau_{\alpha}^2\sigma^2}{n\tau_{\alpha}^2+\sigma^2} \right), \\
B\sim&amp;\, \text{Normal}\left( \frac{1}{n}\left(  \sum_{i=1}^n z_i\left( x_i - \alpha \right) \right) \frac{n \tau_{\beta}^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 + \sigma^2} + \theta_{\beta} \frac{\sigma^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 +\sigma^2}, \frac{\tau_{\beta}^2\sigma^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2 +\sigma^2} \right)\!, \\
\Sigma^2\sim &amp;\, \text{Inverse Gamma}\left( \frac{n_{\sigma}+n}{2}, \frac{\theta_{\sigma} + \sum_{i=1}^n \left( y_i - \alpha - \beta z_{i} \right)^{2}}{2} \right),
\end{align*}\]</span></p>
<p>respectively, assuming the prior distributions mentioned above.</p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.6" href="javascript:toggleTheory('toggleTheory.Theory.6','displayCode.Theory.6');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.6" style="display: none">
<hr />
<p><span class="math inline">\(\textbf{Proof}\)</span>. From Section <a href="ChBayes.html#S:Sec92">9.2</a>, we know that
<span class="math display">\[
f_{A,B,\Sigma^2|\mathbf{X}=\mathbf{x}}(\alpha,\beta,\sigma^2) \propto f_{\mathbf{X}|A=\alpha,\,  B=\beta,\,\Sigma^2=\sigma^2}(\mathbf{x})\,  f_{A}(\alpha)\, f_{B}(\beta) \, f_{\Sigma^2}(\sigma^2),
\]</span>
which is useful to derive the full conditional distributions of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Let us begin with <span class="math inline">\(\alpha\)</span>:
<span class="math display">\[\begin{align*}
&amp;\, f_{A|\mathbf{X}=\mathbf{x},\,B=\beta,\,\Sigma^2=\sigma^2}(\alpha) \\
\propto&amp;\, \exp\left(- \frac{1}{2}\frac{\sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{\sigma^2} \right) \exp\left( - \frac{1}{2} \frac{(\alpha-\theta_{\alpha})^2}{\tau_{\alpha}^2 } \right)  \\
\propto &amp;\,\exp\left(-\frac{1}{2} \left( \frac{n \alpha^2 - 2 \alpha \sum_{i=1}^n (x_i - \beta z_i)}{\sigma^2} + \frac{\alpha^2 -2\alpha \theta_{\alpha} }{\tau_{\alpha}^2} \right) \right) \\
\propto &amp;\,\exp\left( -\frac{1}{2} \left( \frac{\alpha^2 - 2 \alpha \left(  \frac{1}{n}\left( \sum_{i=1}^n x_i - \beta z_i \right) \frac{n \tau_{\alpha}^2}{n\tau_{\alpha}^2 + \sigma^2} + \theta_{\alpha} \frac{\sigma^2}{n\tau_{\alpha}^2+\sigma^2}\right)}{\frac{\tau_{\alpha}^2\sigma^2}{n\tau_{\alpha}^2+\sigma^2}}  \right) \right) \\
\propto &amp;\,\frac{1}{\sqrt{2\pi \left(\frac{\tau_{\alpha}^2\sigma^2}{n\tau_{\alpha}^2+\sigma^2}\right)}} \exp\left( -\frac{1}{2} \left( \frac{\left(\alpha - \left( \frac{1}{n}\left(  \sum_{i=1}^n x_i - \beta z_i \right) \frac{n \tau_{\alpha}^2}{n\tau_{\alpha}^2 + \sigma^2} + \theta_{\alpha} \frac{\sigma^2}{n\tau_{\alpha}^2+\sigma^2}\right)\right)^2}{\frac{\tau_{\alpha}^2\sigma^2}{n\tau_{\alpha}^2+\sigma^2}}  \right) \right)
\end{align*}\]</span>
which is a normal distribution with mean parameter
<span class="math display">\[
\frac{1}{n} \left(\sum_{i=1}^n x_i - \beta z_i \right) \frac{n \tau_{\alpha}^2}{n\tau_{\alpha}^2 + \sigma^2} + \theta_{\alpha} \frac{\sigma^2}{n\tau_{\alpha}^2+\sigma^2}
\]</span>
and variance parameter
<span class="math display">\[
\frac{\tau_{\alpha}^2\sigma^2}{n\tau_{\alpha}^2+\sigma^2}.
\]</span>
The derivation to obtain the full conditional distribution of <span class="math inline">\(\beta\)</span> is similar to that of <span class="math inline">\(\alpha\)</span>:</p>
<p><span class="math display">\[\begin{align*}
&amp;\, f_{B|\mathbf{X}=\mathbf{x},\,A=\alpha,\,\Sigma^2=\sigma^2}(\beta) \\
\propto&amp;\, \exp\left(- \frac{1}{2}\frac{\sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{\sigma^2} \right) \exp\left( - \frac{1}{2} \frac{(\beta-\theta_{\beta})^2}{\tau_{\beta}^2 } \right)  \\
\propto &amp;\,\exp\left(-\frac{1}{2} \left( \frac{\beta^2 \sum_{i=1}^n z_i^2 - 2 \beta \sum_{i=1}^n z_i (x_i - \alpha)}{\sigma^2} + \frac{\beta^2 -2\beta \theta_{\beta} }{\tau_{\beta}^2} \right) \right) \\
\propto &amp;\,\exp\left( -\frac{1}{2} \left( \frac{\beta^2 - 2 \beta \left(  \frac{1}{n}\left( \sum_{i=1}^n z_i \left( x_i - \alpha \right) \right) \frac{n \tau_{\beta}^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2 + \sigma^2} + \theta_{\beta} \frac{\sigma^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2+\sigma^2}\right)}{\frac{\sigma_{\beta}^2\sigma^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2+\sigma^2}}  \right) \right) \\
\propto &amp;\,\frac{1}{\sqrt{2\pi \left(\frac{\tau_{\beta}^2\sigma^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2 +\sigma^2}\right)}} \\
&amp;\, \times \exp\left( -\frac{1}{2} \left( \frac{\left(\beta - \left( \frac{1}{n}\left(  \sum_{i=1}^n z_i\left( x_i - \alpha \right) \right) \frac{n \tau_{\beta}^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 + \sigma^2} + \theta_{\beta} \frac{\sigma^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 +\sigma^2}\right)\right)^2}{\frac{\tau_{\alpha}^2\sigma^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2+\sigma^2}}  \right) \right)
\end{align*}\]</span>
which is a normal distribution with mean parameter
<span class="math display">\[
\frac{1}{n}\left(  \sum_{i=1}^n z_i\left( x_i - \alpha \right) \right) \frac{n \tau_{\beta}^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 + \sigma^2} + \theta_{\beta} \frac{\sigma^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 +\sigma^2}
\]</span>
and variance parameter
<span class="math display">\[
\frac{\tau_{\beta}^2\sigma^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2 +\sigma^2}.
\]</span>
Finally, we apply the same logic to the variance parameter, <span class="math inline">\(\sigma^2\)</span>:
<span class="math display">\[\begin{align*}
&amp;\,f_{\Sigma^2|\mathbf{X}=\mathbf{x},\,A=\alpha,\,B=\beta}(\sigma^2) \\
\propto&amp;\, \left(  2 \pi \sigma^2 \right)^{-n/2} \exp\left(- \frac{1}{2}\frac{\sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{\sigma^2} \right) \left( \frac{1}{\sigma^2} \right)^{n_{\sigma}/2+1} \exp\left( - \frac{\theta_{\sigma}/2}{\sigma^2} \right) \\
\propto&amp;\, \exp\left(- \frac{1}{2}\frac{\theta_{\sigma} + \sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{\sigma^2} \right) \left( \frac{1}{\sigma^2} \right)^{(n_{\sigma}+n)/2+1} \\
\propto &amp;\, \frac{\left(\frac{\theta_{\sigma} + \sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{2}\right)^{(n_{\sigma}+n)/2}}{\Gamma((n_{\sigma}+n)/2)} \left( \frac{1}{\sigma^2} \right)^{(n_{\sigma}+n)/2+1} \exp\left( - \frac{\frac{\theta_{\sigma} + \sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{2}}{\sigma^2} \right),
\end{align*}\]</span>
which is an inverse gamma distribution with shape parameter <span class="math inline">\(\tfrac{n_{\sigma}+n}{2}\)</span> and scale parameter
<span class="math display">\[
\frac{\theta_{\sigma} + \sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{2}.
\]</span></p>
<hr />
</div>
<hr />
<p>We now apply the Gibbs sampler on <em>real</em> data. The example will use motorcycle insurance data from Wasa, a Swedish insurance company, taken from <code>dataOhlsson</code> of the R package <code>insuranceData</code>; see <span class="citation">Wolny-Dominiak and Trzesiok (<a href="bibliography.html#ref-wolny2014package">2014</a>)</span> for more details.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="ChBayes.html#cb135-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;insuranceData&quot;</span>)</span>
<span id="cb135-2"><a href="ChBayes.html#cb135-2" tabindex="-1"></a><span class="fu">data</span>(dataOhlsson)</span></code></pre></div>
<p>This dataset contains information about the number of motorcycle accidents, their claim cost, and some risk factors (e.g., the age of the driver, the age of the vehicle, the geographic zone).</p>
<hr />
<p><strong>Example 9.4.1. Bayesian Linear Regression.</strong> You wish to understand the relationship between the age of the driver and the (logarithm of the) claim cost. Let <span class="math inline">\(x_i\)</span> be the logarithm of the <span class="math inline">\(i^{\text{th}}\)</span> claim cost and <span class="math inline">\(z_i\)</span> be the age associated with the <span class="math inline">\(i^{\text{th}}\)</span> claim. Further assume the following linear relationship between the two quantities:
<span class="math display">\[
x_i = \alpha + \beta z_{i} + \varepsilon_i,
\]</span>
where <span class="math inline">\(\varepsilon_i\)</span> is normally distributed with mean zero and variance <span class="math inline">\(\sigma^2\)</span>. Find the posterior density of the three parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> using the Gibbs sampler.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.4.1" href="javascript:toggleEX('toggleExample.9.4.1','displayExample.9.4.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.4.1" style="display: none">
<p><em>Example Solution.</em> Let us begin by visualizing the data. Figure 9.12 reports the logarithm of the claim cost as a function of the driver’s age. At first sight, it seems that the relationship between the claim cost and age is negative, so we should expect a negative <span class="math inline">\(\beta\)</span>, generally speaking.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig912"></span>
<img src="LDAEd2_files/figure-html/Fig912-1.png" alt="Logarithm of the claim cost as a function of the driver’s age" width="80%" />
<p class="caption">
Figure 9.12: <strong>Logarithm of the claim cost as a function of the driver’s age</strong>
</p>
</div>
<p>*** Let us now turn to Bayesian computation via Gibbs sampling to find the posterior distribution of the three parameters of interest. We will use 10,000 iterations and discard the first 5,000 iterations (i.e., burn-in period). For our prior distributions, we use weakly informative priors by setting <span class="math inline">\(\theta_{\alpha} = \tfrac{1}{n} \sum_{i=1}^n x_i = \overline{x}\)</span>, <span class="math inline">\(\theta_{\beta} = 0\)</span>, <span class="math inline">\(\tau_{\alpha}^2=\tau_{\beta}^2 = 10\)</span>, <span class="math inline">\(n_{\sigma} = 1\)</span>, and <span class="math inline">\(\theta_{\sigma} = 0.1\)</span>. The initial values of the parameters are set to: <span class="math inline">\(\alpha^{(0)} = \overline{x}\)</span>, <span class="math inline">\(\beta^{(0)} = 0\)</span>, and <span class="math inline">\(\sigma^{2\,(0)} = \tfrac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2\)</span>.
***</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="ChBayes.html#cb136-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb136-2"><a href="ChBayes.html#cb136-2" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;nimble&quot;</span>)</span>
<span id="cb136-3"><a href="ChBayes.html#cb136-3" tabindex="-1"></a>dataOhlsson <span class="ot">&lt;-</span> dataOhlsson[dataOhlsson<span class="sc">$</span>skadkost <span class="sc">&gt;</span> <span class="dv">0</span>, ]</span>
<span id="cb136-4"><a href="ChBayes.html#cb136-4" tabindex="-1"></a>dataOhlsson<span class="sc">$</span>logskadkost <span class="ot">&lt;-</span> <span class="fu">log</span>(dataOhlsson<span class="sc">$</span>skadkost)</span>
<span id="cb136-5"><a href="ChBayes.html#cb136-5" tabindex="-1"></a></span>
<span id="cb136-6"><a href="ChBayes.html#cb136-6" tabindex="-1"></a>x <span class="ot">&lt;-</span> dataOhlsson<span class="sc">$</span>logskadkost</span>
<span id="cb136-7"><a href="ChBayes.html#cb136-7" tabindex="-1"></a>z <span class="ot">&lt;-</span> dataOhlsson<span class="sc">$</span>agarald</span>
<span id="cb136-8"><a href="ChBayes.html#cb136-8" tabindex="-1"></a></span>
<span id="cb136-9"><a href="ChBayes.html#cb136-9" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb136-10"><a href="ChBayes.html#cb136-10" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb136-11"><a href="ChBayes.html#cb136-11" tabindex="-1"></a>Mstar <span class="ot">&lt;-</span> <span class="dv">5000</span></span>
<span id="cb136-12"><a href="ChBayes.html#cb136-12" tabindex="-1"></a>thetaa <span class="ot">&lt;-</span> <span class="fu">mean</span>(x)</span>
<span id="cb136-13"><a href="ChBayes.html#cb136-13" tabindex="-1"></a>tau2a <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb136-14"><a href="ChBayes.html#cb136-14" tabindex="-1"></a>thetab <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb136-15"><a href="ChBayes.html#cb136-15" tabindex="-1"></a>tau2b <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb136-16"><a href="ChBayes.html#cb136-16" tabindex="-1"></a>nsigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb136-17"><a href="ChBayes.html#cb136-17" tabindex="-1"></a>thetasigma <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb136-18"><a href="ChBayes.html#cb136-18" tabindex="-1"></a></span>
<span id="cb136-19"><a href="ChBayes.html#cb136-19" tabindex="-1"></a>alphas <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb136-20"><a href="ChBayes.html#cb136-20" tabindex="-1"></a>betas <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb136-21"><a href="ChBayes.html#cb136-21" tabindex="-1"></a>sigma2s <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb136-22"><a href="ChBayes.html#cb136-22" tabindex="-1"></a></span>
<span id="cb136-23"><a href="ChBayes.html#cb136-23" tabindex="-1"></a>alphas[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(x)</span>
<span id="cb136-24"><a href="ChBayes.html#cb136-24" tabindex="-1"></a>betas[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb136-25"><a href="ChBayes.html#cb136-25" tabindex="-1"></a>sigma2s[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">var</span>(x)</span>
<span id="cb136-26"><a href="ChBayes.html#cb136-26" tabindex="-1"></a></span>
<span id="cb136-27"><a href="ChBayes.html#cb136-27" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb136-28"><a href="ChBayes.html#cb136-28" tabindex="-1"></a>    <span class="co"># Generate alpha</span></span>
<span id="cb136-29"><a href="ChBayes.html#cb136-29" tabindex="-1"></a>    den_alpha <span class="ot">&lt;-</span> n <span class="sc">*</span> tau2a <span class="sc">+</span> sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb136-30"><a href="ChBayes.html#cb136-30" tabindex="-1"></a>    mean_alpha <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>n) <span class="sc">*</span> (<span class="fu">sum</span>(x <span class="sc">-</span> betas[m <span class="sc">-</span> <span class="dv">1</span>] <span class="sc">*</span> z)) <span class="sc">*</span> (n <span class="sc">*</span> tau2a)<span class="sc">/</span>den_alpha <span class="sc">+</span> thetaa <span class="sc">*</span></span>
<span id="cb136-31"><a href="ChBayes.html#cb136-31" tabindex="-1"></a>        sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]<span class="sc">/</span>den_alpha</span>
<span id="cb136-32"><a href="ChBayes.html#cb136-32" tabindex="-1"></a>    var_alpha <span class="ot">&lt;-</span> tau2a <span class="sc">*</span> sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]<span class="sc">/</span>den_alpha</span>
<span id="cb136-33"><a href="ChBayes.html#cb136-33" tabindex="-1"></a></span>
<span id="cb136-34"><a href="ChBayes.html#cb136-34" tabindex="-1"></a>    alphas[m] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> mean_alpha, <span class="at">sd =</span> <span class="fu">sqrt</span>(var_alpha))</span>
<span id="cb136-35"><a href="ChBayes.html#cb136-35" tabindex="-1"></a></span>
<span id="cb136-36"><a href="ChBayes.html#cb136-36" tabindex="-1"></a>    <span class="co"># Generate beta</span></span>
<span id="cb136-37"><a href="ChBayes.html#cb136-37" tabindex="-1"></a>    den_beta <span class="ot">&lt;-</span> tau2b <span class="sc">*</span> <span class="fu">sum</span>(z<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb136-38"><a href="ChBayes.html#cb136-38" tabindex="-1"></a>    mean_beta <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>n) <span class="sc">*</span> (<span class="fu">sum</span>(z <span class="sc">*</span> (x <span class="sc">-</span> alphas[m]))) <span class="sc">*</span> (n <span class="sc">*</span> tau2b)<span class="sc">/</span>den_beta <span class="sc">+</span> thetab <span class="sc">*</span></span>
<span id="cb136-39"><a href="ChBayes.html#cb136-39" tabindex="-1"></a>        sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]<span class="sc">/</span>den_beta</span>
<span id="cb136-40"><a href="ChBayes.html#cb136-40" tabindex="-1"></a>    var_beta <span class="ot">&lt;-</span> tau2b <span class="sc">*</span> sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]<span class="sc">/</span>den_beta</span>
<span id="cb136-41"><a href="ChBayes.html#cb136-41" tabindex="-1"></a></span>
<span id="cb136-42"><a href="ChBayes.html#cb136-42" tabindex="-1"></a>    betas[m] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> mean_beta, <span class="at">sd =</span> <span class="fu">sqrt</span>(var_beta))</span>
<span id="cb136-43"><a href="ChBayes.html#cb136-43" tabindex="-1"></a></span>
<span id="cb136-44"><a href="ChBayes.html#cb136-44" tabindex="-1"></a>    <span class="co"># Generate sigma^2</span></span>
<span id="cb136-45"><a href="ChBayes.html#cb136-45" tabindex="-1"></a>    shape_sigma <span class="ot">&lt;-</span> (nsigma <span class="sc">+</span> n)<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb136-46"><a href="ChBayes.html#cb136-46" tabindex="-1"></a>    scale_sigma <span class="ot">&lt;-</span> (thetasigma <span class="sc">+</span> <span class="fu">sum</span>((x <span class="sc">-</span> alphas[m] <span class="sc">-</span> betas[m] <span class="sc">*</span> z)<span class="sc">^</span><span class="dv">2</span>))<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb136-47"><a href="ChBayes.html#cb136-47" tabindex="-1"></a></span>
<span id="cb136-48"><a href="ChBayes.html#cb136-48" tabindex="-1"></a>    sigma2s[m] <span class="ot">&lt;-</span> <span class="fu">rinvgamma</span>(<span class="dv">1</span>, <span class="at">shape =</span> shape_sigma, <span class="at">scale =</span> scale_sigma)</span>
<span id="cb136-49"><a href="ChBayes.html#cb136-49" tabindex="-1"></a>}</span></code></pre></div>
<p>*** Once we have the posterior parameter samples, we can get multiple quantities of interest. For instance, the posterior mean of parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> are <span class="math inline">\(9.843\)</span>, <span class="math inline">\(-0.0208\)</span>, and <span class="math inline">\(2.551\)</span>, respectively. These posterior means are obtained by simply taking the sample means of the respective posterior draws; that is, these are Monte Carlo estimates of the posterior means.
***</p>
<pre><code>The posterior mean for coefficient alpha is 9.84259435</code></pre>
<pre><code>The posterior mean for coefficient beta is -0.0207847772</code></pre>
<pre><code>The posterior mean for the variance parameter is 2.55090494</code></pre>
<p>*** We can also get histograms of the posterior distribution for <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span>; Figure 9.13 reports histograms for the three parameters. The uncertainty around each parameter is very small.
***</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig913"></span>
<img src="LDAEd2_files/figure-html/Fig913-1.png" alt="Histogram of the posterior distribution for parameters \(\alpha\) (top panel), \(\beta\) (middle panel), and \(\sigma^2\) (bottom panel)" width="80%" />
<p class="caption">
Figure 9.13: <strong>Histogram of the posterior distribution for parameters <span class="math inline">\(\alpha\)</span> (top panel), <span class="math inline">\(\beta\)</span> (middle panel), and <span class="math inline">\(\sigma^2\)</span> (bottom panel)</strong>
</p>
</div>
<p>*** The top panel of Figure 9.14 reports a plot of the post-burn-in values of <span class="math inline">\(\alpha\)</span> as a function of the iteration number; this type of plot is known as a trace plot in the literature. These samples are not impacted by the initial parameter value that was selected. Indeed, after about 20–30 iterations, the posterior parameter values obtained by the Gibbs sampler are very close to their posterior means. For instance, the bottom panel of Figure <a href="ChBayes.html#fig:Fig914">9.14</a> shows a plot of the first 50 values of <span class="math inline">\(\alpha\)</span> as a function of the iteration number.
***</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig914"></span>
<img src="LDAEd2_files/figure-html/Fig914-1.png" alt="Trace plot of \(\alpha\) for the post-burn-in iterations (top panel) and for the first 50 iterations (bottom panel)" width="80%" />
<p class="caption">
Figure 9.14: <strong>Trace plot of <span class="math inline">\(\alpha\)</span> for the post-burn-in iterations (top panel) and for the first 50 iterations (bottom panel)</strong>
</p>
</div>
</div>
<hr />
</div>
</div>
<div id="S:Sec943" class="section level3 hasAnchor" number="9.4.3">
<h3><span class="header-section-number">9.4.3</span> The Metropolis–Hastings Algorithm<a href="ChBayes.html#S:Sec943" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Gibbs sampling works well when the full conditional distribution for each parameter in the model can be found and is of a common form. This, unfortunately, is not always possible, meaning that we need to rely on other computational tools to find the posterior distribution of the parameters. One very popular method that copes with the shortcomings of Gibbs’ method is the Metropolis–Hastings sampler.</p>
<p>Let us assume that the current value of the first model parameter is <span class="math inline">\(\theta_1^{(0)}\)</span>. From this current value, we now wish to find a new value for this parameter. To do so, we propose a new value for this parameter, <span class="math inline">\(\theta^*_1\)</span>, from a candidate (or proposal) density <span class="math inline">\(q\left( \theta_1^* \,\middle|\, \theta_1^{(0)} \right)\)</span>. Since this proposal has nothing to do with the posterior distribution of the parameter, we should not keep all candidates in our final sample—we only accept those samples that are representative of the posterior distribution of interest. To determine whether we accept or reject the candidate, we compute a so-called acceptance ratio <span class="math inline">\(\alpha\left( \theta_1^{(0)}, \theta_1^* \right)\)</span> using
<span class="math display">\[
\alpha\left(\theta_1^{(0)},\theta_1^*\right) = \frac{h\left(\theta_1^*\vphantom{\theta_1^{(1)}}\right) q\left( \theta_1^{(0)} \,\middle|\,\theta_1^*  \right)}{h\left(\theta_1^{(1)}\right) q\left( \theta_1^* \,\middle|\, \theta_1^{(0)} \right)}
\]</span>
where
<span class="math display">\[
h(\theta_1) = f_{\mathbf{X}\,|\,\Theta_1 =  \,\theta_1, \, \pmb{\Theta}_{\backslash1}=\,\pmb{\theta}_{\backslash1}}(\mathbf{x}) \, f_{\Theta_1,\pmb{\Theta}_{\backslash1}}\left(\theta_1,\pmb{\theta}_{\backslash1}\right)
\]</span>
and <span class="math inline">\(\pmb{\theta}_{\backslash 1}\)</span> represents all parameters except for the first one. Then, we accept the proposed value <span class="math inline">\(\theta_1^*\)</span> with probability <span class="math inline">\(\alpha\left( \theta_1^{(0)}, \theta_1^* \right)\)</span> and reject it with probability <span class="math inline">\(1-\alpha\left( \theta_1^{(0)}, \theta_1^* \right)\)</span>. Specifically,
<span class="math display">\[
\theta_1^{(1)} = \left\{
\begin{array}{ll}
\theta_1^* &amp; \text{with probability }\,\alpha\left(\theta_1^{(0)},\theta_1^*\right) \\
\theta_1^{(0)} &amp;\text{with probability }\,1-\alpha\left(\theta_1^{(0)},\theta_1^*\right)
\end{array}
\right.
\]</span>
We can repeat the same process for all other parameters to obtain <span class="math inline">\(\theta_2^{(1)}\)</span> to <span class="math inline">\(\theta_k^{(1)}\)</span>, while replacing the parameters <span class="math inline">\(\pmb{\theta}_{\backslash i}\)</span> by their most current values in the chain. Once we have updated all values, we can repeat this process for all <span class="math inline">\(m\)</span> in <span class="math inline">\(\{2,3,...,M\}\)</span>, similar to the iterative process used in the Gibbs sampler.<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a></p>
<p><strong>Special Case: Symmetric Proposal Distribution.</strong> If a proposal distribution is symmetric, then
<span class="math display">\[
q\left( \theta_i^{(m)} \,\middle|\,\theta_i^*  \right) = q\left( \theta_i^*  \,\middle|\,\theta_i^{(m)} \right),
\]</span>
and those terms cancel out, leaving
<span class="math display">\[
\alpha\left(\theta_i^{(m)},\theta_1^*\right) = \frac{h\left(\theta_i^*\vphantom{\theta_1^{(1)}}\right)}{h\left(\theta_i^{(m)}\right) }.
\]</span>
This special case is called the <em>Metropolis algorithm.</em></p>
<hr />
<p>The Metropolis–Hastings sampler requires a lot of fine-tuning, generally speaking, because the experimenter needs to select a proposal distribution for each parameter. A common approach is to assume a normal proposal distribution centered at the previous value; that is,
<span class="math display">\[
\Theta_i^* \sim \text{Normal}\left( \theta_i^{(m-1)} , \delta_i^2 \right),
\]</span>
at step <span class="math inline">\(m\)</span>, where <span class="math inline">\(\delta_i^2\)</span> is the variance of the <span class="math inline">\(i^{\text{th}}\)</span> parameter’s proposal distribution.</p>
<hr />
<p><strong>Example 9.4.2. Impact of Proposal Density on the Acceptance Rate.</strong>
Assume that each policyholder’s claim count (frequency) is distributed as a Poisson random variable such that
<span class="math display">\[
p_{N_i\,|\,\Lambda=\lambda}(n_i) = \frac{\lambda^{n_i}e^{-\lambda}}{n_i!},
\]</span>
where <span class="math inline">\(n_i\)</span> is the number of claims associated with the <span class="math inline">\(i^{\text{th}}\)</span> policyholder. Further assume a noninformative, flat prior over <span class="math inline">\([0,\infty]\)</span>; that is,
<span class="math display">\[
f_{\Lambda}(\lambda) \propto 1, \quad \lambda \in [0,\infty].
\]</span>
Find the posterior distribution of the parameter using 1,000 iterations of the Metropolis–Hastings sampler assuming the claim count data of the Singapore Insurance Data (see Example 9.1.4 for more details). Use a normal proposal with small (<span class="math inline">\(1\times 10^{-7}\)</span>), moderate (<span class="math inline">\(1\times 10^{-4}\)</span>), and large (<span class="math inline">\(1 \times 10^{-1}\)</span>) values as the proposal variance <span class="math inline">\(\delta\)</span> in your tests and comment on the differences.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.4.2" href="javascript:toggleEX('toggleExample.9.4.2','displayExample.9.4.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.4.2" style="display: none">
<p><em>Example Solution.</em> Starting from the the likelihood function and the prior distribution, we have that
<span class="math display">\[
h(\lambda) \propto \prod_{i=1}^N \frac{\lambda^{n_i}e^{-\lambda}}{n_i!}.
\]</span></p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="ChBayes.html#cb140-1" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb140-2"><a href="ChBayes.html#cb140-2" tabindex="-1"></a></span>
<span id="cb140-3"><a href="ChBayes.html#cb140-3" tabindex="-1"></a><span class="co"># First variance: 1 x 10^-7</span></span>
<span id="cb140-4"><a href="ChBayes.html#cb140-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb140-5"><a href="ChBayes.html#cb140-5" tabindex="-1"></a>delta21 <span class="ot">&lt;-</span> <span class="fl">0.0000001</span></span>
<span id="cb140-6"><a href="ChBayes.html#cb140-6" tabindex="-1"></a></span>
<span id="cb140-7"><a href="ChBayes.html#cb140-7" tabindex="-1"></a>lambdas1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb140-8"><a href="ChBayes.html#cb140-8" tabindex="-1"></a>lambdas1[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(sgautonb<span class="sc">$</span>Clm_Count)</span>
<span id="cb140-9"><a href="ChBayes.html#cb140-9" tabindex="-1"></a>accept1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb140-10"><a href="ChBayes.html#cb140-10" tabindex="-1"></a></span>
<span id="cb140-11"><a href="ChBayes.html#cb140-11" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb140-12"><a href="ChBayes.html#cb140-12" tabindex="-1"></a>    <span class="co"># Compute logarithm of h for past value</span></span>
<span id="cb140-13"><a href="ChBayes.html#cb140-13" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb140-14"><a href="ChBayes.html#cb140-14" tabindex="-1"></a></span>
<span id="cb140-15"><a href="ChBayes.html#cb140-15" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb140-16"><a href="ChBayes.html#cb140-16" tabindex="-1"></a>    lambdastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> lambdas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(delta21))</span>
<span id="cb140-17"><a href="ChBayes.html#cb140-17" tabindex="-1"></a>    <span class="cf">if</span> (lambdastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb140-18"><a href="ChBayes.html#cb140-18" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb140-19"><a href="ChBayes.html#cb140-19" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb140-20"><a href="ChBayes.html#cb140-20" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb140-21"><a href="ChBayes.html#cb140-21" tabindex="-1"></a>    }</span>
<span id="cb140-22"><a href="ChBayes.html#cb140-22" tabindex="-1"></a></span>
<span id="cb140-23"><a href="ChBayes.html#cb140-23" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb140-24"><a href="ChBayes.html#cb140-24" tabindex="-1"></a>    alpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb140-25"><a href="ChBayes.html#cb140-25" tabindex="-1"></a>    <span class="cf">if</span> (alpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb140-26"><a href="ChBayes.html#cb140-26" tabindex="-1"></a>        lambdas1[m] <span class="ot">&lt;-</span> lambdastar</span>
<span id="cb140-27"><a href="ChBayes.html#cb140-27" tabindex="-1"></a>        accept1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb140-28"><a href="ChBayes.html#cb140-28" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb140-29"><a href="ChBayes.html#cb140-29" tabindex="-1"></a>        lambdas1[m] <span class="ot">&lt;-</span> lambdas1[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb140-30"><a href="ChBayes.html#cb140-30" tabindex="-1"></a>        accept1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb140-31"><a href="ChBayes.html#cb140-31" tabindex="-1"></a>    }</span>
<span id="cb140-32"><a href="ChBayes.html#cb140-32" tabindex="-1"></a>}</span>
<span id="cb140-33"><a href="ChBayes.html#cb140-33" tabindex="-1"></a></span>
<span id="cb140-34"><a href="ChBayes.html#cb140-34" tabindex="-1"></a><span class="co"># Second variance: 1 x 10^-4</span></span>
<span id="cb140-35"><a href="ChBayes.html#cb140-35" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb140-36"><a href="ChBayes.html#cb140-36" tabindex="-1"></a>delta22 <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb140-37"><a href="ChBayes.html#cb140-37" tabindex="-1"></a></span>
<span id="cb140-38"><a href="ChBayes.html#cb140-38" tabindex="-1"></a>lambdas2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb140-39"><a href="ChBayes.html#cb140-39" tabindex="-1"></a>lambdas2[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(sgautonb<span class="sc">$</span>Clm_Count)</span>
<span id="cb140-40"><a href="ChBayes.html#cb140-40" tabindex="-1"></a>accept2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb140-41"><a href="ChBayes.html#cb140-41" tabindex="-1"></a></span>
<span id="cb140-42"><a href="ChBayes.html#cb140-42" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb140-43"><a href="ChBayes.html#cb140-43" tabindex="-1"></a>    <span class="co"># Compute logarithm of h for past value</span></span>
<span id="cb140-44"><a href="ChBayes.html#cb140-44" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb140-45"><a href="ChBayes.html#cb140-45" tabindex="-1"></a></span>
<span id="cb140-46"><a href="ChBayes.html#cb140-46" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb140-47"><a href="ChBayes.html#cb140-47" tabindex="-1"></a>    lambdastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> lambdas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(delta22))</span>
<span id="cb140-48"><a href="ChBayes.html#cb140-48" tabindex="-1"></a>    <span class="cf">if</span> (lambdastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb140-49"><a href="ChBayes.html#cb140-49" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb140-50"><a href="ChBayes.html#cb140-50" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb140-51"><a href="ChBayes.html#cb140-51" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb140-52"><a href="ChBayes.html#cb140-52" tabindex="-1"></a>    }</span>
<span id="cb140-53"><a href="ChBayes.html#cb140-53" tabindex="-1"></a></span>
<span id="cb140-54"><a href="ChBayes.html#cb140-54" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb140-55"><a href="ChBayes.html#cb140-55" tabindex="-1"></a>    alpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb140-56"><a href="ChBayes.html#cb140-56" tabindex="-1"></a>    <span class="cf">if</span> (alpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb140-57"><a href="ChBayes.html#cb140-57" tabindex="-1"></a>        lambdas2[m] <span class="ot">&lt;-</span> lambdastar</span>
<span id="cb140-58"><a href="ChBayes.html#cb140-58" tabindex="-1"></a>        accept2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb140-59"><a href="ChBayes.html#cb140-59" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb140-60"><a href="ChBayes.html#cb140-60" tabindex="-1"></a>        lambdas2[m] <span class="ot">&lt;-</span> lambdas2[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb140-61"><a href="ChBayes.html#cb140-61" tabindex="-1"></a>        accept2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb140-62"><a href="ChBayes.html#cb140-62" tabindex="-1"></a>    }</span>
<span id="cb140-63"><a href="ChBayes.html#cb140-63" tabindex="-1"></a>}</span>
<span id="cb140-64"><a href="ChBayes.html#cb140-64" tabindex="-1"></a></span>
<span id="cb140-65"><a href="ChBayes.html#cb140-65" tabindex="-1"></a><span class="co"># Third variance: 1 x 10^-1</span></span>
<span id="cb140-66"><a href="ChBayes.html#cb140-66" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb140-67"><a href="ChBayes.html#cb140-67" tabindex="-1"></a>delta23 <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb140-68"><a href="ChBayes.html#cb140-68" tabindex="-1"></a></span>
<span id="cb140-69"><a href="ChBayes.html#cb140-69" tabindex="-1"></a>lambdas3 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb140-70"><a href="ChBayes.html#cb140-70" tabindex="-1"></a>lambdas3[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(sgautonb<span class="sc">$</span>Clm_Count)</span>
<span id="cb140-71"><a href="ChBayes.html#cb140-71" tabindex="-1"></a>accept3 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb140-72"><a href="ChBayes.html#cb140-72" tabindex="-1"></a></span>
<span id="cb140-73"><a href="ChBayes.html#cb140-73" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb140-74"><a href="ChBayes.html#cb140-74" tabindex="-1"></a>    <span class="co"># Compute logarithm of h for past value</span></span>
<span id="cb140-75"><a href="ChBayes.html#cb140-75" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdas3[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb140-76"><a href="ChBayes.html#cb140-76" tabindex="-1"></a></span>
<span id="cb140-77"><a href="ChBayes.html#cb140-77" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb140-78"><a href="ChBayes.html#cb140-78" tabindex="-1"></a>    lambdastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> lambdas3[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(delta23))</span>
<span id="cb140-79"><a href="ChBayes.html#cb140-79" tabindex="-1"></a>    <span class="cf">if</span> (lambdastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb140-80"><a href="ChBayes.html#cb140-80" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb140-81"><a href="ChBayes.html#cb140-81" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb140-82"><a href="ChBayes.html#cb140-82" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb140-83"><a href="ChBayes.html#cb140-83" tabindex="-1"></a>    }</span>
<span id="cb140-84"><a href="ChBayes.html#cb140-84" tabindex="-1"></a></span>
<span id="cb140-85"><a href="ChBayes.html#cb140-85" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb140-86"><a href="ChBayes.html#cb140-86" tabindex="-1"></a>    alpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb140-87"><a href="ChBayes.html#cb140-87" tabindex="-1"></a>    <span class="cf">if</span> (alpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb140-88"><a href="ChBayes.html#cb140-88" tabindex="-1"></a>        lambdas3[m] <span class="ot">&lt;-</span> lambdastar</span>
<span id="cb140-89"><a href="ChBayes.html#cb140-89" tabindex="-1"></a>        accept3[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb140-90"><a href="ChBayes.html#cb140-90" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb140-91"><a href="ChBayes.html#cb140-91" tabindex="-1"></a>        lambdas3[m] <span class="ot">&lt;-</span> lambdas3[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb140-92"><a href="ChBayes.html#cb140-92" tabindex="-1"></a>        accept3[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb140-93"><a href="ChBayes.html#cb140-93" tabindex="-1"></a>    }</span>
<span id="cb140-94"><a href="ChBayes.html#cb140-94" tabindex="-1"></a>}</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig915"></span>
<img src="LDAEd2_files/figure-html/Fig915-1.png" alt="Trace plots based on three different proposals: \(\sigma^2 = 1 \times 10^{-7}\) (top panel), \(\sigma^2=1\times 10^{-4}\) (middle panel), and \(\sigma^2 = 1 \times 10^{-1}\) (bottom panel)" width="80%" />
<p class="caption">
Figure 9.15: <strong>Trace plots based on three different proposals: <span class="math inline">\(\sigma^2 = 1 \times 10^{-7}\)</span> (top panel), <span class="math inline">\(\sigma^2=1\times 10^{-4}\)</span> (middle panel), and <span class="math inline">\(\sigma^2 = 1 \times 10^{-1}\)</span> (bottom panel)</strong>
</p>
</div>
<p>*** Different variance parameters lead to different results. In this example, if <span class="math inline">\(\delta^2\)</span> is too small, then the experimenter tends to draw samples that are very similar from one iteration to the other. This increases the acceptance rate (i.e., the rate at which we accept the proposal), but also means that the chain is travelling slowly around the posterior distribution. This ultimately imply that it will take longer chains to visit the whole posterior distribution. One way to see this issue in practice is by computing autocorrelation coefficients for the sample of parameter (more details on this in Section 9.4.4). The top panel of Figure 9.1.5 indeed shows this strong autocorrelation and slow travelling around the posterior distribution.</p>
<p>On the other hand, if <span class="math inline">\(\delta^2\)</span> is too large, then the proposal are seldom accepted, and the chain will tend to stick—exhibiting long period for which the chain stays constant. For instance, the case with large proposal variance above leads to an acceptance rate of 1.2 percent, which is very low. The bottom panel of Figure 9.1.5 reports this issue.</p>
<p>The moderate proposal variance case reports an acceptance rate of 32.4 percent, which is not too high nor too low. The general behavior of this chain resembles that of a hairy caterpillar—a good sign—meaning that the mixing seems adequate and that we accept a decent amount of proposed values.</p>
<p>Finding the right proposal variance values for problems of interest requires some fine-tuning. As a general guideline, experimenters should target acceptance rates between 20 and 50 percent.
***</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig916"></span>
<img src="LDAEd2_files/figure-html/Fig916-1.png" alt="Posterior densities based on three different proposals: \(\sigma^2 = 1 \times 10^{-7}\) (top panel), \(\sigma^2=1\times 10^{-4}\) (middle panel), and \(\sigma^2 = 1 \times 10^{-1}\) (bottom panel)" width="80%" />
<p class="caption">
Figure 9.16: <strong>Posterior densities based on three different proposals: <span class="math inline">\(\sigma^2 = 1 \times 10^{-7}\)</span> (top panel), <span class="math inline">\(\sigma^2=1\times 10^{-4}\)</span> (middle panel), and <span class="math inline">\(\sigma^2 = 1 \times 10^{-1}\)</span> (bottom panel)</strong>
</p>
</div>
<p>*** Using the wrong proposal distribution can have an impact on the computational efficiency of the Metropolis–Hastings algorithm, as shown in Figure 9.16. A small variance takes a long time to travel throughout the posterior distribution, whereas a large variance tends to stick.
***</p>
</div>
<hr />
<p><strong>Example 9.4.3. Impact of Initial Parameters.</strong>
Consider the motorcycle insurance data from Wasa used in Example 9.4.1. We wish to model the claim amount from motorcycle losses with a gamma distribution; that is,
<span class="math display">\[
f_{X_i\,|\,\Theta = \theta,\, A = \alpha}(x_i) = \frac{1}{\theta^{\alpha}\Gamma(\alpha)} x_i^{\alpha-1} e^{-\frac{x_i}{\theta}},
\]</span>
where <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i^{\text{th}}\)</span> claim amount. We assume that the prior distributions for both <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\alpha\)</span> are noninformative and flat; that is,
<span class="math display">\[
f_{\Theta , A}( \theta,\alpha) \propto 1, \quad \theta \in [0,\infty], \quad \alpha \in [0,\infty].
\]</span>
Find the posterior distribution of the parameter using 1,000 iterations of the Metropolis–Hastings sampler. Use a normal proposal with a proposal variance <span class="math inline">\(5 \times 10^7\)</span> for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(1 \times 10^{-2}\)</span> for <span class="math inline">\(\alpha\)</span>, and rely on <span class="math inline">\(\theta^{(0)} = 50,000\)</span> and <span class="math inline">\(\alpha^{(0)} = 0.5\)</span> to start the Metropolis–Hastings sampler. Redo the experiment with <span class="math inline">\(\theta^{(0)} = 10,000\)</span> and <span class="math inline">\(\alpha^{(0)} = 2.5\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.4.3" href="javascript:toggleEX('toggleExample.9.4.3','displayExample.9.4.3');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.4.3" style="display: none">
<p><em>Example Solution.</em> Starting from the the likelihood function and the prior distribution, we have that
<span class="math display">\[
h(\theta,\alpha) \propto \prod_{i=1}^N \frac{1}{\theta^{\alpha}\Gamma(\alpha)} x_i^{\alpha-1} e^{-\frac{x_i}{\theta}}.
\]</span></p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="ChBayes.html#cb141-1" tabindex="-1"></a>dataOhlsson <span class="ot">&lt;-</span> dataOhlsson[dataOhlsson<span class="sc">$</span>skadkost <span class="sc">&gt;</span> <span class="dv">0</span>, ]</span>
<span id="cb141-2"><a href="ChBayes.html#cb141-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> dataOhlsson<span class="sc">$</span>skadkost</span>
<span id="cb141-3"><a href="ChBayes.html#cb141-3" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb141-4"><a href="ChBayes.html#cb141-4" tabindex="-1"></a></span>
<span id="cb141-5"><a href="ChBayes.html#cb141-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb141-6"><a href="ChBayes.html#cb141-6" tabindex="-1"></a>sigma2theta <span class="ot">&lt;-</span> <span class="dv">50000000</span></span>
<span id="cb141-7"><a href="ChBayes.html#cb141-7" tabindex="-1"></a>sigma2alpha <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb141-8"><a href="ChBayes.html#cb141-8" tabindex="-1"></a></span>
<span id="cb141-9"><a href="ChBayes.html#cb141-9" tabindex="-1"></a><span class="co"># First set of initial values</span></span>
<span id="cb141-10"><a href="ChBayes.html#cb141-10" tabindex="-1"></a>thetas1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb141-11"><a href="ChBayes.html#cb141-11" tabindex="-1"></a>thetas1[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb141-12"><a href="ChBayes.html#cb141-12" tabindex="-1"></a>alphas1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb141-13"><a href="ChBayes.html#cb141-13" tabindex="-1"></a>alphas1[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb141-14"><a href="ChBayes.html#cb141-14" tabindex="-1"></a></span>
<span id="cb141-15"><a href="ChBayes.html#cb141-15" tabindex="-1"></a>accepttheta1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb141-16"><a href="ChBayes.html#cb141-16" tabindex="-1"></a>acceptalpha1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb141-17"><a href="ChBayes.html#cb141-17" tabindex="-1"></a></span>
<span id="cb141-18"><a href="ChBayes.html#cb141-18" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb141-19"><a href="ChBayes.html#cb141-19" tabindex="-1"></a>    <span class="co"># Let us start with theta and compute the logarithm of h for the past value</span></span>
<span id="cb141-20"><a href="ChBayes.html#cb141-20" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">shape =</span> alphas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb141-21"><a href="ChBayes.html#cb141-21" tabindex="-1"></a></span>
<span id="cb141-22"><a href="ChBayes.html#cb141-22" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb141-23"><a href="ChBayes.html#cb141-23" tabindex="-1"></a>    thetastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> thetas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2theta))</span>
<span id="cb141-24"><a href="ChBayes.html#cb141-24" tabindex="-1"></a>    <span class="cf">if</span> (thetastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb141-25"><a href="ChBayes.html#cb141-25" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetastar, <span class="at">shape =</span> alphas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb141-26"><a href="ChBayes.html#cb141-26" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb141-27"><a href="ChBayes.html#cb141-27" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb141-28"><a href="ChBayes.html#cb141-28" tabindex="-1"></a>    }</span>
<span id="cb141-29"><a href="ChBayes.html#cb141-29" tabindex="-1"></a></span>
<span id="cb141-30"><a href="ChBayes.html#cb141-30" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb141-31"><a href="ChBayes.html#cb141-31" tabindex="-1"></a>    alphatheta <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb141-32"><a href="ChBayes.html#cb141-32" tabindex="-1"></a>    <span class="cf">if</span> (alphatheta <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb141-33"><a href="ChBayes.html#cb141-33" tabindex="-1"></a>        thetas1[m] <span class="ot">&lt;-</span> thetastar</span>
<span id="cb141-34"><a href="ChBayes.html#cb141-34" tabindex="-1"></a>        accepttheta1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb141-35"><a href="ChBayes.html#cb141-35" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb141-36"><a href="ChBayes.html#cb141-36" tabindex="-1"></a>        thetas1[m] <span class="ot">&lt;-</span> thetas1[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb141-37"><a href="ChBayes.html#cb141-37" tabindex="-1"></a>        accepttheta1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb141-38"><a href="ChBayes.html#cb141-38" tabindex="-1"></a>    }</span>
<span id="cb141-39"><a href="ChBayes.html#cb141-39" tabindex="-1"></a></span>
<span id="cb141-40"><a href="ChBayes.html#cb141-40" tabindex="-1"></a>    <span class="co"># And then deal with alpha and compute the logarithm of h for the past</span></span>
<span id="cb141-41"><a href="ChBayes.html#cb141-41" tabindex="-1"></a>    <span class="co"># value</span></span>
<span id="cb141-42"><a href="ChBayes.html#cb141-42" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas1[m], <span class="at">shape =</span> alphas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb141-43"><a href="ChBayes.html#cb141-43" tabindex="-1"></a></span>
<span id="cb141-44"><a href="ChBayes.html#cb141-44" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb141-45"><a href="ChBayes.html#cb141-45" tabindex="-1"></a>    alphastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> alphas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2alpha))</span>
<span id="cb141-46"><a href="ChBayes.html#cb141-46" tabindex="-1"></a>    <span class="cf">if</span> (thetastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb141-47"><a href="ChBayes.html#cb141-47" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas1[m], <span class="at">shape =</span> alphastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb141-48"><a href="ChBayes.html#cb141-48" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb141-49"><a href="ChBayes.html#cb141-49" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb141-50"><a href="ChBayes.html#cb141-50" tabindex="-1"></a>    }</span>
<span id="cb141-51"><a href="ChBayes.html#cb141-51" tabindex="-1"></a></span>
<span id="cb141-52"><a href="ChBayes.html#cb141-52" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb141-53"><a href="ChBayes.html#cb141-53" tabindex="-1"></a>    alphaalpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb141-54"><a href="ChBayes.html#cb141-54" tabindex="-1"></a>    <span class="cf">if</span> (alphaalpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb141-55"><a href="ChBayes.html#cb141-55" tabindex="-1"></a>        alphas1[m] <span class="ot">&lt;-</span> alphastar</span>
<span id="cb141-56"><a href="ChBayes.html#cb141-56" tabindex="-1"></a>        acceptalpha1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb141-57"><a href="ChBayes.html#cb141-57" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb141-58"><a href="ChBayes.html#cb141-58" tabindex="-1"></a>        alphas1[m] <span class="ot">&lt;-</span> alphas1[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb141-59"><a href="ChBayes.html#cb141-59" tabindex="-1"></a>        acceptalpha1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb141-60"><a href="ChBayes.html#cb141-60" tabindex="-1"></a>    }</span>
<span id="cb141-61"><a href="ChBayes.html#cb141-61" tabindex="-1"></a>}</span>
<span id="cb141-62"><a href="ChBayes.html#cb141-62" tabindex="-1"></a></span>
<span id="cb141-63"><a href="ChBayes.html#cb141-63" tabindex="-1"></a><span class="co"># Second set of initial values</span></span>
<span id="cb141-64"><a href="ChBayes.html#cb141-64" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb141-65"><a href="ChBayes.html#cb141-65" tabindex="-1"></a>thetas2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb141-66"><a href="ChBayes.html#cb141-66" tabindex="-1"></a>thetas2[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb141-67"><a href="ChBayes.html#cb141-67" tabindex="-1"></a>alphas2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb141-68"><a href="ChBayes.html#cb141-68" tabindex="-1"></a>alphas2[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">2.5</span></span>
<span id="cb141-69"><a href="ChBayes.html#cb141-69" tabindex="-1"></a></span>
<span id="cb141-70"><a href="ChBayes.html#cb141-70" tabindex="-1"></a>accepttheta2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb141-71"><a href="ChBayes.html#cb141-71" tabindex="-1"></a>acceptalpha2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb141-72"><a href="ChBayes.html#cb141-72" tabindex="-1"></a></span>
<span id="cb141-73"><a href="ChBayes.html#cb141-73" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb141-74"><a href="ChBayes.html#cb141-74" tabindex="-1"></a>    <span class="co"># Let us start with theta and compute the logarithm of h for the past value</span></span>
<span id="cb141-75"><a href="ChBayes.html#cb141-75" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">shape =</span> alphas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb141-76"><a href="ChBayes.html#cb141-76" tabindex="-1"></a></span>
<span id="cb141-77"><a href="ChBayes.html#cb141-77" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb141-78"><a href="ChBayes.html#cb141-78" tabindex="-1"></a>    thetastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> thetas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2theta))</span>
<span id="cb141-79"><a href="ChBayes.html#cb141-79" tabindex="-1"></a>    <span class="cf">if</span> (thetastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb141-80"><a href="ChBayes.html#cb141-80" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetastar, <span class="at">shape =</span> alphas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb141-81"><a href="ChBayes.html#cb141-81" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb141-82"><a href="ChBayes.html#cb141-82" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb141-83"><a href="ChBayes.html#cb141-83" tabindex="-1"></a>    }</span>
<span id="cb141-84"><a href="ChBayes.html#cb141-84" tabindex="-1"></a></span>
<span id="cb141-85"><a href="ChBayes.html#cb141-85" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb141-86"><a href="ChBayes.html#cb141-86" tabindex="-1"></a>    alphatheta <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb141-87"><a href="ChBayes.html#cb141-87" tabindex="-1"></a>    <span class="cf">if</span> (alphatheta <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb141-88"><a href="ChBayes.html#cb141-88" tabindex="-1"></a>        thetas2[m] <span class="ot">&lt;-</span> thetastar</span>
<span id="cb141-89"><a href="ChBayes.html#cb141-89" tabindex="-1"></a>        accepttheta2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb141-90"><a href="ChBayes.html#cb141-90" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb141-91"><a href="ChBayes.html#cb141-91" tabindex="-1"></a>        thetas2[m] <span class="ot">&lt;-</span> thetas2[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb141-92"><a href="ChBayes.html#cb141-92" tabindex="-1"></a>        accepttheta2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb141-93"><a href="ChBayes.html#cb141-93" tabindex="-1"></a>    }</span>
<span id="cb141-94"><a href="ChBayes.html#cb141-94" tabindex="-1"></a></span>
<span id="cb141-95"><a href="ChBayes.html#cb141-95" tabindex="-1"></a>    <span class="co"># And then deal with alpha and compute the logarithm of h for the past</span></span>
<span id="cb141-96"><a href="ChBayes.html#cb141-96" tabindex="-1"></a>    <span class="co"># value</span></span>
<span id="cb141-97"><a href="ChBayes.html#cb141-97" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas1[m], <span class="at">shape =</span> alphas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb141-98"><a href="ChBayes.html#cb141-98" tabindex="-1"></a></span>
<span id="cb141-99"><a href="ChBayes.html#cb141-99" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb141-100"><a href="ChBayes.html#cb141-100" tabindex="-1"></a>    alphastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> alphas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2alpha))</span>
<span id="cb141-101"><a href="ChBayes.html#cb141-101" tabindex="-1"></a>    <span class="cf">if</span> (thetastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb141-102"><a href="ChBayes.html#cb141-102" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas2[m], <span class="at">shape =</span> alphastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb141-103"><a href="ChBayes.html#cb141-103" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb141-104"><a href="ChBayes.html#cb141-104" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb141-105"><a href="ChBayes.html#cb141-105" tabindex="-1"></a>    }</span>
<span id="cb141-106"><a href="ChBayes.html#cb141-106" tabindex="-1"></a></span>
<span id="cb141-107"><a href="ChBayes.html#cb141-107" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb141-108"><a href="ChBayes.html#cb141-108" tabindex="-1"></a>    alphaalpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb141-109"><a href="ChBayes.html#cb141-109" tabindex="-1"></a>    <span class="cf">if</span> (alphaalpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb141-110"><a href="ChBayes.html#cb141-110" tabindex="-1"></a>        alphas2[m] <span class="ot">&lt;-</span> alphastar</span>
<span id="cb141-111"><a href="ChBayes.html#cb141-111" tabindex="-1"></a>        acceptalpha2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb141-112"><a href="ChBayes.html#cb141-112" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb141-113"><a href="ChBayes.html#cb141-113" tabindex="-1"></a>        alphas2[m] <span class="ot">&lt;-</span> alphas2[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb141-114"><a href="ChBayes.html#cb141-114" tabindex="-1"></a>        acceptalpha2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb141-115"><a href="ChBayes.html#cb141-115" tabindex="-1"></a>    }</span>
<span id="cb141-116"><a href="ChBayes.html#cb141-116" tabindex="-1"></a>}</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig917"></span>
<img src="LDAEd2_files/figure-html/Fig917-1.png" alt="Trace plots based on two different starting parameter sets: \(\theta^{(0)} = 50,000\) and \(\alpha^{(0)} = 0.5\) (left panels), and \(\theta^{(0)} = 10,000\) and \(\alpha^{(0)} = 2.5\) (right panels)" width="80%" />
<p class="caption">
Figure 9.17: <strong>Trace plots based on two different starting parameter sets: <span class="math inline">\(\theta^{(0)} = 50,000\)</span> and <span class="math inline">\(\alpha^{(0)} = 0.5\)</span> (left panels), and <span class="math inline">\(\theta^{(0)} = 10,000\)</span> and <span class="math inline">\(\alpha^{(0)} = 2.5\)</span> (right panels)</strong>
</p>
</div>
<p>*** Clearly, from Figure 9.17, the initial parameter value matters: for the first set, the starting value is close to the posterior mode, meaning that the final sample does not depend much on the starting value. For the second set, on the other hand, it takes about 200 iterations to get closer to where most of the density resides. Having a burn-in in the case of Metropolis–Hastings sampler is therefore a good idea to reduce the impact of initial guesses on the final posterior distribution.
***</p>
</div>
<hr />
<p>In the next subsection, we learn a few methods and metrics to diagnose the convergence of the Markov chains generated via MCMC methods.</p>
</div>
<div id="S:Sec944" class="section level3 hasAnchor" number="9.4.4">
<h3><span class="header-section-number">9.4.4</span> Markov Chain Diagnostics<a href="ChBayes.html#S:Sec944" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are many different tuning parameters in MCMC schemes, and they all have an impact on the convergence of the Markov chains generated by these methods. To understand the impact of these choices on the chains (e.g., number of iterations, length of burn-in, proposal distribution), we introduce a few methods to analyze their convergence.</p>
<div id="examining-trace-plots-and-autocorrelation" class="section level4 unnumbered hasAnchor">
<h4>Examining Trace Plots and Autocorrelation<a href="ChBayes.html#examining-trace-plots-and-autocorrelation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Trace Plot.</strong> The most elementary tool to assess whether MCMC chains have converged to the posterior distribution is the trace plot. As mentioned above, a trace plot displays the sequence of samples as a function of the iteration number, with the sample value on the <span class="math inline">\(y\)</span>-axis and the iteration number on the <span class="math inline">\(x\)</span>-axis. If the chain has converged, the trace plot should show a stable sequence of samples around the true posterior distribution that looks like a hairy caterpillar. However, if the chain has not yet converged, the trace plot may show a sequence of samples that still appear to be changing or have not yet settled into a stable pattern.</p>
<p>In addition to assessing convergence, trace plots can also be used to diagnose potential problems with MCMC algorithms, such as poor mixing or autocorrelation. For example, if the trace plot shows long periods of no change followed by abrupt jumps, this may indicate poor mixing and suggest that the MCMC algorithm needs to be adjusted or a different method should be used.</p>
<p><strong>Lag-1 Autocorrelation.</strong> Another quantity that might be helpful is the lag-1 autocorrelation—the correlation between consecutive samples in a given chain:
<span class="math display">\[
\mathrm{Cov}\left[ \theta_i^{(m)}, \theta_i^{(m-1)} \right].
\]</span>
Note that if the autocorrelation is too high, it can indicate that the chain is not mixing well and is not sampling the posterior distribution effectively. This can result in poor convergence, longer run times, and decreased precision of the estimates obtained from the MCMC algorithm.</p>
<p>In addition to examining trace plots and computing autocorrelation coefficients, we can use other, more formal tools to evaluate whether the chains obtained are reliable and have converged.</p>
</div>
<div id="comparing-parallel-chains" class="section level4 unnumbered hasAnchor">
<h4>Comparing Parallel Chains<a href="ChBayes.html#comparing-parallel-chains" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Gelman–Rubin Statistic</strong> Another way to assess convergence is to run multiple chains in parallel from different starting points and check if their behavior is similar. In addition to comparing their trace plots, the chains can be compared by using a statistical test—the Gelman–Rubin test of <span class="citation">Gelman and Rubin (<a href="bibliography.html#ref-gelman1992inference">1992</a>)</span>. The latter test compares the within-chain variance to the between-chain variance; to calculate the statistic, we need to generate a small number of chains (say, <span class="math inline">\(R\)</span>), each for <span class="math inline">\(M-M^*\)</span> post-burn-in iterations.</p>
<p>If the chains have converged, the within-chain variance should be similar to the between-chain variance. Assuming the parameter of interest is <span class="math inline">\(\theta_i\)</span>, the within-chain variance is
<span class="math display">\[
W = \frac{1}{R(M-M^*-1)} \sum_{r=1}^R \sum_{m=M^*+1}^{M} \left( \theta_{i,r}^{(m)} - \overline{\theta}_{i,r} \right)^2 ,
\]</span>
where <span class="math inline">\(\theta_{i,r}^{(m)}\)</span> is the <span class="math inline">\(m^{\text{th}}\)</span> draw of <span class="math inline">\(\theta_i\)</span> in the <span class="math inline">\(r^{\text{th}}\)</span> chain and <span class="math inline">\(\overline{\theta}_{i,r}\)</span> is the sample mean of <span class="math inline">\(\theta_i\)</span> for the <span class="math inline">\(r^{\text{th}}\)</span> chain. The between-chain variance is given by
<span class="math display">\[
B = \frac{M-M^*}{R-1} \sum_{r=1}^R \left( \overline{\theta}_{i,r} - \overline{\theta}_{i} \right),
\]</span>
where <span class="math inline">\(\overline{\theta}_{i}\)</span> is the overall sample mean of <span class="math inline">\(\theta_i\)</span> from all chains. The Gelman–Rubin statistic is
<span class="math display">\[
\sqrt{\left( \frac{M-M^*-1}{M-M^*} + \frac{R+1}{R(M-M^*)} \frac{B}{W} \right) \frac{\text{df}}{\text{df}-2} },
\]</span>
where <span class="math inline">\(\text{df}\)</span> is the degrees of freedom from Student’s <span class="math inline">\(t\)</span>-distribution that approximates the posterior distribution. The statistic should produce a value close to 1 if the chain has converged. On the other hand, if the statistic value is greater than 1.1 or 1.2, this indicates that the chains may not have converged, and further analysis may be needed to determine why the chains are not mixing well.</p>
</div>
<div id="calculating-effective-sample-sizes" class="section level4 unnumbered hasAnchor">
<h4>Calculating Effective Sample Sizes<a href="ChBayes.html#calculating-effective-sample-sizes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Effective Sample Size.</strong> The effective sample size (ESS) is a measure of the number of independent samples obtained from an MCMC chain. Recall that in an MCMC chain, each sample is correlated with the previous sample; as a result, the effective number of independent samples is usually much smaller than the total number of samples generated by the MCMC algorithm. The ESS takes this correlation into account and provides an estimate of the number of independent samples that are equivalent to the correlated samples in the chain.</p>
<p>In general, a higher effective sample size indicates that the MCMC algorithm has produced more independent samples and is more likely to have accurately sampled the posterior distribution. A lower effective sample size, on the other hand, suggests that the MCMC algorithm may require further tuning or optimization to produce reliable posterior estimates.</p>
<p>The function <code>multiESS</code>of the R package <code>mcmcse</code> contains a function that gives the ESS of a multivariate Markov chain as described in <span class="citation">Vats, Flegal, and Jones (<a href="bibliography.html#ref-vats2019multivariate">2019</a>)</span>. The package also includes an estimate of the minimum ESS required for a specified relative tolerance level (see function <code>minESS</code>).</p>
<p>We now apply these various diagnostics to an example.</p>
<hr />
<p><strong>Example 9.4.4. Markov Chain Diagnostics.</strong> Consider the setup of Example 9.4.2. Using chains of 51,000 iterations and a burn-in of 1,000 iterations, calculate the various Markov chain diagnostics mentioned above.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.4.4" href="javascript:toggleEX('toggleExample.9.4.4','displayExample.9.4.4');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.4.4" style="display: none">
<p><span class="math inline">\(\textbf{Example~Solution}\)</span>. Let us begin by generating five chains.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="ChBayes.html#cb142-1" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">51000</span></span>
<span id="cb142-2"><a href="ChBayes.html#cb142-2" tabindex="-1"></a>Mstar <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb142-3"><a href="ChBayes.html#cb142-3" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb142-4"><a href="ChBayes.html#cb142-4" tabindex="-1"></a></span>
<span id="cb142-5"><a href="ChBayes.html#cb142-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb142-6"><a href="ChBayes.html#cb142-6" tabindex="-1"></a>delta2 <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb142-7"><a href="ChBayes.html#cb142-7" tabindex="-1"></a></span>
<span id="cb142-8"><a href="ChBayes.html#cb142-8" tabindex="-1"></a>lambdas <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="cn">NA</span>, <span class="at">ncol =</span> M <span class="sc">+</span> <span class="dv">1</span>, <span class="at">nrow =</span> R)</span>
<span id="cb142-9"><a href="ChBayes.html#cb142-9" tabindex="-1"></a>accept <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="cn">NA</span>, <span class="at">ncol =</span> M, <span class="at">nrow =</span> R)</span>
<span id="cb142-10"><a href="ChBayes.html#cb142-10" tabindex="-1"></a></span>
<span id="cb142-11"><a href="ChBayes.html#cb142-11" tabindex="-1"></a><span class="cf">for</span> (r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>R) {</span>
<span id="cb142-12"><a href="ChBayes.html#cb142-12" tabindex="-1"></a>    lambdas[r, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fl">0.000001</span>, <span class="fu">mean</span>(sgautonb<span class="sc">$</span>Clm_Count) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>,</span>
<span id="cb142-13"><a href="ChBayes.html#cb142-13" tabindex="-1"></a>        <span class="at">sd =</span> <span class="fl">0.1</span>))</span>
<span id="cb142-14"><a href="ChBayes.html#cb142-14" tabindex="-1"></a></span>
<span id="cb142-15"><a href="ChBayes.html#cb142-15" tabindex="-1"></a>    <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb142-16"><a href="ChBayes.html#cb142-16" tabindex="-1"></a>        <span class="co"># Compute logarithm of h for past value</span></span>
<span id="cb142-17"><a href="ChBayes.html#cb142-17" tabindex="-1"></a>        loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdas[r, m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb142-18"><a href="ChBayes.html#cb142-18" tabindex="-1"></a></span>
<span id="cb142-19"><a href="ChBayes.html#cb142-19" tabindex="-1"></a>        <span class="co"># Generate proposed parameter and compute logarithm of h for proposed</span></span>
<span id="cb142-20"><a href="ChBayes.html#cb142-20" tabindex="-1"></a>        <span class="co"># value</span></span>
<span id="cb142-21"><a href="ChBayes.html#cb142-21" tabindex="-1"></a>        lambdastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> lambdas[r, m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(delta2))</span>
<span id="cb142-22"><a href="ChBayes.html#cb142-22" tabindex="-1"></a>        <span class="cf">if</span> (lambdastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb142-23"><a href="ChBayes.html#cb142-23" tabindex="-1"></a>            loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb142-24"><a href="ChBayes.html#cb142-24" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb142-25"><a href="ChBayes.html#cb142-25" tabindex="-1"></a>            loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb142-26"><a href="ChBayes.html#cb142-26" tabindex="-1"></a>        }</span>
<span id="cb142-27"><a href="ChBayes.html#cb142-27" tabindex="-1"></a></span>
<span id="cb142-28"><a href="ChBayes.html#cb142-28" tabindex="-1"></a>        <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb142-29"><a href="ChBayes.html#cb142-29" tabindex="-1"></a>        alpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb142-30"><a href="ChBayes.html#cb142-30" tabindex="-1"></a>        <span class="cf">if</span> (alpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb142-31"><a href="ChBayes.html#cb142-31" tabindex="-1"></a>            lambdas[r, m] <span class="ot">&lt;-</span> lambdastar</span>
<span id="cb142-32"><a href="ChBayes.html#cb142-32" tabindex="-1"></a>            accept[r, m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb142-33"><a href="ChBayes.html#cb142-33" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb142-34"><a href="ChBayes.html#cb142-34" tabindex="-1"></a>            lambdas[r, m] <span class="ot">&lt;-</span> lambdas[r, m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb142-35"><a href="ChBayes.html#cb142-35" tabindex="-1"></a>            accept[r, m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb142-36"><a href="ChBayes.html#cb142-36" tabindex="-1"></a>        }</span>
<span id="cb142-37"><a href="ChBayes.html#cb142-37" tabindex="-1"></a>    }</span>
<span id="cb142-38"><a href="ChBayes.html#cb142-38" tabindex="-1"></a>}</span>
<span id="cb142-39"><a href="ChBayes.html#cb142-39" tabindex="-1"></a></span>
<span id="cb142-40"><a href="ChBayes.html#cb142-40" tabindex="-1"></a><span class="fu">save</span>(lambdas, accept, sgautonb, alpha, lambdastar, loghpast, loghstar, m, M, Mstar,</span>
<span id="cb142-41"><a href="ChBayes.html#cb142-41" tabindex="-1"></a>    r, R, delta2, <span class="at">file =</span> <span class="st">&quot;../IntermediateCalcs/BayesChap/Example844.Rdata&quot;</span>)</span></code></pre></div>
<p>Figure <a href="ChBayes.html#fig:Fig918">9.18</a> reports the trace plot for the first chain: it indeed looks like a hairy caterpillar, which is a good sign.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig918"></span>
<img src="LDAEd2_files/figure-html/Fig918-1.png" alt="Trace plot for parameter \(\lambda\)" width="80%" />
<p class="caption">
Figure 9.18: <strong>Trace plot for parameter <span class="math inline">\(\lambda\)</span></strong>
</p>
</div>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="ChBayes.html#cb143-1" tabindex="-1"></a>autocorr <span class="ot">&lt;-</span> <span class="fu">acf</span>(lambdas[<span class="dv">1</span>, (Mstar <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>M], <span class="at">lag.max =</span> <span class="dv">1</span>, <span class="at">plot =</span> <span class="cn">FALSE</span>)</span>
<span id="cb143-2"><a href="ChBayes.html#cb143-2" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The lag-1 autocorrelation coefficient is&quot;</span>, autocorr<span class="sc">$</span>acf[<span class="dv">2</span>])</span></code></pre></div>
<pre><code>The lag-1 autocorrelation coefficient is 0.651510865</code></pre>
<p>The autocorrelation is also mild at 65%, again pointing towards good convergence behavior.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="ChBayes.html#cb145-1" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb145-2"><a href="ChBayes.html#cb145-2" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb145-3"><a href="ChBayes.html#cb145-3" tabindex="-1"></a><span class="cf">for</span> (r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>R) {</span>
<span id="cb145-4"><a href="ChBayes.html#cb145-4" tabindex="-1"></a>    W <span class="ot">&lt;-</span> W <span class="sc">+</span> <span class="dv">1</span><span class="sc">/</span>(R <span class="sc">*</span> (M <span class="sc">-</span> Mstar <span class="sc">-</span> <span class="dv">1</span>)) <span class="sc">*</span> <span class="fu">sum</span>((lambdas[r, (Mstar <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>M] <span class="sc">-</span> <span class="fu">mean</span>(lambdas[r,</span>
<span id="cb145-5"><a href="ChBayes.html#cb145-5" tabindex="-1"></a>        (Mstar <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>M]))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb145-6"><a href="ChBayes.html#cb145-6" tabindex="-1"></a>    B <span class="ot">&lt;-</span> B <span class="sc">+</span> (M <span class="sc">-</span> Mstar)<span class="sc">/</span>(R <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> <span class="fu">sum</span>((<span class="fu">mean</span>(lambdas[r, (Mstar <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>M]) <span class="sc">-</span> <span class="fu">mean</span>(lambdas[,</span>
<span id="cb145-7"><a href="ChBayes.html#cb145-7" tabindex="-1"></a>        (Mstar <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>M]))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb145-8"><a href="ChBayes.html#cb145-8" tabindex="-1"></a>}</span>
<span id="cb145-9"><a href="ChBayes.html#cb145-9" tabindex="-1"></a><span class="co"># Assuming that df/(df-2) tends to 1</span></span>
<span id="cb145-10"><a href="ChBayes.html#cb145-10" tabindex="-1"></a>GR <span class="ot">&lt;-</span> <span class="fu">sqrt</span>((M <span class="sc">-</span> Mstar <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span>(M <span class="sc">-</span> Mstar) <span class="sc">+</span> (R <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">/</span>(R <span class="sc">*</span> (M <span class="sc">-</span> Mstar)) <span class="sc">*</span> B<span class="sc">/</span>W)</span>
<span id="cb145-11"><a href="ChBayes.html#cb145-11" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The Gelman-Rubin statistic is&quot;</span>, GR)</span></code></pre></div>
<pre><code>The Gelman-Rubin statistic is 1.00012696</code></pre>
<p>The Gelman–Rubin statistic is very close to 1 in this case, meaning that the chains converged.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="ChBayes.html#cb147-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;mcmcse&quot;</span>)</span>
<span id="cb147-2"><a href="ChBayes.html#cb147-2" tabindex="-1"></a>ess <span class="ot">&lt;-</span> <span class="fu">multiESS</span>(<span class="fu">matrix</span>(lambdas[<span class="dv">1</span>, ]))</span>
<span id="cb147-3"><a href="ChBayes.html#cb147-3" tabindex="-1"></a>mess <span class="ot">&lt;-</span> <span class="fu">minESS</span>(<span class="at">p =</span> <span class="dv">1</span>)</span>
<span id="cb147-4"><a href="ChBayes.html#cb147-4" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The ESS is&quot;</span>, ess, <span class="st">&quot;and the minimum ESS is&quot;</span>, mess)</span></code></pre></div>
<pre><code>The ESS is 9927.29934 and the minimum ESS is 6146</code></pre>
<p>The last diagnostic refers to the ESS, and its comparison to the minimum ESS. In our case, the ESS is about 9,927, and the minimum ESS is 6,146. Since our ESS is above the minimum, we know we have a large enough sample to adequately capture the posterior distribution of <span class="math inline">\(\lambda\)</span>.</p>
</div>
<hr />
</div>
</div>
</div>
<div id="S:Sec95" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Bayesian Statistics in Practice<a href="ChBayes.html#S:Sec95" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In Section <a href="ChBayes.html#S:Sec95">9.5</a>, you learn how to:</p>
<ul>
<li>Describe the main computing resources available for Bayesian statistics and modeling.</li>
<li>Apply one of them to loss data.</li>
</ul>
<hr />
<p>Fortunately for end users, some of these methods are readily available in R, meaning that they are quite accessible. Some popular computing resources used in Bayesian statistics are listed below:</p>
<ul>
<li><code>RSTAN</code>, named in honor of Stanislaw Ulam, is an R implementation of the widely used STAN probabilistic programming language for Bayesian statistical modeling and inference. It is highly flexible and allows users to define complex statistical models.</li>
<li><code>nimble</code> stands for Numerical Inference for Bayesian and Likelihood Estimation and is an R package designed for statistical computing and hierarchical modeling. <code>nimble</code> provides a high-level programming language that allows users to define complex statistical models with ease.</li>
<li><code>R2OpenBUGS</code> allows R users to use OpenBUGS, a classic and widely-used software package for Bayesian data analysis. It uses MCMC techniques like Gibbs sampling to obtain samples from the posterior distribution.</li>
<li><code>rjags</code> is an R implementation of the JAGS (Just Another Gibbs Sampler) program. It is an open-source software that was developed as an extension of BUGS. It provides a platform-independent engine for the BUGS language, allowing for the use of BUGS models in various environments. Like BUGS, <code>JAGS</code> is also used for Bayesian analysis through MCMC sampling techniques.</li>
</ul>
<p>In what follows, we will use the <code>nimble</code> package in the context of loss data.</p>
<hr />
<p><strong>Example 9.5.1. The <code>nimble</code> package.</strong> Similar to the setup of Example 9.4.2, consider that each policyholder’s claim count (frequency) is distributed as a Poisson random variable such that
<span class="math display">\[
p_{N_i\,|\,\Lambda=\lambda}(n_i) = \frac{\lambda^{n_i}e^{-\lambda}}{n_i!},
\]</span>
where <span class="math inline">\(n_i\)</span> is the number of claims associated with the <span class="math inline">\(i^{\text{th}}\)</span> policyholder. Unlike the previous example, however, let us assume an inverse gamma prior with a shape parameter of 2 and a scale parameter of 5.<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a></p>
<p>Find the posterior distribution of the parameter by creating a chain of 51,000 iterations and a burn-in of 1,000 iterations using the <code>nimble</code> package.</p>
<h5 style="text-align: center;">
<a id="displayExample.9.5.1" href="javascript:toggleEX('toggleExample.9.5.1','displayExample.9.5.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.9.5.1" style="display: none">
<p><em>Example Solution.</em>
First, we need to define the model using the <code>nimble</code> language. Simply put, the model is comprised of a likelihood density and a prior density. The former links the observations to a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>, and the latter states the prior distribution, which is inverse gamma with shape and scale parameters of 2 and 5, respectively.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="ChBayes.html#cb149-1" tabindex="-1"></a>claimmodel <span class="ot">&lt;-</span> <span class="fu">nimbleCode</span>({</span>
<span id="cb149-2"><a href="ChBayes.html#cb149-2" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb149-3"><a href="ChBayes.html#cb149-3" tabindex="-1"></a>        <span class="co"># Likelihood</span></span>
<span id="cb149-4"><a href="ChBayes.html#cb149-4" tabindex="-1"></a>        count[i] <span class="sc">~</span> <span class="fu">dpois</span>(lambda)</span>
<span id="cb149-5"><a href="ChBayes.html#cb149-5" tabindex="-1"></a>    }</span>
<span id="cb149-6"><a href="ChBayes.html#cb149-6" tabindex="-1"></a>    <span class="co"># Prior distribution</span></span>
<span id="cb149-7"><a href="ChBayes.html#cb149-7" tabindex="-1"></a>    lambda <span class="sc">~</span> <span class="fu">dinvgamma</span>(<span class="at">shape =</span> <span class="dv">2</span>, <span class="at">scale =</span> <span class="dv">5</span>)</span>
<span id="cb149-8"><a href="ChBayes.html#cb149-8" tabindex="-1"></a>})</span></code></pre></div>
<p>Then, we define the data, the constant (i.e., the number of observations in this case), the parameter list (i.e., only <span class="math inline">\(\lambda\)</span> here), and the initial value set to 0.05 in this illustration.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="ChBayes.html#cb150-1" tabindex="-1"></a>claimdata <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">count =</span> sgautonb<span class="sc">$</span>Clm_Count)</span>
<span id="cb150-2"><a href="ChBayes.html#cb150-2" tabindex="-1"></a>claimconstant <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">N =</span> <span class="fu">length</span>(sgautonb<span class="sc">$</span>Clm_Count))</span>
<span id="cb150-3"><a href="ChBayes.html#cb150-3" tabindex="-1"></a>claimparameters <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;lambda&quot;</span>)</span>
<span id="cb150-4"><a href="ChBayes.html#cb150-4" tabindex="-1"></a>claiminitial <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">lambda =</span> <span class="fl">0.05</span>)</span></code></pre></div>
<p>The MCMC chain is then run using for 51,000 iterations and a burn-in of 1,000 iterations.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="ChBayes.html#cb151-1" tabindex="-1"></a>mcmcoutput <span class="ot">&lt;-</span> <span class="fu">nimbleMCMC</span>(<span class="at">code =</span> claimmodel, <span class="at">data =</span> claimdata, <span class="at">constants =</span> claimconstant,</span>
<span id="cb151-2"><a href="ChBayes.html#cb151-2" tabindex="-1"></a>    <span class="at">inits =</span> claiminitial, <span class="at">monitors =</span> claimparameters, <span class="at">niter =</span> <span class="dv">51000</span>, <span class="at">nburnin =</span> <span class="dv">1000</span>,</span>
<span id="cb151-3"><a href="ChBayes.html#cb151-3" tabindex="-1"></a>    <span class="at">nchains =</span> <span class="dv">1</span>)</span>
<span id="cb151-4"><a href="ChBayes.html#cb151-4" tabindex="-1"></a><span class="fu">save</span>(mcmcoutput, <span class="at">file =</span> <span class="st">&quot;../IntermediateCalcs/BayesChap/Example951.Rdata&quot;</span>)</span></code></pre></div>
<p>Finally, we display the trace plot, obtain the histogram of the posterior distribution of <span class="math inline">\(\lambda\)</span>, and compute some descriptive statistics of the parameter.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Fig919"></span>
<img src="LDAEd2_files/figure-html/Fig919-1.png" alt="Trace plot and posterior density for parameter \(\lambda\)" width="80%" />
<p class="caption">
Figure 9.19: <strong>Trace plot and posterior density for parameter <span class="math inline">\(\lambda\)</span></strong>
</p>
</div>
<pre><code>The posterior mean of the parameter is 0.0781913941</code></pre>
<pre><code>The posterior standard deviation of the parameter is 0.00309462947</code></pre>
</div>
<p>This simple illustration demonstrates the simplicity of utilizing R packages to generate MCMC chains, all without the need for writing extensive code. For more details on the <code>nimble</code> package, see <span class="citation">Valpine et al. (<a href="bibliography.html#ref-de2017programming">2017</a>)</span>.</p>
</div>
<div id="S:Sec96" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Further Resources and Contributors<a href="ChBayes.html#S:Sec96" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Many great books exist on Bayesian statistics and MCMC schemes. We refer the interested reader to <span class="citation">Bernardo and Smith (<a href="bibliography.html#ref-bernardo2009bayesian">2009</a>)</span> and <span class="citation">Robert and Casella (<a href="bibliography.html#ref-robert1999monte">1999</a>)</span> for an advanced treatment of these topics.</p>
<p>A number of academic articles in actuarial science relied on Bayesian statistics and MCMC schemes over the past 40 years; see, for instance, <span class="citation">Heckman and Meyers (<a href="bibliography.html#ref-heckman1983calculation">1983</a>)</span>, <span class="citation">Meyers and Schenker (<a href="bibliography.html#ref-meyers1983parameter">1983</a>)</span>, <span class="citation">Cairns (<a href="bibliography.html#ref-cairns2000discussion">2000</a>)</span>, <span class="citation">Cairns, Blake, and Dowd (<a href="bibliography.html#ref-cairns2006two">2006</a>)</span>, <span class="citation">Hartman and Heaton (<a href="bibliography.html#ref-hartman2011accounting">2011</a>)</span>, <span class="citation">Bermúdez and Karlis (<a href="bibliography.html#ref-bermudez2011bayesian">2011</a>)</span>, <span class="citation">Hartman and Groendyke (<a href="bibliography.html#ref-hartman2013model">2013</a>)</span>, <span class="citation">Fellingham, Kottas, and Hartman (<a href="bibliography.html#ref-fellingham2015bayesian">2015</a>)</span>, <span class="citation">Bignozzi and Tsanakas (<a href="bibliography.html#ref-bignozzi2016parameter">2016</a>)</span>, <span class="citation">Bégin (<a href="bibliography.html#ref-begin2019economic">2019</a>)</span>, <span class="citation">Huang and Meng (<a href="bibliography.html#ref-huang2020bayesian">2020</a>)</span>, <span class="citation">Bégin (<a href="bibliography.html#ref-begin2021complex">2021</a>)</span>, <span class="citation">Cheung et al. (<a href="bibliography.html#ref-cheung2021bayesian">2021</a>)</span>, and <span class="citation">Bégin (<a href="bibliography.html#ref-begin2023ensemble">2023</a>)</span>.</p>
<div id="contributors-8" class="section level3 unnumbered hasAnchor">
<h3>Contributors<a href="ChBayes.html#contributors-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>Jean-François Bégin</strong>, Simon Fraser University, is the principal author of the initial version of this chapter. Email: <a href="mailto:jbegin@sfu.ca" class="email">jbegin@sfu.ca</a> for chapter comments and suggested improvements.</li>
<li>Chapter reviewers include: Brian Hartman, Chun Yong, Margie Rosenberg, and Gary Dean.</li>
</ul>

<!-- # Chap 1 -->
<!-- # Chap 2 -->
<!-- # Chap 3 -->
<!-- # Chap 4 -->
<!-- # Chap 5 -->
<!-- # Chap 6 -->
<!-- # Chap 7 -->
<!-- # Chap 8 -->
<!-- # Chap 9 -->
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>Each coin toss can be seen as a Bernoulli random variable, meaning that their sum is a binomial with parameters <span class="math inline">\(q=0.5\)</span> and <span class="math inline">\(m=5\)</span>. See Chapter <a href="ChapSummaryDistributions.html#S:DiscreteDistributions">20.1</a> for more details.<a href="ChBayes.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>Specifically, we use a uniform over <span class="math inline">\([0,1]\)</span> for our prior distribution. As explained in Section <a href="ChBayes.html#S:Sec923">9.2.3</a>, this type of prior is said to be noninformative.<a href="ChBayes.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>This is also an application of the beta–binomial conjugate family that will be explained in Section <a href="ChBayes.html#S:Sec931">9.3.1</a><a href="ChBayes.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>There is also a rich history blending prior information with data in loss modeling and in actuarial science, generally speaking; it is known as credibility. In technical terms, credibility theory’s main challenge lies in identifying the optimal linear approximation to the mean of the Bayesian predictive density. This is the reason credibility theory shares numerous outcomes with both linear filtering and the broader field of Bayesian statistics. For more details on experience rating using credibility theory, see Chapter <a href="ChapCredibility.html#ChapCredibility">12</a>.<a href="ChBayes.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>This question was adapted from the Be An Actuary website. See <a href="https://www.beanactuary.org/do_the_math/do-the-math-q4/">here</a> for more details.<a href="ChBayes.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>The law of total probability states that the total probability of an event <span class="math inline">\(B\)</span> is equal to the sum of the probabilities of <span class="math inline">\(B\)</span> occurring under different conditions, weighted by the probabilities of those conditions. In the case where there are only two different conditions (let us say <span class="math inline">\(A\)</span> and <span class="math inline">\(A^{\text{c}}\)</span>), we simply need to consider these two conditions. In all generality, however, we would need to consider more possibilities if the sample space cannot be divided into only two events.<a href="ChBayes.html#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>This question was taken from the Society of Actuaries Sample Questions for Exam P. See <a href="https://www.soa.org/globalassets/assets/files/edu/edu-exam-p-sample-quest.pdf">here</a> for more details.<a href="ChBayes.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>The data are from the General Insurance Association of Singapore, an organization consisting of non-life insurers in Singapore. These data contain the number of car accidents for <span class="math inline">\(n=7{,}483\)</span> auto insurance policies with several categorical explanatory variables and the exposure for each policy.<a href="ChBayes.html#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p>For the sake of simplicity, we only consider one parameter in our derivation here. Note that, later, we will consider cases with more than one parameter and that this extension does not change the bulk of our results and derivations.<a href="ChBayes.html#fnref17" class="footnote-back">↩︎</a></p></li>
<li id="fn18"><p>This question is a modified version of Sample Question 184 of the Society of Actuaries Exam C sample questions.<a href="ChBayes.html#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn19"><p>Here, we assume that the domain of the beta is <span class="math inline">\([0,1]\)</span>, meaning that <span class="math inline">\(\theta = 1\)</span>. For more details, see Chapter <a href="ChapSummaryDistributions.html#ChapSummaryDistributions">20</a>.<a href="ChBayes.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>This question is Sample Question 5 of the Society of Actuaries Exam C sample questions.<a href="ChBayes.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>Conjugate families for the normal distribution with unknown <span class="math inline">\(\sigma^2\)</span> can also be derived. For the sake of simplicity, we will only focus on the case with known variance parameter in this book.<a href="ChBayes.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p>For an overview of the theory behind MCMC methods, see <span class="citation">Robert and Casella (<a href="bibliography.html#ref-robert1999monte">1999</a>)</span>.<a href="ChBayes.html#fnref22" class="footnote-back">↩︎</a></p></li>
<li id="fn23"><p>The Gibbs sampler can be seen as a special case of the more general Metropolis–Hastings algorithm. Specifically, with Gibbs’ method, all proposals are automatically accepted; that is, <span class="math inline">\(\alpha\left(\theta_1^{(0)},\theta_1^*\right)=1\)</span>.<a href="ChBayes.html#fnref23" class="footnote-back">↩︎</a></p></li>
<li id="fn24"><p>Note that the inverse gamma prior combined with a Poisson distribution does not generally lead to closed-form posterior densities and thus requires us to use MCMC methods.<a href="ChBayes.html#fnref24" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ChapSimulation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ChapPremiumFoundations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["LDAEd2.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
